Index: kernel-rpi-6_6/drivers/Kconfig
===================================================================
--- kernel-rpi-6_6.orig/drivers/Kconfig
+++ kernel-rpi-6_6/drivers/Kconfig
@@ -243,4 +243,6 @@ source "drivers/hte/Kconfig"
 
 source "drivers/cdx/Kconfig"
 
+source "drivers/pkglist/Kconfig"
+
 endmenu
Index: kernel-rpi-6_6/drivers/pkglist/Kconfig
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/drivers/pkglist/Kconfig
@@ -0,0 +1,30 @@
+config PKGLIST
+	tristate "Package list for emulated 'SD card' file system for Android"
+	depends on CONFIGFS_FS || !CONFIGFS_FS
+	help
+	  Pkglist presents an interface for Android's emulated sdcard layer.
+	  It relates the names of packages to their package ids, so that they can be
+	  given access to their app specific folders.
+
+	  Additionally, pkglist allows configuring the gid assigned to the lower file
+	  outside of package specific directories for the purpose of tracking storage
+	  with quotas.
+
+choice
+	prompt "Configuration options"
+	depends on PKGLIST
+	help
+	  Configuration options. This controls how you provide the emulated
+	  SD card layer with configuration information from userspace.
+
+config PKGLIST_USE_CONFIGFS
+	bool "Use Configfs based pkglist"
+	depends on CONFIGFS_FS
+	help
+	  Use configfs based pkglist driver for configuration information.
+
+config PKGLIST_NO_CONFIG
+	bool "None"
+	help
+	  This does not allow configuration of sdcardfs.
+endchoice
Index: kernel-rpi-6_6/drivers/pkglist/Makefile
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/drivers/pkglist/Makefile
@@ -0,0 +1,3 @@
+obj-$(CONFIG_PKGLIST) += pkg.o
+pkg-$(CONFIG_PKGLIST_USE_CONFIGFS) += pkglist.o
+pkg-$(CONFIG_PKGLIST_NO_CONFIG) += pkglist_none.o
Index: kernel-rpi-6_6/drivers/pkglist/pkglist.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/drivers/pkglist/pkglist.c
@@ -0,0 +1,966 @@
+/*
+ * Copyright (C) 2017 Google Inc., Author: Daniel Rosenberg <drosen@google.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/module.h>
+#include <linux/hashtable.h>
+#include <linux/atomic.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/configfs.h>
+#include <linux/dcache.h>
+#include <linux/ctype.h>
+#include <linux/cred.h>
+
+#include <linux/pkglist.h>
+
+/*
+ * This presents a configfs interface for Android's emulated sdcard layer.
+ * It relates the names of packages to their package ids, so that they can be
+ * given access to their app specific folders.
+ *
+ * To add a package, create a directory at the base level with the name of that
+ * package. Within these folders, write to appid to set its id.
+ * If an Android user should not know of an app's installation, write their
+ * Android user id to excluded_userids. Write to clear_userid to remove users
+ * from that list.
+ *
+ * remove_userid offers a way to remove all instances of a user from all exclude
+ * lists.
+ *
+ * Additionally, pkglist allows configuring the gid assigned to the lower file
+ * outside of package specific directories for the purpose of tracking storage
+ * with quotas.
+ *
+ * To track files with a particular extension, create a folder inside extensions
+ * for each class of thing you wish to track. Inside that directory, write the
+ * gid you want to associate to the group to ext_gid, and make a directory for
+ * extension you want to include. All are assumed to be case insensitive.
+ *
+ * ex: mkdir /config/[config_location]/extension/audio/
+ *     echo 1055 > /config/[config_location]/extension/audio/ext_gid
+ *     mkdir /config/[config_location]/extension/audio/
+ *
+ */
+
+static char *pkglist_config_location = "sdcardfs";
+module_param(pkglist_config_location, charp, 0);
+MODULE_PARM_DESC(pkglist_config_location, "Location of pkglist in configfs");
+
+static struct kmem_cache *hashtable_entry_cachep;
+
+static DEFINE_HASHTABLE(package_to_appid, 8);
+static DEFINE_HASHTABLE(package_to_userid, 8);
+static DEFINE_HASHTABLE(ext_to_groupid, 8);
+static DEFINE_MUTEX(pkg_list_lock);
+static LIST_HEAD(pkglist_listeners);
+
+struct extensions_value {
+	struct config_group group;
+	kgid_t gid;
+};
+
+struct extension_details {
+	struct config_item item;
+	struct hlist_node hlist;
+	struct qstr name;
+	struct extensions_value *value;
+};
+
+struct hashtable_entry {
+	struct hlist_node hlist;
+	struct hlist_node dlist; /* for deletion cleanup */
+	struct qstr key;
+	atomic_t value;
+};
+
+static unsigned int full_name_case_hash(const unsigned char *name,
+					unsigned int len)
+{
+	unsigned long hash = init_name_hash(0);
+
+	while (len--)
+		hash = partial_name_hash(tolower(*name++), hash);
+	return end_name_hash(hash);
+}
+
+static inline void qstr_init(struct qstr *q, const char *name)
+{
+	q->name = name;
+	q->len = strlen(q->name);
+	q->hash = full_name_case_hash(q->name, q->len);
+}
+
+static inline int qstr_copy(const struct qstr *src, struct qstr *dest)
+{
+	dest->name = kstrdup(src->name, GFP_KERNEL);
+	dest->hash_len = src->hash_len;
+	return !!dest->name;
+}
+
+static kuid_t __get_appid(const struct qstr *key)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = key->hash;
+	uid_t ret_id;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(package_to_appid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			ret_id = atomic_read(&hash_cur->value);
+			rcu_read_unlock();
+			return make_kuid(&init_user_ns, ret_id);
+		}
+	}
+	rcu_read_unlock();
+	return INVALID_UID;
+}
+
+kuid_t pkglist_get_appid(const char *key)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	return __get_appid(&q);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_appid);
+
+static kgid_t __get_ext_gid(const struct qstr *key)
+{
+	struct extension_details *hash_cur;
+	unsigned int hash = key->hash;
+	kgid_t ret_id;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(ext_to_groupid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->name)) {
+			ret_id = hash_cur->value->gid;
+			rcu_read_unlock();
+			return ret_id;
+		}
+	}
+	rcu_read_unlock();
+	return INVALID_GID;
+}
+
+kgid_t pkglist_get_ext_gid(const char *key)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	return __get_ext_gid(&q);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_ext_gid);
+
+static bool __is_excluded(const struct qstr *app_name, uint32_t user)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = app_name->hash;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (atomic_read(&hash_cur->value) == user &&
+				qstr_case_eq(app_name, &hash_cur->key)) {
+			rcu_read_unlock();
+			return true;
+		}
+	}
+	rcu_read_unlock();
+	return false;
+}
+
+bool pkglist_user_is_excluded(const char *key, uint32_t user)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	return __is_excluded(&q, user);
+}
+EXPORT_SYMBOL_GPL(pkglist_user_is_excluded);
+
+kuid_t pkglist_get_allowed_appid(const char *key, uint32_t user)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	if (!__is_excluded(&q, user))
+		return __get_appid(&q);
+	else
+		return INVALID_UID;
+}
+EXPORT_SYMBOL_GPL(pkglist_get_allowed_appid);
+
+static struct hashtable_entry *alloc_hashtable_entry(const struct qstr *key,
+		uid_t value)
+{
+	struct hashtable_entry *ret = kmem_cache_alloc(hashtable_entry_cachep,
+			GFP_KERNEL);
+	if (!ret)
+		return NULL;
+	INIT_HLIST_NODE(&ret->dlist);
+	INIT_HLIST_NODE(&ret->hlist);
+
+	if (!qstr_copy(key, &ret->key)) {
+		kmem_cache_free(hashtable_entry_cachep, ret);
+		return NULL;
+	}
+
+	atomic_set(&ret->value, value);
+	return ret;
+}
+
+static int insert_packagelist_appid_entry_locked(const struct qstr *key,
+						kuid_t value)
+{
+	struct hashtable_entry *hash_cur;
+	struct hashtable_entry *new_entry;
+	unsigned int hash = key->hash;
+
+	hash_for_each_possible_rcu(package_to_appid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			atomic_set(&hash_cur->value, value.val);
+			return 0;
+		}
+	}
+	new_entry = alloc_hashtable_entry(key, value.val);
+	if (!new_entry)
+		return -ENOMEM;
+	hash_add_rcu(package_to_appid, &new_entry->hlist, hash);
+	return 0;
+}
+
+static int insert_ext_gid_entry_locked(struct extension_details *ed)
+{
+	struct extension_details *hash_cur;
+	unsigned int hash = ed->name.hash;
+
+	/* An extension can only belong to one gid */
+	hash_for_each_possible_rcu(ext_to_groupid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(&ed->name, &hash_cur->name))
+			return -EINVAL;
+	}
+
+	hash_add_rcu(ext_to_groupid, &ed->hlist, hash);
+	return 0;
+}
+
+static int insert_userid_exclude_entry_locked(const struct qstr *key,
+						unsigned int value)
+{
+	struct hashtable_entry *hash_cur;
+	struct hashtable_entry *new_entry;
+	unsigned int hash = key->hash;
+
+	/* Only insert if not already present */
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (atomic_read(&hash_cur->value) == value &&
+				qstr_case_eq(key, &hash_cur->key))
+			return 0;
+	}
+	new_entry = alloc_hashtable_entry(key, value);
+	if (!new_entry)
+		return -ENOMEM;
+	hash_add_rcu(package_to_userid, &new_entry->hlist, hash);
+	return 0;
+}
+
+static int insert_packagelist_entry(const struct qstr *key, kuid_t value)
+{
+	struct pkg_list *pkg;
+	int err;
+
+	mutex_lock(&pkg_list_lock);
+	err = insert_packagelist_appid_entry_locked(key, value);
+	if (!err) {
+		list_for_each_entry(pkg, &pkglist_listeners, list) {
+			pkg->update(BY_NAME, key, 0);
+		}
+	}
+	mutex_unlock(&pkg_list_lock);
+
+	return err;
+}
+
+static int insert_ext_gid_entry(struct extension_details *ed)
+{
+	int err;
+
+	mutex_lock(&pkg_list_lock);
+	err = insert_ext_gid_entry_locked(ed);
+	mutex_unlock(&pkg_list_lock);
+
+	return err;
+}
+
+static int insert_userid_exclude_entry(const struct qstr *key, uint32_t value)
+{
+	int err;
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	err = insert_userid_exclude_entry_locked(key, value);
+	if (!err) {
+		list_for_each_entry(pkg, &pkglist_listeners, list) {
+			pkg->update(BY_NAME|BY_USERID, key, value);
+		}
+	}
+	mutex_unlock(&pkg_list_lock);
+
+	return err;
+}
+
+static void free_hashtable_entry(struct hashtable_entry *entry)
+{
+	kfree(entry->key.name);
+	kmem_cache_free(hashtable_entry_cachep, entry);
+}
+
+static void remove_packagelist_entry_locked(const struct qstr *key)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = key->hash;
+	struct hlist_node *h_t;
+	HLIST_HEAD(free_list);
+
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			hash_del_rcu(&hash_cur->hlist);
+			hlist_add_head(&hash_cur->dlist, &free_list);
+		}
+	}
+	hash_for_each_possible_rcu(package_to_appid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			hash_del_rcu(&hash_cur->hlist);
+			hlist_add_head(&hash_cur->dlist, &free_list);
+			break;
+		}
+	}
+	synchronize_rcu();
+	hlist_for_each_entry_safe(hash_cur, h_t, &free_list, dlist)
+		free_hashtable_entry(hash_cur);
+}
+
+static void remove_packagelist_entry(const struct qstr *key)
+{
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	remove_packagelist_entry_locked(key);
+	list_for_each_entry(pkg, &pkglist_listeners, list) {
+		pkg->update(BY_NAME, key, 0);
+	}
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void remove_ext_gid_entry_locked(struct extension_details *ed)
+{
+	struct extension_details *hash_cur;
+	struct qstr *key = &ed->name;
+	unsigned int hash = key->hash;
+
+	hash_for_each_possible_rcu(ext_to_groupid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->name)
+				&& hash_cur->value == ed->value) {
+			hash_del_rcu(&hash_cur->hlist);
+			synchronize_rcu();
+			break;
+		}
+	}
+}
+
+static void remove_ext_gid_entry(struct extension_details *ed)
+{
+	mutex_lock(&pkg_list_lock);
+	remove_ext_gid_entry_locked(ed);
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void remove_userid_all_entry_locked(uint32_t userid)
+{
+	struct hashtable_entry *hash_cur;
+	struct hlist_node *h_t;
+	HLIST_HEAD(free_list);
+	int i;
+
+	hash_for_each_rcu(package_to_userid, i, hash_cur, hlist) {
+		if (atomic_read(&hash_cur->value) == userid) {
+			hash_del_rcu(&hash_cur->hlist);
+			hlist_add_head(&hash_cur->dlist, &free_list);
+		}
+	}
+	synchronize_rcu();
+	hlist_for_each_entry_safe(hash_cur, h_t, &free_list, dlist) {
+		free_hashtable_entry(hash_cur);
+	}
+}
+
+static void remove_userid_all_entry(uint32_t userid)
+{
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	remove_userid_all_entry_locked(userid);
+
+	list_for_each_entry(pkg, &pkglist_listeners, list) {
+		pkg->update(BY_USERID, NULL, userid);
+	}
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void remove_userid_exclude_entry_locked(const struct qstr *key,
+						uint32_t userid)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = key->hash;
+
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key) &&
+				atomic_read(&hash_cur->value) == userid) {
+			hash_del_rcu(&hash_cur->hlist);
+			synchronize_rcu();
+			free_hashtable_entry(hash_cur);
+			break;
+		}
+	}
+}
+
+static void remove_userid_exclude_entry(const struct qstr *key, uint32_t userid)
+{
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	remove_userid_exclude_entry_locked(key, userid);
+	list_for_each_entry(pkg, &pkglist_listeners, list) {
+		pkg->update(BY_NAME|BY_USERID, key, userid);
+	}
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void packagelist_destroy(void)
+{
+	struct hashtable_entry *hash_cur;
+	struct hlist_node *h_t;
+	HLIST_HEAD(free_list);
+	int i;
+
+	mutex_lock(&pkg_list_lock);
+	hash_for_each_rcu(package_to_appid, i, hash_cur, hlist) {
+		hash_del_rcu(&hash_cur->hlist);
+		hlist_add_head(&hash_cur->dlist, &free_list);
+	}
+	hash_for_each_rcu(package_to_userid, i, hash_cur, hlist) {
+		hash_del_rcu(&hash_cur->hlist);
+		hlist_add_head(&hash_cur->dlist, &free_list);
+	}
+	synchronize_rcu();
+	hlist_for_each_entry_safe(hash_cur, h_t, &free_list, dlist)
+		free_hashtable_entry(hash_cur);
+	mutex_unlock(&pkg_list_lock);
+	pr_info("pkglist: destroyed pkglist\n");
+}
+
+#define PACKAGE_DETAILS_ATTR(_pfx, _name)			\
+static struct configfs_attribute _pfx##attr_##_name = {	\
+	.ca_name	= __stringify(_name),		\
+	.ca_mode	= S_IRUGO | S_IWUGO,		\
+	.ca_owner	= THIS_MODULE,			\
+	.show		= _pfx##_name##_show,		\
+	.store		= _pfx##_name##_store,		\
+}
+
+#define PACKAGE_DETAILS_ATTR_RO(_pfx, _name)			\
+static struct configfs_attribute _pfx##attr_##_name = {	\
+	.ca_name	= __stringify(_name),		\
+	.ca_mode	= S_IRUGO,			\
+	.ca_owner	= THIS_MODULE,			\
+	.show		= _pfx##_name##_show,		\
+}
+
+#define PACKAGE_DETAILS_ATTR_WO(_pfx, _name)			\
+static struct configfs_attribute _pfx##attr_##_name = {	\
+	.ca_name	= __stringify(_name),		\
+	.ca_mode	= S_IWUGO,			\
+	.ca_owner	= THIS_MODULE,			\
+	.store		= _pfx##_name##_store,		\
+}
+
+
+struct package_details {
+	struct config_item item;
+	struct qstr name;
+};
+
+static inline struct package_details *to_package_details(
+						struct config_item *item)
+{
+	return item ? container_of(item, struct package_details, item) : NULL;
+}
+
+#define PACKAGE_DETAILS_ATTRIBUTE(name) (&package_details_attr_##name)
+
+static ssize_t package_details_appid_show(struct config_item *item, char *page)
+{
+	return scnprintf(page, PAGE_SIZE, "%u\n", from_kuid(current_user_ns(),
+				__get_appid(&to_package_details(item)->name)));
+}
+
+static ssize_t package_details_appid_store(struct config_item *item,
+					   const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+	kuid_t uid;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+
+	uid = make_kuid(current_user_ns(), tmp);
+
+	ret = insert_packagelist_entry(&to_package_details(item)->name, uid);
+
+	if (ret)
+		return ret;
+
+	return count;
+}
+
+static ssize_t package_details_excluded_userids_show(struct config_item *item,
+						     char *page)
+{
+	struct package_details *package_details = to_package_details(item);
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = package_details->name.hash;
+	int count = 0;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(&package_details->name, &hash_cur->key))
+			count += scnprintf(page + count, PAGE_SIZE - count,
+					   "%d ", atomic_read(&hash_cur->value));
+	}
+	rcu_read_unlock();
+	if (count)
+		count--;
+	count += scnprintf(page + count, PAGE_SIZE - count, "\n");
+	return count;
+}
+
+static ssize_t package_details_excluded_userids_store(struct config_item *item,
+						      const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+
+	ret = insert_userid_exclude_entry(&to_package_details(item)->name, tmp);
+
+	if (ret)
+		return ret;
+
+	return count;
+}
+
+static ssize_t package_details_clear_userid_store(struct config_item *item,
+						  const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+	remove_userid_exclude_entry(&to_package_details(item)->name, tmp);
+	return count;
+}
+
+static void package_details_release(struct config_item *item)
+{
+	struct package_details *package_details = to_package_details(item);
+
+	pr_debug("pkglist: removing %s\n", package_details->name.name);
+	remove_packagelist_entry(&package_details->name);
+	kfree(package_details->name.name);
+	kfree(package_details);
+}
+
+PACKAGE_DETAILS_ATTR(package_details_, appid);
+PACKAGE_DETAILS_ATTR(package_details_, excluded_userids);
+PACKAGE_DETAILS_ATTR_WO(package_details_, clear_userid);
+
+static struct configfs_attribute *package_details_attrs[] = {
+	PACKAGE_DETAILS_ATTRIBUTE(appid),
+	PACKAGE_DETAILS_ATTRIBUTE(excluded_userids),
+	PACKAGE_DETAILS_ATTRIBUTE(clear_userid),
+	NULL,
+};
+
+static struct configfs_item_operations package_details_item_ops = {
+	.release = package_details_release,
+};
+
+static struct config_item_type package_appid_type = {
+	.ct_item_ops	= &package_details_item_ops,
+	.ct_attrs	= package_details_attrs,
+	.ct_owner	= THIS_MODULE,
+};
+
+static inline struct extensions_value *to_extensions_value(
+					struct config_item *item)
+{
+	return item ? container_of(to_config_group(item),
+				struct extensions_value, group)
+			: NULL;
+}
+
+static inline struct extension_details *to_extension_details(
+					struct config_item *item)
+{
+	return item ? container_of(item, struct extension_details, item)
+			: NULL;
+}
+
+#define EXTENSIONS_VALUE_ATTRIBUTE(name) (&extensions_value_attr_##name)
+
+static void extension_details_release(struct config_item *item)
+{
+	struct extension_details *ed = to_extension_details(item);
+
+	pr_debug("pkglist: No longer mapping %s files to gid %d\n",
+				ed->name.name,
+				from_kgid(current_user_ns(), ed->value->gid));
+	remove_ext_gid_entry(ed);
+	kfree(ed->name.name);
+	kfree(ed);
+}
+
+static struct configfs_item_operations extension_details_item_ops = {
+	.release = extension_details_release,
+};
+
+static ssize_t extensions_value_ext_gid_show(
+			struct config_item *item, char *page)
+{
+	return scnprintf(page, PAGE_SIZE, "%u\n",
+				from_kgid(current_user_ns(), to_extensions_value(item)->gid));
+}
+
+static ssize_t extensions_value_ext_gid_store(
+				struct config_item *item,
+				const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+
+	to_extensions_value(item)->gid = make_kgid(current_user_ns(), tmp);
+
+	return count;
+}
+
+PACKAGE_DETAILS_ATTR(extensions_value_, ext_gid);
+
+static struct configfs_attribute *extensions_value_attrs[] = {
+	EXTENSIONS_VALUE_ATTRIBUTE(ext_gid),
+	NULL,
+};
+
+static struct config_item_type extension_details_type = {
+	.ct_item_ops = &extension_details_item_ops,
+	.ct_owner = THIS_MODULE,
+};
+
+static struct config_item *extension_details_make_item(
+				struct config_group *group, const char *name)
+{
+	struct extensions_value *extensions_value =
+			to_extensions_value(&group->cg_item);
+	struct extension_details *extension_details =
+			kzalloc(sizeof(struct extension_details), GFP_KERNEL);
+	const char *tmp;
+	int ret;
+
+	if (!extension_details)
+		return ERR_PTR(-ENOMEM);
+
+	tmp = kstrdup(name, GFP_KERNEL);
+	if (!tmp) {
+		kfree(extension_details);
+		return ERR_PTR(-ENOMEM);
+	}
+	qstr_init(&extension_details->name, tmp);
+	extension_details->value = extensions_value;
+	ret = insert_ext_gid_entry(extension_details);
+
+	if (ret) {
+		kfree(extension_details->name.name);
+		kfree(extension_details);
+		return ERR_PTR(ret);
+	}
+	config_item_init_type_name(&extension_details->item, name,
+					&extension_details_type);
+
+	return &extension_details->item;
+}
+
+static struct configfs_group_operations extensions_value_group_ops = {
+	.make_item = extension_details_make_item,
+};
+
+static struct config_item_type extensions_name_type = {
+	.ct_attrs	= extensions_value_attrs,
+	.ct_group_ops	= &extensions_value_group_ops,
+	.ct_owner	= THIS_MODULE,
+};
+
+static struct config_group *extensions_make_group(struct config_group *group,
+							const char *name)
+{
+	struct extensions_value *extensions_value;
+	unsigned int tmp;
+	int ret;
+
+	extensions_value = kzalloc(sizeof(struct extensions_value), GFP_KERNEL);
+	if (!extensions_value)
+		return ERR_PTR(-ENOMEM);
+	/* For legacy reasons, if the name is a number, assume it's the gid*/
+	ret = kstrtouint(name, 10, &tmp);
+	if (!ret)
+		extensions_value->gid = make_kgid(current_user_ns(), tmp);
+
+	config_group_init_type_name(&extensions_value->group, name,
+						&extensions_name_type);
+	return &extensions_value->group;
+}
+
+static void extensions_drop_group(struct config_group *group,
+					struct config_item *item)
+{
+	struct extensions_value *value = to_extensions_value(item);
+
+	pr_debug("pkglist: No longer mapping any files to gid %d\n",
+			from_kgid(current_user_ns(), value->gid));
+	kfree(value);
+}
+
+static struct configfs_group_operations extensions_group_ops = {
+	.make_group	= extensions_make_group,
+	.drop_item	= extensions_drop_group,
+};
+
+static struct config_item_type extensions_type = {
+	.ct_group_ops	= &extensions_group_ops,
+	.ct_owner	= THIS_MODULE,
+};
+
+static struct config_group extension_group = {
+	.cg_item = {
+		.ci_namebuf = "extensions",
+		.ci_type = &extensions_type,
+	},
+};
+
+struct packages {
+	struct configfs_subsystem subsystem;
+};
+
+static inline struct packages *to_packages(struct config_item *item)
+{
+	return item ? container_of(
+			to_configfs_subsystem(to_config_group(item)),
+					struct packages, subsystem) : NULL;
+}
+
+static struct config_item *packages_make_item(struct config_group *group,
+							const char *name)
+{
+	struct package_details *package_details;
+	const char *tmp;
+
+	package_details = kzalloc(sizeof(struct package_details), GFP_KERNEL);
+	if (!package_details)
+		return ERR_PTR(-ENOMEM);
+	tmp = kstrdup(name, GFP_KERNEL);
+	if (!tmp) {
+		kfree(package_details);
+		return ERR_PTR(-ENOMEM);
+	}
+	qstr_init(&package_details->name, tmp);
+	config_item_init_type_name(&package_details->item, name,
+						&package_appid_type);
+
+	return &package_details->item;
+}
+
+static ssize_t packages_list_show(struct config_item *item, char *page)
+{
+	struct hashtable_entry *hash_cur_app;
+	struct hashtable_entry *hash_cur_user;
+	int i;
+	int count = 0, written = 0;
+	const char errormsg[] = "<truncated>\n";
+	unsigned int hash;
+
+	rcu_read_lock();
+	hash_for_each_rcu(package_to_appid, i, hash_cur_app, hlist) {
+		written = scnprintf(page + count,
+				    PAGE_SIZE - sizeof(errormsg) - count,
+				    "%s %d\n",
+				    hash_cur_app->key.name,
+				    atomic_read(&hash_cur_app->value));
+		hash = hash_cur_app->key.hash;
+		hash_for_each_possible_rcu(package_to_userid, hash_cur_user, hlist, hash) {
+			if (qstr_case_eq(&hash_cur_app->key, &hash_cur_user->key)) {
+				written += scnprintf(page + count + written - 1,
+					PAGE_SIZE - sizeof(errormsg) - count - written + 1,
+					" %d\n", atomic_read(&hash_cur_user->value)) - 1;
+			}
+		}
+		if (count + written == PAGE_SIZE - sizeof(errormsg) - 1) {
+			count += scnprintf(page + count, PAGE_SIZE - count, errormsg);
+			break;
+		}
+		count += written;
+	}
+	rcu_read_unlock();
+
+	return count;
+}
+
+static ssize_t packages_remove_userid_store(struct config_item *item,
+					    const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+	remove_userid_all_entry(tmp);
+	return count;
+}
+
+static struct configfs_attribute packages_attr_packages_gid_list = {
+    .ca_name	= "packages_gid.list",
+    .ca_mode	= S_IRUGO,
+    .ca_owner	= THIS_MODULE,
+    .show	= packages_list_show,
+};
+PACKAGE_DETAILS_ATTR_WO(packages_, remove_userid);
+
+static struct configfs_attribute *packages_attrs[] = {
+	&packages_attr_packages_gid_list,
+	&packages_attr_remove_userid,
+	NULL,
+};
+
+/*
+ * Note that, since no extra work is required on ->drop_item(),
+ * no ->drop_item() is provided.
+ */
+static struct configfs_group_operations packages_group_ops = {
+	.make_item	= packages_make_item,
+};
+
+static struct config_item_type packages_type = {
+	.ct_group_ops	= &packages_group_ops,
+	.ct_attrs	= packages_attrs,
+	.ct_owner	= THIS_MODULE,
+};
+
+static struct config_group *sd_default_groups[] = {
+	&extension_group,
+	NULL,
+};
+
+static struct packages pkglist_packages = {
+	.subsystem = {
+		.su_group = {
+			.cg_item = {
+				.ci_type = &packages_type,
+			},
+		},
+	},
+};
+
+static int configfs_pkglist_init(void)
+{
+	int ret, i;
+	struct configfs_subsystem *subsys = &pkglist_packages.subsystem;
+	config_item_set_name(&pkglist_packages.subsystem.su_group.cg_item,
+						pkglist_config_location);
+	config_group_init(&subsys->su_group);
+
+	for (i = 0; sd_default_groups[i]; i++) {
+		config_group_init(sd_default_groups[i]);
+		configfs_add_default_group(sd_default_groups[i], &subsys->su_group);
+	}
+	mutex_init(&subsys->su_mutex);
+	ret = configfs_register_subsystem(subsys);
+	if (ret) {
+		pr_err("Error %d while registering subsystem %s\n", ret,
+				subsys->su_group.cg_item.ci_namebuf);
+	}
+	return ret;
+}
+
+static void configfs_pkglist_exit(void)
+{
+	configfs_unregister_subsystem(&pkglist_packages.subsystem);
+}
+
+void pkglist_register_update_listener(struct pkg_list *pkg)
+{
+	if (!pkg->update)
+		return;
+	mutex_lock(&pkg_list_lock);
+	list_add(&pkg->list, &pkglist_listeners);
+	mutex_unlock(&pkg_list_lock);
+}
+EXPORT_SYMBOL_GPL(pkglist_register_update_listener);
+
+void pkglist_unregister_update_listener(struct pkg_list *pkg)
+{
+	mutex_lock(&pkg_list_lock);
+	list_del(&pkg->list);
+	mutex_unlock(&pkg_list_lock);
+}
+EXPORT_SYMBOL_GPL(pkglist_unregister_update_listener);
+
+static int __init pkglist_init(void)
+{
+	hashtable_entry_cachep =
+		kmem_cache_create("packagelist_hashtable_entry",
+				sizeof(struct hashtable_entry), 0, 0, NULL);
+	if (!hashtable_entry_cachep) {
+		pr_err("pkglist: failed creating pkgl_hashtable entry slab cache\n");
+		return -ENOMEM;
+	}
+
+	return configfs_pkglist_init();
+}
+module_init(pkglist_init);
+
+static void __exit pkglist_exit(void)
+{
+	configfs_pkglist_exit();
+	packagelist_destroy();
+	kmem_cache_destroy(hashtable_entry_cachep);
+}
+
+module_exit(pkglist_exit);
+
+MODULE_AUTHOR("Daniel Rosenberg, Google");
+MODULE_DESCRIPTION("Configfs Pkglist implementation");
+MODULE_LICENSE("GPL v2");
Index: kernel-rpi-6_6/drivers/pkglist/pkglist_none.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/drivers/pkglist/pkglist_none.c
@@ -0,0 +1,57 @@
+/*
+ * Copyright (C) 2017 Google Inc., Author: Daniel Rosenberg <drosen@google.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/ctype.h>
+#include <linux/dcache.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/pkglist.h>
+
+kuid_t pkglist_get_appid(const char *key)
+{
+	return make_kuid(&init_user_ns, 0);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_appid);
+
+kgid_t pkglist_get_ext_gid(const char *key)
+{
+	return make_kgid(&init_user_ns, 0);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_ext_gid);
+
+bool pkglist_user_is_excluded(const char *key, uint32_t user)
+{
+	return false;
+}
+EXPORT_SYMBOL_GPL(pkglist_user_is_excluded);
+
+kuid_t pkglist_get_allowed_appid(const char *key, uint32_t user)
+{
+	return make_kuid(&init_user_ns, 0);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_allowed_appid);
+
+void pkglist_register_update_listener(struct pkg_list *pkg) { }
+EXPORT_SYMBOL_GPL(pkglist_register_update_listener);
+
+void pkglist_unregister_update_listener(struct pkg_list *pkg) { }
+EXPORT_SYMBOL_GPL(pkglist_unregister_update_listener);
+
+static int __init pkglist_init(void)
+{
+	return 0;
+}
+module_init(pkglist_init);
+
+static void pkglist_exit(void) { }
+
+module_exit(pkglist_exit);
+
+MODULE_AUTHOR("Daniel Rosenberg, Google");
+MODULE_DESCRIPTION("Empty Pkglist implementation");
+MODULE_LICENSE("GPL v2");
Index: kernel-rpi-6_6/drivers/Makefile
===================================================================
--- kernel-rpi-6_6.orig/drivers/Makefile
+++ kernel-rpi-6_6/drivers/Makefile
@@ -195,7 +195,9 @@ obj-$(CONFIG_COUNTER)		+= counter/
 obj-$(CONFIG_MOST)		+= most/
 obj-$(CONFIG_PECI)		+= peci/
 obj-$(CONFIG_HTE)		+= hte/
+obj-$(CONFIG_PKGLIST)   += pkglist/
 obj-$(CONFIG_DRM_ACCEL)		+= accel/
 obj-$(CONFIG_CDX_BUS)		+= cdx/
 
 obj-$(CONFIG_S390)		+= s390/
+
Index: kernel-rpi-6_6/drivers/char/mem.c
===================================================================
--- kernel-rpi-6_6.orig/drivers/char/mem.c
+++ kernel-rpi-6_6/drivers/char/mem.c
@@ -30,6 +30,7 @@
 #include <linux/uio.h>
 #include <linux/uaccess.h>
 #include <linux/security.h>
+#include <linux/low-mem-notify.h>
 
 #ifdef CONFIG_IA64
 # include <linux/efi.h>
@@ -710,6 +711,9 @@ static const struct memdev {
 #ifdef CONFIG_PRINTK
 	[11] = { "kmsg", &kmsg_fops, 0, 0644 },
 #endif
+#ifdef CONFIG_LOW_MEM_NOTIFY
+	[12] = { "chromeos-low-mem", &low_mem_notify_fops, 0, 0666 },
+#endif
 };
 
 static int memory_open(struct inode *inode, struct file *filp)
Index: kernel-rpi-6_6/fs/Kconfig
===================================================================
--- kernel-rpi-6_6.orig/fs/Kconfig
+++ kernel-rpi-6_6/fs/Kconfig
@@ -315,6 +315,7 @@ source "fs/orangefs/Kconfig"
 source "fs/adfs/Kconfig"
 source "fs/affs/Kconfig"
 source "fs/ecryptfs/Kconfig"
+source "fs/esdfs/Kconfig"
 source "fs/hfs/Kconfig"
 source "fs/hfsplus/Kconfig"
 source "fs/befs/Kconfig"
Index: kernel-rpi-6_6/fs/Makefile
===================================================================
--- kernel-rpi-6_6.orig/fs/Makefile
+++ kernel-rpi-6_6/fs/Makefile
@@ -4,7 +4,7 @@
 #
 # 14 Sep 2000, Christoph Hellwig <hch@infradead.org>
 # Rewritten to use lists instead of if-statements.
-# 
+#
 
 
 obj-y :=	open.o read_write.o file_table.o super.o \
@@ -57,7 +57,7 @@ obj-$(CONFIG_CONFIGFS_FS)	+= configfs/
 obj-y				+= devpts/
 
 obj-$(CONFIG_DLM)		+= dlm/
- 
+
 # Do not add any filesystems before this line
 obj-$(CONFIG_NETFS_SUPPORT)	+= netfs/
 obj-$(CONFIG_FSCACHE)		+= fscache/
@@ -81,6 +81,7 @@ obj-$(CONFIG_ISO9660_FS)	+= isofs/
 obj-$(CONFIG_HFSPLUS_FS)	+= hfsplus/ # Before hfs to find wrapped HFS+
 obj-$(CONFIG_HFS_FS)		+= hfs/
 obj-$(CONFIG_ECRYPT_FS)		+= ecryptfs/
+obj-$(CONFIG_ESD_FS)           += esdfs/
 obj-$(CONFIG_VXFS_FS)		+= freevxfs/
 obj-$(CONFIG_NFS_FS)		+= nfs/
 obj-$(CONFIG_EXPORTFS)		+= exportfs/
Index: kernel-rpi-6_6/fs/esdfs/Kconfig
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/esdfs/Kconfig
@@ -0,0 +1,7 @@
+config ESD_FS
+	tristate "Emulated 'SD card' file system for Android (EXPERIMENTAL)"
+	depends on PKGLIST
+	depends on USER_NS
+	help
+	  Esdfs is a wrapfs-based file system, designed to implement the
+	  Android "sdcard" FUSE-backed file system from within the kernel.
Index: kernel-rpi-6_6/fs/esdfs/Makefile
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/esdfs/Makefile
@@ -0,0 +1,7 @@
+ESDFS_VERSION="0.3"
+
+EXTRA_CFLAGS += -DESDFS_VERSION=\"$(ESDFS_VERSION)\"
+
+obj-$(CONFIG_ESD_FS) += esdfs.o
+
+esdfs-y := dentry.o file.o inode.o main.o super.o lookup.o mmap.o derive.o
Index: kernel-rpi-6_6/fs/esdfs/dentry.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/esdfs/dentry.c
@@ -0,0 +1,158 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/ctype.h>
+#include "esdfs.h"
+
+/*
+ * returns: -ERRNO if error (returned to user)
+ *          0: tell VFS to invalidate dentry
+ *          1: dentry is valid
+ */
+static int esdfs_d_revalidate(struct dentry *dentry, unsigned int flags)
+{
+	struct path lower_path;
+	struct path lower_parent_path;
+	struct dentry *parent_dentry = NULL;
+	struct dentry *lower_dentry = NULL;
+	struct dentry *lower_parent_dentry = NULL;
+	int err = 1;
+
+	if (flags & LOOKUP_RCU)
+		return -ECHILD;
+
+	/* short-circuit if it's root */
+	spin_lock(&dentry->d_lock);
+	if (IS_ROOT(dentry)) {
+		spin_unlock(&dentry->d_lock);
+		return 1;
+	}
+	spin_unlock(&dentry->d_lock);
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	esdfs_get_lower_parent(dentry, lower_dentry, &lower_parent_dentry);
+
+	parent_dentry = dget_parent(dentry);
+	esdfs_get_lower_path(parent_dentry, &lower_parent_path);
+
+	if (lower_parent_path.dentry != lower_parent_dentry)
+		goto drop;
+
+	if (lower_dentry->d_flags & DCACHE_OP_REVALIDATE) {
+		err = lower_dentry->d_op->d_revalidate(lower_dentry, flags);
+		if (err == 0)
+			goto drop;
+	}
+
+	/* can't do strcmp if lower is hashed */
+	spin_lock(&lower_dentry->d_lock);
+	if (d_unhashed(lower_dentry)) {
+		spin_unlock(&lower_dentry->d_lock);
+		goto drop;
+	}
+
+	spin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);
+
+	if (!qstr_case_eq(&lower_dentry->d_name, &dentry->d_name)) {
+		err = 0;
+		__d_drop(dentry);	/* already holding spin lock */
+	}
+
+	spin_unlock(&dentry->d_lock);
+	spin_unlock(&lower_dentry->d_lock);
+
+	esdfs_revalidate_perms(dentry);
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(dentry->d_sb)) &&
+	    esdfs_derived_revalidate(dentry, parent_dentry))
+		goto drop;
+
+	goto out;
+
+drop:
+	d_drop(dentry);
+	err = 0;
+out:
+	esdfs_put_lower_path(parent_dentry, &lower_parent_path);
+	dput(parent_dentry);
+	esdfs_put_lower_parent(dentry, &lower_parent_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	return err;
+}
+
+/* directly from fs/fat/namei_vfat.c */
+static unsigned int __vfat_striptail_len(unsigned int len, const char *name)
+{
+	while (len && name[len - 1] == '.')
+		len--;
+	return len;
+}
+
+static unsigned int vfat_striptail_len(const struct qstr *qstr)
+{
+	return __vfat_striptail_len(qstr->len, qstr->name);
+}
+
+
+/* based on vfat_hashi() in fs/fat/namei_vfat.c (no code pages) */
+static int esdfs_d_hash(const struct dentry *dentry, struct qstr *qstr)
+{
+	const unsigned char *name;
+	unsigned int len;
+	unsigned long hash;
+
+	name = qstr->name;
+	len = vfat_striptail_len(qstr);
+
+	hash = init_name_hash(dentry);
+	while (len--)
+		hash = partial_name_hash(tolower(*name++), hash);
+	qstr->hash = end_name_hash(hash);
+
+	return 0;
+}
+
+/* based on vfat_cmpi() in fs/fat/namei_vfat.c (no code pages) */
+static int esdfs_d_compare(const struct dentry *dentry, unsigned int len,
+			   const char *str, const struct qstr *name)
+{
+	unsigned int alen, blen;
+
+	/* A filename cannot end in '.' or we treat it like it has none */
+	alen = vfat_striptail_len(name);
+	blen = __vfat_striptail_len(len, str);
+	if (alen == blen) {
+		if (str_n_case_eq(name->name, str, alen))
+			return 0;
+	}
+	return 1;
+}
+
+static void esdfs_d_release(struct dentry *dentry)
+{
+	if (!dentry || !dentry->d_fsdata)
+		return;
+
+	/* release and reset the lower paths */
+	esdfs_put_reset_lower_paths(dentry);
+	esdfs_release_lower_parent(dentry);
+	esdfs_free_dentry_private_data(dentry);
+}
+
+const struct dentry_operations esdfs_dops = {
+	.d_revalidate	= esdfs_d_revalidate,
+	.d_delete	= always_delete_dentry,
+	.d_hash		= esdfs_d_hash,
+	.d_compare	= esdfs_d_compare,
+	.d_release	= esdfs_d_release,
+};
Index: kernel-rpi-6_6/fs/esdfs/derive.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/esdfs/derive.c
@@ -0,0 +1,609 @@
+/*
+ * Copyright (c) 2013-2014 Motorola Mobility LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/proc_fs.h>
+#include <linux/hashtable.h>
+#include <linux/syscalls.h>
+#include <linux/fcntl.h>
+#include <linux/ctype.h>
+#include <linux/vmalloc.h>
+#include <linux/security.h>
+#include <linux/uaccess.h>
+#include <linux/filelock.h>
+#include "esdfs.h"
+
+static struct qstr names_secure[] = {
+	QSTR_LITERAL("autorun.inf"),
+	QSTR_LITERAL(".android_secure"),
+	QSTR_LITERAL("android_secure"),
+	QSTR_LITERAL("")
+};
+
+/* special path name searches */
+static inline bool match_name(struct qstr *name, struct qstr names[])
+{
+	int i = 0;
+
+	BUG_ON(!name);
+	for (i = 0; *names[i].name; i++)
+		if (qstr_case_eq(name, &names[i]))
+			return true;
+
+	return false;
+}
+
+unsigned esdfs_package_list_version;
+
+static void fixup_perms_by_flag(int flags, const struct qstr *key,
+					uint32_t userid)
+{
+	esdfs_package_list_version++;
+}
+
+static struct pkg_list esdfs_pkg_list = {
+		.update = fixup_perms_by_flag,
+};
+
+int esdfs_init_package_list(void)
+{
+	pkglist_register_update_listener(&esdfs_pkg_list);
+	return 0;
+}
+
+void esdfs_destroy_package_list(void)
+{
+	pkglist_unregister_update_listener(&esdfs_pkg_list);
+}
+
+/*
+ * Derive an entry's premissions tree position based on its parent.
+ */
+void esdfs_derive_perms(struct dentry *dentry)
+{
+	struct esdfs_inode_info *inode_i = ESDFS_I(dentry->d_inode);
+	bool is_root;
+	int __maybe_unused ret;
+	kuid_t appid;
+	struct qstr q_Download = QSTR_LITERAL("Download");
+	struct qstr q_Android = QSTR_LITERAL("Android");
+	struct qstr q_data = QSTR_LITERAL("data");
+	struct qstr q_obb = QSTR_LITERAL("obb");
+	struct qstr q_media = QSTR_LITERAL("media");
+	struct qstr q_cache = QSTR_LITERAL("cache");
+	struct qstr q_user = QSTR_LITERAL("user");
+	struct esdfs_inode_info *parent_i = ESDFS_I(dentry->d_parent->d_inode);
+
+	spin_lock(&dentry->d_lock);
+	is_root = IS_ROOT(dentry);
+	spin_unlock(&dentry->d_lock);
+	if (is_root)
+		return;
+
+	/* Inherit from the parent to start */
+	inode_i->tree = parent_i->tree;
+	inode_i->userid = parent_i->userid;
+	inode_i->appid = parent_i->appid;
+	inode_i->under_obb = parent_i->under_obb;
+
+	/*
+	 * ESDFS_TREE_MEDIA* are intentionally dead ends.
+	 */
+	switch (inode_i->tree) {
+	case ESDFS_TREE_ROOT_LEGACY:
+		inode_i->tree = ESDFS_TREE_ROOT;
+		ret = kstrtou32(dentry->d_name.name, 0, &inode_i->userid);
+		if (qstr_case_eq(&dentry->d_name, &q_obb))
+			inode_i->tree = ESDFS_TREE_ANDROID_OBB;
+		break;
+
+	case ESDFS_TREE_ROOT:
+		inode_i->tree = ESDFS_TREE_MEDIA;
+		if (qstr_case_eq(&dentry->d_name, &q_Download))
+			inode_i->tree = ESDFS_TREE_DOWNLOAD;
+		else if (qstr_case_eq(&dentry->d_name, &q_Android))
+			inode_i->tree = ESDFS_TREE_ANDROID;
+		break;
+
+	case ESDFS_TREE_ANDROID:
+		if (qstr_case_eq(&dentry->d_name, &q_data)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_DATA;
+		} else if (qstr_case_eq(&dentry->d_name, &q_obb)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_OBB;
+			inode_i->under_obb = true;
+		} else if (qstr_case_eq(&dentry->d_name, &q_media)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_MEDIA;
+		} else if (ESDFS_RESTRICT_PERMS(ESDFS_SB(dentry->d_sb)) &&
+			 qstr_case_eq(&dentry->d_name, &q_user)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_USER;
+		}
+		break;
+
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_OBB:
+	case ESDFS_TREE_ANDROID_MEDIA:
+		appid = pkglist_get_allowed_appid(dentry->d_name.name,
+						inode_i->userid);
+		if (uid_valid(appid))
+			inode_i->appid = esdfs_from_kuid(
+					ESDFS_SB(dentry->d_sb), appid);
+		else
+			inode_i->appid = 0;
+		inode_i->tree = ESDFS_TREE_ANDROID_APP;
+		break;
+	case ESDFS_TREE_ANDROID_APP:
+		if (qstr_case_eq(&dentry->d_name, &q_cache))
+			inode_i->tree = ESDFS_TREE_ANDROID_APP_CACHE;
+		break;
+	case ESDFS_TREE_ANDROID_USER:
+		/* Another user, so start over */
+		inode_i->tree = ESDFS_TREE_ROOT;
+		ret = kstrtou32(dentry->d_name.name, 0, &inode_i->userid);
+		break;
+	}
+}
+
+/* Apply tree position-specific permissions */
+void esdfs_set_derived_perms(struct inode *inode)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(inode->i_sb);
+	struct esdfs_inode_info *inode_i = ESDFS_I(inode);
+	gid_t gid = sbi->upper_perms.gid;
+
+	esdfs_i_uid_write(inode, sbi->upper_perms.uid);
+	inode->i_mode &= S_IFMT;
+	if (ESDFS_RESTRICT_PERMS(sbi))
+		esdfs_i_gid_write(inode, gid);
+	else {
+		if (gid == AID_SDCARD_RW && !test_opt(sbi, DEFAULT_NORMAL))
+			esdfs_i_gid_write(inode, AID_SDCARD_RW);
+		else
+			esdfs_i_gid_write(inode, derive_uid(inode_i, gid));
+		inode->i_mode |= sbi->upper_perms.dmask;
+	}
+
+	switch (inode_i->tree) {
+	case ESDFS_TREE_ROOT_LEGACY:
+		if (ESDFS_RESTRICT_PERMS(sbi))
+			inode->i_mode |= sbi->upper_perms.dmask;
+		else if (test_opt(sbi, DERIVE_MULTI)) {
+			inode->i_mode &= S_IFMT;
+			inode->i_mode |= 0711;
+		}
+		break;
+
+	case ESDFS_TREE_NONE:
+	case ESDFS_TREE_ROOT:
+		if (ESDFS_RESTRICT_PERMS(sbi)) {
+			esdfs_i_gid_write(inode, AID_SDCARD_R);
+			inode->i_mode |= sbi->upper_perms.dmask;
+		} else if (test_opt(sbi, DERIVE_PUBLIC) &&
+			   test_opt(ESDFS_SB(inode->i_sb), DERIVE_CONFINE)) {
+			inode->i_mode &= S_IFMT;
+			inode->i_mode |= 0771;
+		}
+		break;
+
+	case ESDFS_TREE_MEDIA:
+		if (ESDFS_RESTRICT_PERMS(sbi)) {
+			esdfs_i_gid_write(inode, AID_SDCARD_R);
+			inode->i_mode |= 0770;
+		}
+		break;
+
+	case ESDFS_TREE_DOWNLOAD:
+	case ESDFS_TREE_ANDROID:
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_OBB:
+	case ESDFS_TREE_ANDROID_MEDIA:
+		if (ESDFS_RESTRICT_PERMS(sbi))
+			inode->i_mode |= 0771;
+		break;
+
+	case ESDFS_TREE_ANDROID_APP:
+	case ESDFS_TREE_ANDROID_APP_CACHE:
+		if (inode_i->appid)
+			esdfs_i_uid_write(inode, derive_uid(inode_i,
+							inode_i->appid));
+		if (ESDFS_RESTRICT_PERMS(sbi))
+			inode->i_mode |= 0770;
+		break;
+
+	case ESDFS_TREE_ANDROID_USER:
+		if (ESDFS_RESTRICT_PERMS(sbi)) {
+			esdfs_i_gid_write(inode, AID_SDCARD_ALL);
+			inode->i_mode |= 0770;
+		}
+		inode->i_mode |= 0770;
+		break;
+	}
+
+	/* strip execute bits from any non-directories */
+	if (!S_ISDIR(inode->i_mode))
+		inode->i_mode &= ~S_IXUGO;
+}
+
+/*
+ * Before rerouting a lookup to follow a pseudo hard link, make sure that
+ * a stub exists at the source.  Without it, readdir won't see an entry there
+ * resulting in a strange user experience.
+ */
+static int lookup_link_source(struct dentry *dentry, struct dentry *parent)
+{
+	struct path lower_parent_path, lower_path;
+	int err;
+
+	esdfs_get_lower_path(parent, &lower_parent_path);
+
+	/* Check if the stub user profile folder is there. */
+	err = esdfs_lookup_nocase(&lower_parent_path, &dentry->d_name,
+					&lower_path);
+	/* Remember it to handle renames and removal. */
+	if (!err)
+		esdfs_set_lower_stub_path(dentry, &lower_path);
+
+	esdfs_put_lower_path(parent, &lower_parent_path);
+
+	return err;
+}
+
+int esdfs_is_dl_lookup(struct dentry *dentry, struct dentry *parent)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(parent->d_sb);
+	struct esdfs_inode_info *parent_i = ESDFS_I(parent->d_inode);
+	/*
+	 * Return 1 if this is the Download directory:
+	 * The test for download checks:
+	 * 1. The parent is the mount root.
+	 * 2. The directory is named 'Download'.
+	 * 3. The stub for the directory exists.
+	 */
+	if (test_opt(sbi, SPECIAL_DOWNLOAD) &&
+			parent_i->tree == ESDFS_TREE_ROOT &&
+			ESDFS_DENTRY_NEEDS_DL_LINK(dentry) &&
+			lookup_link_source(dentry, parent) == 0) {
+		return 1;
+	}
+
+	return 0;
+}
+
+int esdfs_derived_lookup(struct dentry *dentry, struct dentry **parent)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB((*parent)->d_sb);
+	struct esdfs_inode_info *parent_i = ESDFS_I((*parent)->d_inode);
+	struct qstr q_Android = QSTR_LITERAL("Android");
+
+	/* Deny access to security-sensitive entries. */
+	if (ESDFS_I((*parent)->d_inode)->tree == ESDFS_TREE_ROOT &&
+	    match_name(&dentry->d_name, names_secure)) {
+		pr_debug("esdfs: denying access to: %s", dentry->d_name.name);
+		return -EACCES;
+	}
+
+	/* Pin the unified mode obb link parent as it flies by. */
+	if (!sbi->obb_parent &&
+	    test_opt(sbi, DERIVE_UNIFIED) &&
+	    parent_i->tree == ESDFS_TREE_ROOT &&
+	    parent_i->userid == 0 &&
+	    qstr_case_eq(&dentry->d_name, &q_Android))
+		sbi->obb_parent = dget(dentry);		/* keep it pinned */
+
+	/*
+	 * Handle obb directory "grafting" as a pseudo hard link by overriding
+	 * its parent to point to the target obb directory's parent.  The rest
+	 * of the lookup process will take care of setting up the bottom half
+	 * to point to the real obb directory.
+	 */
+	if (parent_i->tree == ESDFS_TREE_ANDROID &&
+	    ESDFS_DENTRY_NEEDS_LINK(dentry) &&
+	    lookup_link_source(dentry, *parent) == 0) {
+		BUG_ON(!sbi->obb_parent);
+		if (ESDFS_INODE_CAN_LINK((*parent)->d_inode))
+			*parent = dget(sbi->obb_parent);
+	}
+
+	return 0;
+}
+
+int esdfs_derived_revalidate(struct dentry *dentry, struct dentry *parent)
+{
+	/*
+	 * If obb is not linked yet, it means the dentry is pointing to the
+	 * stub.  Invalidate the dentry to force another lookup.
+	 */
+	if (ESDFS_I(parent->d_inode)->tree == ESDFS_TREE_ANDROID &&
+	    ESDFS_INODE_CAN_LINK(dentry->d_inode) &&
+	    ESDFS_DENTRY_NEEDS_LINK(dentry) &&
+	    !ESDFS_DENTRY_IS_LINKED(dentry))
+		return -ESTALE;
+	if (ESDFS_I(parent->d_inode)->tree == ESDFS_TREE_ROOT &&
+	    ESDFS_DENTRY_NEEDS_DL_LINK(dentry) &&
+	    !ESDFS_DENTRY_IS_LINKED(dentry))
+		return -ESTALE;
+	return 0;
+}
+
+/*
+ * Implement the extra checking that is done based on the caller's package
+ * list-based access rights.
+ */
+int esdfs_check_derived_permission(struct inode *inode, int mask)
+{
+	const struct cred *cred;
+	uid_t uid, appid;
+
+	/*
+	 * If we don't need to restrict access based on app GIDs and confine
+	 * writes to outside of the Android/... tree, we can skip all of this.
+	 */
+	if (!ESDFS_RESTRICT_PERMS(ESDFS_SB(inode->i_sb)) &&
+	    !test_opt(ESDFS_SB(inode->i_sb), DERIVE_CONFINE))
+			return 0;
+
+	cred = current_cred();
+	uid = from_kuid(&init_user_ns, cred->uid);
+	appid = uid % PKG_APPID_PER_USER;
+
+	/* Reads, owners, and root are always granted access */
+	if (!(mask & (MAY_WRITE | ESDFS_MAY_CREATE)) ||
+	    uid == 0 || uid_eq(cred->uid, inode->i_uid))
+		return 0;
+
+	/*
+	 * Grant access to sdcard_rw holders, unless we are in unified mode
+	 * and we are trying to write to the protected /Android tree or to
+	 * create files in the root (aka, "confined" access).
+	 */
+	if ((!test_opt(ESDFS_SB(inode->i_sb), DERIVE_UNIFIED) ||
+	     (ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_DOWNLOAD &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_DATA &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_OBB &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_MEDIA &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_APP &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_APP_CACHE &&
+	      (ESDFS_I(inode)->tree != ESDFS_TREE_ROOT ||
+	       !(mask & ESDFS_MAY_CREATE)))))
+		return 0;
+
+	pr_debug("esdfs: %s: denying access to appid: %u\n", __func__, appid);
+	return -EACCES;
+}
+
+static gid_t get_type(struct esdfs_sb_info *sbi, const char *name)
+{
+	const char *ext = strrchr(name, '.');
+	kgid_t id;
+
+	if (ext && ext[0]) {
+		ext = &ext[1];
+		id = pkglist_get_ext_gid(ext);
+		return gid_valid(id)?esdfs_from_kgid(sbi, id):AID_MEDIA_RW;
+	}
+	return AID_MEDIA_RW;
+}
+
+static kuid_t esdfs_get_derived_lower_uid(struct esdfs_sb_info *sbi,
+				struct esdfs_inode_info *info)
+{
+	uid_t uid = sbi->lower_perms.uid;
+	int perm;
+
+	perm = info->tree;
+	if (info->under_obb)
+		perm = ESDFS_TREE_ANDROID_OBB;
+
+	switch (perm) {
+	case ESDFS_TREE_DOWNLOAD:
+		if (test_opt(sbi, SPECIAL_DOWNLOAD))
+			return make_kuid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_uid);
+		fallthrough;
+	case ESDFS_TREE_ROOT:
+	case ESDFS_TREE_MEDIA:
+	case ESDFS_TREE_ANDROID:
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_MEDIA:
+	case ESDFS_TREE_ANDROID_APP:
+	case ESDFS_TREE_ANDROID_APP_CACHE:
+		uid = derive_uid(info, uid);
+		break;
+	case ESDFS_TREE_ANDROID_OBB:
+		uid = AID_MEDIA_OBB;
+		break;
+	case ESDFS_TREE_ROOT_LEGACY:
+	default:
+		break;
+	}
+	return esdfs_make_kuid(sbi, uid);
+}
+
+static kgid_t esdfs_get_derived_lower_gid(struct esdfs_sb_info *sbi,
+				struct esdfs_inode_info *info, const char *name)
+{
+	gid_t gid = sbi->lower_perms.gid;
+	uid_t upper_uid;
+	int perm;
+
+	upper_uid = esdfs_i_uid_read(&info->vfs_inode);
+	perm = info->tree;
+	if (info->under_obb)
+		perm = ESDFS_TREE_ANDROID_OBB;
+
+	switch (perm) {
+	case ESDFS_TREE_DOWNLOAD:
+		if (test_opt(sbi, SPECIAL_DOWNLOAD))
+			return make_kgid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_gid);
+		fallthrough;
+	case ESDFS_TREE_ROOT:
+	case ESDFS_TREE_MEDIA:
+	case ESDFS_TREE_ANDROID:
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_MEDIA:
+		if (S_ISDIR(info->vfs_inode.i_mode))
+			gid = derive_uid(info, AID_MEDIA_RW);
+		else
+			gid = derive_uid(info, get_type(sbi, name));
+		break;
+	case ESDFS_TREE_ANDROID_OBB:
+		gid = AID_MEDIA_OBB;
+		break;
+	case ESDFS_TREE_ANDROID_APP:
+		if (uid_is_app(upper_uid))
+			gid = multiuser_get_ext_gid(upper_uid);
+		else
+			gid = derive_uid(info, AID_MEDIA_RW);
+		break;
+	case ESDFS_TREE_ANDROID_APP_CACHE:
+		if (uid_is_app(upper_uid))
+			gid = multiuser_get_ext_cache_gid(upper_uid);
+		else
+			gid = derive_uid(info, AID_MEDIA_RW);
+		break;
+	case ESDFS_TREE_ROOT_LEGACY:
+	default:
+		break;
+	}
+	return esdfs_make_kgid(sbi, gid);
+}
+
+void esdfs_derive_lower_ownership(struct dentry *dentry, const char *name)
+{
+	struct path path;
+	struct inode *inode;
+	struct inode *delegated_inode = NULL;
+	int error;
+	struct esdfs_sb_info *sbi = ESDFS_SB(dentry->d_sb);
+	struct esdfs_inode_info *info = ESDFS_I(dentry->d_inode);
+	kuid_t kuid;
+	kgid_t kgid;
+	struct iattr newattrs;
+
+	if (!test_opt(sbi, GID_DERIVATION))
+		return;
+
+	esdfs_get_lower_path(dentry, &path);
+	inode = path.dentry->d_inode;
+	kuid = esdfs_get_derived_lower_uid(sbi, info);
+	kgid = esdfs_get_derived_lower_gid(sbi, info, name);
+	if (!gid_eq(path.dentry->d_inode->i_gid, kgid)
+		|| !uid_eq(path.dentry->d_inode->i_uid, kuid)) {
+retry_deleg:
+		newattrs.ia_valid = ATTR_GID | ATTR_UID | ATTR_FORCE;
+		newattrs.ia_uid = kuid;
+		newattrs.ia_gid = kgid;
+		if (!S_ISDIR(inode->i_mode))
+			newattrs.ia_valid |= ATTR_KILL_SUID | ATTR_KILL_SGID
+						| ATTR_KILL_PRIV;
+		inode_lock(inode);
+		error = security_path_chown(&path, newattrs.ia_uid,
+						newattrs.ia_gid);
+		if (!error)
+			error = notify_change(&nop_mnt_idmap, path.dentry,
+						&newattrs, &delegated_inode);
+		inode_unlock(inode);
+		if (delegated_inode) {
+			error = break_deleg_wait(&delegated_inode);
+			if (!error)
+				goto retry_deleg;
+		}
+		if (error)
+			pr_debug("esdfs: Failed to touch up lower fs gid/uid for %s\n", name);
+	}
+	esdfs_put_lower_path(dentry, &path);
+}
+
+/*
+ * The sdcard service has a hack that creates .nomedia files along certain
+ * paths to stop MediaScanner.  Create those here.
+ */
+int esdfs_derive_mkdir_contents(struct dentry *dir_dentry)
+{
+	struct esdfs_inode_info *inode_i;
+	struct qstr nomedia;
+	struct dentry *lower_dentry;
+	struct path lower_dir_path, lower_path;
+	struct dentry *lower_parent_dentry = NULL;
+	umode_t mode;
+	int err = 0;
+	const struct cred *creds;
+	int mask = 0;
+
+	if (!dir_dentry->d_inode)
+		return 0;
+
+	inode_i = ESDFS_I(dir_dentry->d_inode);
+
+	/*
+	 * Only create .nomedia in Android/data and Android/obb, but never in
+	 * pseudo link stubs.
+	 */
+	if ((inode_i->tree != ESDFS_TREE_ANDROID_DATA &&
+	     inode_i->tree != ESDFS_TREE_ANDROID_OBB) ||
+	    (ESDFS_INODE_CAN_LINK(dir_dentry->d_inode) &&
+	     ESDFS_DENTRY_NEEDS_LINK(dir_dentry) &&
+	     !ESDFS_DENTRY_IS_LINKED(dir_dentry)))
+		return 0;
+
+	esdfs_get_lower_path(dir_dentry, &lower_dir_path);
+
+	nomedia.name = ".nomedia";
+	nomedia.len = strlen(nomedia.name);
+	nomedia.hash = full_name_hash(lower_dir_path.dentry, nomedia.name,
+				      nomedia.len);
+
+	/* check if lower has its own hash */
+	if (lower_dir_path.dentry->d_flags & DCACHE_OP_HASH)
+		lower_dir_path.dentry->d_op->d_hash(lower_dir_path.dentry,
+							&nomedia);
+
+	creds = esdfs_override_creds(ESDFS_SB(dir_dentry->d_sb),
+					inode_i, &mask);
+	/* See if the lower file is there already. */
+	err = vfs_path_lookup(lower_dir_path.dentry, lower_dir_path.mnt,
+			      nomedia.name, 0, &lower_path);
+	if (!err)
+		path_put(&lower_path);
+	/* If it's there or there was an error, we're done */
+	if (!err || err != -ENOENT)
+		goto out;
+
+	/* The lower file is not there.  See if the dentry is in the cache. */
+	lower_dentry = d_lookup(lower_dir_path.dentry, &nomedia);
+	if (!lower_dentry) {
+		/* It's not there, so create a negative lower dentry. */
+		lower_dentry = d_alloc(lower_dir_path.dentry, &nomedia);
+		if (!lower_dentry) {
+			err = -ENOMEM;
+			goto out;
+		}
+		d_add(lower_dentry, NULL);
+	}
+
+	/* Now create the lower file. */
+	mode = S_IFREG;
+	lower_parent_dentry = lock_parent(lower_dentry);
+	esdfs_set_lower_mode(ESDFS_SB(dir_dentry->d_sb), inode_i, &mode);
+	err = vfs_create(&nop_mnt_idmap, lower_dir_path.dentry->d_inode,
+			 lower_dentry, mode, true);
+	unlock_dir(lower_parent_dentry);
+	dput(lower_dentry);
+
+out:
+	esdfs_put_lower_path(dir_dentry, &lower_dir_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
Index: kernel-rpi-6_6/fs/esdfs/esdfs.h
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/esdfs/esdfs.h
@@ -0,0 +1,631 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef _ESDFS_H_
+#define _ESDFS_H_
+
+#include <linux/dcache.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/iversion.h>
+#include <linux/aio.h>
+#include <linux/mm.h>
+#include <linux/mount.h>
+#include <uapi/linux/mount.h>
+#include <linux/namei.h>
+#include <linux/seq_file.h>
+#include <linux/statfs.h>
+#include <linux/fs_stack.h>
+#include <linux/magic.h>
+#include <linux/uaccess.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/fs_struct.h>
+#include <linux/uidgid.h>
+#include <linux/user_namespace.h>
+#include <linux/pkglist.h>
+#include <linux/pagemap.h>
+#include <linux/splice.h>
+#include <linux/proc_ns.h>
+#include <linux/security.h>
+
+#include "../internal.h"
+
+/* the file system name */
+#define ESDFS_NAME "esdfs"
+
+/* ioctl command */
+#define ESDFS_IOCTL_MAGIC	'e'
+#define ESDFS_IOC_DIS_ACCESS	_IO(ESDFS_IOCTL_MAGIC, 1)
+
+/* esdfs root inode number */
+#define ESDFS_ROOT_INO     1
+
+/* useful for tracking code reachability */
+#define UDBG printk(KERN_DEFAULT "DBG:%s:%s:%d\n", __FILE__, __func__, __LINE__)
+
+/* mount options */
+#define ESDFS_MOUNT_DERIVE_LEGACY	0x00000001
+#define ESDFS_MOUNT_DERIVE_UNIFIED	0x00000002
+#define ESDFS_MOUNT_DERIVE_MULTI	0x00000004
+#define ESDFS_MOUNT_DERIVE_PUBLIC	0x00000008
+#define ESDFS_MOUNT_DERIVE_CONFINE	0x00000010
+#define ESDFS_MOUNT_ACCESS_DISABLE	0x00000020
+#define ESDFS_MOUNT_GID_DERIVATION	0x00000040
+#define ESDFS_MOUNT_DEFAULT_NORMAL	0x00000080
+#define ESDFS_MOUNT_SPECIAL_DOWNLOAD	0x00000100
+
+#define clear_opt(sbi, option)	(sbi->options &= ~ESDFS_MOUNT_##option)
+#define set_opt(sbi, option)	(sbi->options |= ESDFS_MOUNT_##option)
+#define test_opt(sbi, option)	(sbi->options & ESDFS_MOUNT_##option)
+
+#define ESDFS_DERIVE_PERMS(sbi)	(test_opt(sbi, DERIVE_UNIFIED) || \
+				 test_opt(sbi, DERIVE_LEGACY))
+#define ESDFS_RESTRICT_PERMS(sbi) (ESDFS_DERIVE_PERMS(sbi) && \
+				   !test_opt(sbi, DERIVE_PUBLIC) && \
+				   !test_opt(sbi, DERIVE_MULTI))
+
+/* from android_filesystem_config.h */
+#define AID_ROOT             0
+#define AID_SDCARD_RW     1015
+#define AID_MEDIA_RW      1023
+#define AID_SDCARD_R      1028
+#define AID_SDCARD_PICS   1033
+#define AID_SDCARD_AV     1034
+#define AID_SDCARD_ALL    1035
+#define AID_MEDIA_OBB     1059
+
+/* used in extra persmission check during file creation */
+#define ESDFS_MAY_CREATE	0x00001000
+
+/* derived permissions model based on tree location */
+enum {
+	ESDFS_TREE_NONE = 0,		/* permissions not derived */
+	ESDFS_TREE_ROOT_LEGACY,		/* root for legacy emulated storage */
+	ESDFS_TREE_ROOT,		/* root for a user */
+	ESDFS_TREE_MEDIA,		/* per-user basic permissions */
+	ESDFS_TREE_DOWNLOAD,		/* .../Download */
+	ESDFS_TREE_ANDROID,		/* .../Android */
+	ESDFS_TREE_ANDROID_DATA,	/* .../Android/data */
+	ESDFS_TREE_ANDROID_OBB,		/* .../Android/obb */
+	ESDFS_TREE_ANDROID_MEDIA,	/* .../Android/media */
+	ESDFS_TREE_ANDROID_APP,		/* .../Android/data|obb|media/... */
+	ESDFS_TREE_ANDROID_APP_CACHE,	/* .../Android/data|obb|media/.../cache */
+	ESDFS_TREE_ANDROID_USER,	/* .../Android/user */
+};
+
+/* for permissions table lookups */
+enum {
+	ESDFS_PERMS_LOWER_DEFAULT = 0,
+	ESDFS_PERMS_UPPER_LEGACY,
+	ESDFS_PERMS_UPPER_DERIVED,
+	ESDFS_PERMS_LOWER_DOWNLOAD,
+	ESDFS_PERMS_TABLE_SIZE
+
+};
+
+#define PKG_NAME_MAX		128
+#define PKG_APPID_PER_USER	100000
+#define AID_APP_START		10000 /* first app user */
+#define AID_APP_END		19999 /* last app user */
+#define AID_CACHE_GID_START	20000 /* start of gids for apps to mark cached data */
+#define AID_EXT_GID_START	30000 /* start of gids for apps to mark external data */
+#define AID_EXT_CACHE_GID_START	40000 /* start of gids for apps to mark external cached data */
+#define AID_EXT_CACHE_GID_END	49999 /* end of gids for apps to mark external cached data */
+#define AID_SHARED_GID_START	50000 /* start of gids for apps in each user to share */
+#define PKG_APPID_MIN		1000
+#define PKG_APPID_MAX		(PKG_APPID_PER_USER - 1)
+
+/* operations vectors defined in specific files */
+extern const struct file_operations esdfs_main_fops;
+extern const struct file_operations esdfs_dir_fops;
+extern const struct inode_operations esdfs_main_iops;
+extern const struct inode_operations esdfs_dir_iops;
+extern const struct inode_operations esdfs_symlink_iops;
+extern const struct super_operations esdfs_sops;
+extern const struct dentry_operations esdfs_dops;
+extern const struct address_space_operations esdfs_aops, esdfs_dummy_aops;
+extern const struct vm_operations_struct esdfs_vm_ops;
+
+extern void esdfs_msg(struct super_block *, const char *, const char *, ...);
+extern int esdfs_init_inode_cache(void);
+extern void esdfs_destroy_inode_cache(void);
+extern int esdfs_init_dentry_cache(void);
+extern void esdfs_destroy_dentry_cache(void);
+extern int esdfs_new_dentry_private_data(struct dentry *dentry);
+extern void esdfs_free_dentry_private_data(struct dentry *dentry);
+extern struct dentry *esdfs_lookup(struct inode *dir, struct dentry *dentry,
+				   unsigned int flags);
+extern struct inode *esdfs_iget(struct super_block *sb,
+				struct inode *lower_inode,
+				uint32_t id);
+extern int esdfs_interpose(struct dentry *dentry, struct super_block *sb,
+			   struct path *lower_path, uint32_t id);
+extern int esdfs_init_package_list(void);
+extern void esdfs_destroy_package_list(void);
+extern void esdfs_derive_perms(struct dentry *dentry);
+extern void esdfs_set_derived_perms(struct inode *inode);
+extern int esdfs_is_dl_lookup(struct dentry *dentry, struct dentry *parent);
+extern int esdfs_derived_lookup(struct dentry *dentry, struct dentry **parent);
+extern int esdfs_derived_revalidate(struct dentry *dentry,
+				    struct dentry *parent);
+extern int esdfs_check_derived_permission(struct inode *inode, int mask);
+extern int esdfs_derive_mkdir_contents(struct dentry *dentry);
+extern int esdfs_lookup_nocase(struct path *lower_parent_path,
+		const struct qstr *name, struct path *lower_path);
+
+/* file private data */
+struct esdfs_file_info {
+	struct file *lower_file;
+	const struct vm_operations_struct *lower_vm_ops;
+};
+
+struct esdfs_perms {
+	uid_t raw_uid;
+	uid_t raw_gid;
+	uid_t uid;
+	gid_t gid;
+	unsigned short fmask;
+	unsigned short dmask;
+};
+
+/* esdfs inode data in memory */
+struct esdfs_inode_info {
+	struct inode *lower_inode;
+	struct inode vfs_inode;
+	unsigned version;	/* package list version this was derived from */
+	int tree;		/* storage tree location */
+	uint32_t userid;	/* Android User ID (not Linux UID) */
+	uid_t appid;		/* Linux UID for this app/user combo */
+	bool under_obb;
+};
+
+/* esdfs dentry data in memory */
+struct esdfs_dentry_info {
+	spinlock_t lock;	/* protects lower_path and lower_stub_path */
+	struct path lower_path;
+	struct path lower_stub_path;
+	struct dentry *real_parent;
+};
+
+/* esdfs super-block data in memory */
+struct esdfs_sb_info {
+	struct super_block *lower_sb;
+	struct super_block *s_sb;
+	struct user_namespace *base_ns;
+	struct list_head s_list;
+	struct esdfs_perms lower_perms;
+	struct esdfs_perms upper_perms;	   /* root in derived mode */
+	struct dentry *obb_parent;	   /* pinned dentry for obb link parent */
+	struct path dl_path;		   /* path of lower downloads folder */
+	struct qstr dl_name;		   /* name of lower downloads folder */
+	const char *dl_loc;		   /* location of dl folder */
+	struct esdfs_perms lower_dl_perms; /* permissions for lower downloads folder */
+	struct user_namespace *dl_ns;	   /* lower downloads namespace */
+	int ns_fd;
+	unsigned int options;
+};
+
+extern struct esdfs_perms esdfs_perms_table[ESDFS_PERMS_TABLE_SIZE];
+extern unsigned esdfs_package_list_version;
+
+void esdfs_add_super(struct esdfs_sb_info *, struct super_block *);
+void esdfs_truncate_share(struct super_block *, struct inode *, loff_t newsize);
+
+void esdfs_derive_lower_ownership(struct dentry *dentry, const char *name);
+
+static inline bool is_obb(struct qstr *name)
+{
+	struct qstr q_obb = QSTR_LITERAL("obb");
+	return qstr_case_eq(name, &q_obb);
+}
+
+static inline bool is_dl(struct qstr *name)
+{
+	struct qstr q_dl = QSTR_LITERAL("Download");
+
+	return qstr_case_eq(name, &q_dl);
+}
+
+#define ESDFS_INODE_IS_STALE(i) ((i)->version != esdfs_package_list_version)
+#define ESDFS_INODE_CAN_LINK(i) (test_opt(ESDFS_SB((i)->i_sb), \
+					  DERIVE_LEGACY) || \
+				 (test_opt(ESDFS_SB((i)->i_sb), \
+					   DERIVE_UNIFIED) && \
+				  ESDFS_I(i)->userid > 0))
+#define ESDFS_DENTRY_NEEDS_LINK(d) (is_obb(&(d)->d_name))
+#define ESDFS_DENTRY_NEEDS_DL_LINK(d) (is_dl(&(d)->d_name))
+#define ESDFS_DENTRY_IS_LINKED(d) (ESDFS_D(d)->real_parent)
+#define ESDFS_DENTRY_HAS_STUB(d) (ESDFS_D(d)->lower_stub_path.dentry)
+
+/*
+ * inode to private data
+ *
+ * Since we use containers and the struct inode is _inside_ the
+ * esdfs_inode_info structure, ESDFS_I will always (given a non-NULL
+ * inode pointer), return a valid non-NULL pointer.
+ */
+static inline struct esdfs_inode_info *ESDFS_I(const struct inode *inode)
+{
+	return container_of(inode, struct esdfs_inode_info, vfs_inode);
+}
+
+/* dentry to private data */
+#define ESDFS_D(dent) ((struct esdfs_dentry_info *)(dent)->d_fsdata)
+
+/* superblock to private data */
+#define ESDFS_SB(super) ((struct esdfs_sb_info *)(super)->s_fs_info)
+
+/* file to private Data */
+#define ESDFS_F(file) ((struct esdfs_file_info *)((file)->private_data))
+
+/* file to lower file */
+static inline struct file *esdfs_lower_file(const struct file *f)
+{
+	return ESDFS_F(f)->lower_file;
+}
+
+static inline void esdfs_set_lower_file(struct file *f, struct file *val)
+{
+	ESDFS_F(f)->lower_file = val;
+}
+
+/* inode to lower inode. */
+static inline struct inode *esdfs_lower_inode(const struct inode *i)
+{
+	return ESDFS_I(i)->lower_inode;
+}
+
+static inline void esdfs_set_lower_inode(struct inode *i, struct inode *val)
+{
+	ESDFS_I(i)->lower_inode = val;
+}
+
+/* superblock to lower superblock */
+static inline struct super_block *esdfs_lower_super(
+	const struct super_block *sb)
+{
+	return ESDFS_SB(sb)->lower_sb;
+}
+
+static inline void esdfs_set_lower_super(struct super_block *sb,
+					  struct super_block *val)
+{
+	ESDFS_SB(sb)->lower_sb = val;
+}
+
+/* path based (dentry/mnt) macros */
+static inline void pathcpy(struct path *dst, const struct path *src)
+{
+	dst->dentry = src->dentry;
+	dst->mnt = src->mnt;
+}
+/* Returns struct path.  Caller must path_put it. */
+static inline void esdfs_get_lower_path(const struct dentry *dent,
+					 struct path *lower_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(lower_path, &ESDFS_D(dent)->lower_path);
+	path_get(lower_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_get_lower_stub_path(const struct dentry *dent,
+					     struct path *lower_stub_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(lower_stub_path, &ESDFS_D(dent)->lower_stub_path);
+	path_get(lower_stub_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_put_lower_path(const struct dentry *dent,
+					 struct path *lower_path)
+{
+	path_put(lower_path);
+}
+static inline void esdfs_set_lower_path(const struct dentry *dent,
+					 struct path *lower_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(&ESDFS_D(dent)->lower_path, lower_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_set_lower_stub_path(const struct dentry *dent,
+					     struct path *lower_stub_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(&ESDFS_D(dent)->lower_stub_path, lower_stub_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_put_reset_lower_paths(const struct dentry *dent)
+{
+	struct path lower_path;
+	struct path lower_stub_path = {
+		.mnt = NULL,
+		.dentry = NULL,
+	};
+
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(&lower_path, &ESDFS_D(dent)->lower_path);
+	ESDFS_D(dent)->lower_path.dentry = NULL;
+	ESDFS_D(dent)->lower_path.mnt = NULL;
+	if (ESDFS_DENTRY_HAS_STUB(dent)) {
+		pathcpy(&lower_stub_path, &ESDFS_D(dent)->lower_stub_path);
+		ESDFS_D(dent)->lower_stub_path.dentry = NULL;
+		ESDFS_D(dent)->lower_stub_path.mnt = NULL;
+	}
+	spin_unlock(&ESDFS_D(dent)->lock);
+
+	path_put(&lower_path);
+	if (lower_stub_path.dentry)
+		path_put(&lower_stub_path);
+}
+static inline void esdfs_get_lower_parent(const struct dentry *dent,
+					  struct dentry *lower_dentry,
+					  struct dentry **lower_parent)
+{
+	*lower_parent = NULL;
+	spin_lock(&ESDFS_D(dent)->lock);
+	if (ESDFS_DENTRY_IS_LINKED(dent)) {
+		*lower_parent = ESDFS_D(dent)->real_parent;
+		dget(*lower_parent);
+	}
+	spin_unlock(&ESDFS_D(dent)->lock);
+	if (!*lower_parent)
+		*lower_parent = dget_parent(lower_dentry);
+}
+static inline void esdfs_put_lower_parent(const struct dentry *dent,
+					  struct dentry **lower_parent)
+{
+	dput(*lower_parent);
+}
+static inline void esdfs_set_lower_parent(const struct dentry *dent,
+					  struct dentry *parent)
+{
+	struct dentry *old_parent = NULL;
+
+	spin_lock(&ESDFS_D(dent)->lock);
+	if (ESDFS_DENTRY_IS_LINKED(dent))
+		old_parent = ESDFS_D(dent)->real_parent;
+	ESDFS_D(dent)->real_parent = parent;
+	dget(parent);	/* pin the lower parent */
+	spin_unlock(&ESDFS_D(dent)->lock);
+	if (old_parent)
+		dput(old_parent);
+}
+static inline void esdfs_release_lower_parent(const struct dentry *dent)
+{
+	struct dentry *real_parent = NULL;
+
+	spin_lock(&ESDFS_D(dent)->lock);
+	if (ESDFS_DENTRY_IS_LINKED(dent)) {
+		real_parent = ESDFS_D(dent)->real_parent;
+		ESDFS_D(dent)->real_parent = NULL;
+	}
+	spin_unlock(&ESDFS_D(dent)->lock);
+	if (real_parent)
+		dput(real_parent);
+}
+
+/* locking helpers */
+static inline struct dentry *lock_parent(struct dentry *dentry)
+{
+	struct dentry *dir = dget_parent(dentry);
+
+	inode_lock_nested(dir->d_inode, I_MUTEX_PARENT);
+	return dir;
+}
+
+static inline void unlock_dir(struct dentry *dir)
+{
+	inode_unlock(dir->d_inode);
+	dput(dir);
+}
+
+static inline void esdfs_set_lower_mode(struct esdfs_sb_info *sbi,
+		struct esdfs_inode_info *inode_i, umode_t *mode)
+{
+	struct esdfs_perms *perms = &sbi->lower_perms;
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD) &&
+			inode_i->tree == ESDFS_TREE_DOWNLOAD)
+		perms = &sbi->lower_dl_perms;
+
+	if (S_ISDIR(*mode))
+		*mode = (*mode & S_IFMT) | perms->dmask;
+	else
+		*mode = (*mode & S_IFMT) | perms->fmask;
+}
+
+static inline void esdfs_set_perms(struct inode *inode)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(inode->i_sb);
+
+	if (ESDFS_DERIVE_PERMS(sbi)) {
+		esdfs_set_derived_perms(inode);
+		return;
+	}
+	i_uid_write(inode, sbi->upper_perms.uid);
+	i_gid_write(inode, sbi->upper_perms.gid);
+	if (S_ISDIR(inode->i_mode))
+		inode->i_mode = (inode->i_mode & S_IFMT) |
+				sbi->upper_perms.dmask;
+	else
+		inode->i_mode = (inode->i_mode & S_IFMT) |
+				sbi->upper_perms.fmask;
+}
+
+static inline void esdfs_revalidate_perms(struct dentry *dentry)
+{
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(dentry->d_sb)) &&
+	    dentry->d_inode &&
+	    ESDFS_INODE_IS_STALE(ESDFS_I(dentry->d_inode))) {
+		esdfs_derive_perms(dentry);
+		esdfs_set_perms(dentry->d_inode);
+	}
+}
+
+static inline uid_t derive_uid(struct esdfs_inode_info *inode_i, uid_t uid)
+{
+	return inode_i->userid * PKG_APPID_PER_USER +
+	       (uid % PKG_APPID_PER_USER);
+}
+
+static inline bool uid_is_app(uid_t uid)
+{
+	uid_t appid = uid % PKG_APPID_PER_USER;
+
+	return appid >= AID_APP_START && appid <= AID_APP_END;
+}
+
+static inline gid_t multiuser_get_ext_cache_gid(uid_t uid)
+{
+	return uid - AID_APP_START + AID_EXT_CACHE_GID_START;
+}
+
+static inline gid_t multiuser_get_ext_gid(uid_t uid)
+{
+	return uid - AID_APP_START + AID_EXT_GID_START;
+}
+
+/* file attribute helpers */
+static inline void esdfs_copy_lower_attr(struct inode *dest,
+					 const struct inode *src)
+{
+	dest->i_mode = src->i_mode & S_IFMT;
+	dest->i_rdev = src->i_rdev;
+	dest->i_atime = src->i_atime;
+	dest->i_mtime = src->i_mtime;
+  inode_set_ctime_to_ts(dest, inode_get_ctime(src));
+	dest->i_blkbits = src->i_blkbits;
+	dest->i_flags = src->i_flags;
+	set_nlink(dest, src->i_nlink);
+}
+
+static inline void esdfs_copy_attr(struct inode *dest, const struct inode *src)
+{
+	esdfs_copy_lower_attr(dest, src);
+	esdfs_set_perms(dest);
+}
+
+static inline uid_t esdfs_from_local_uid(struct esdfs_sb_info *sbi, uid_t uid)
+{
+	return from_kuid(sbi->base_ns, make_kuid(current_user_ns(), uid));
+}
+
+static inline gid_t esdfs_from_local_gid(struct esdfs_sb_info *sbi, gid_t gid)
+{
+	return from_kgid(sbi->base_ns, make_kgid(current_user_ns(), gid));
+}
+
+static inline uid_t esdfs_from_kuid(struct esdfs_sb_info *sbi, kuid_t uid)
+{
+	return from_kuid(sbi->base_ns, uid);
+}
+
+static inline gid_t esdfs_from_kgid(struct esdfs_sb_info *sbi, kgid_t gid)
+{
+	return from_kgid(sbi->base_ns, gid);
+}
+
+static inline kuid_t esdfs_make_kuid(struct esdfs_sb_info *sbi, uid_t uid)
+{
+	return make_kuid(sbi->base_ns, uid);
+}
+
+static inline kgid_t esdfs_make_kgid(struct esdfs_sb_info *sbi, gid_t gid)
+{
+	return make_kgid(sbi->base_ns, gid);
+}
+
+/* Helper functions to read and write to inode uid/gids without
+ * having to worry about translating into/out of esdfs's preferred
+ * base user namespace.
+ */
+static inline uid_t esdfs_i_uid_read(const struct inode *inode)
+{
+	return esdfs_from_kuid(ESDFS_SB(inode->i_sb), inode->i_uid);
+}
+
+static inline gid_t esdfs_i_gid_read(const struct inode *inode)
+{
+	return esdfs_from_kgid(ESDFS_SB(inode->i_sb), inode->i_gid);
+}
+
+static inline void esdfs_i_uid_write(struct inode *inode, uid_t uid)
+{
+	inode->i_uid = esdfs_make_kuid(ESDFS_SB(inode->i_sb), uid);
+}
+
+static inline void esdfs_i_gid_write(struct inode *inode, gid_t gid)
+{
+	inode->i_gid = esdfs_make_kgid(ESDFS_SB(inode->i_sb), gid);
+}
+
+/*
+ * Based on nfs4_save_creds() and nfs4_reset_creds() in nfsd/nfs4recover.c.
+ * Returns NULL if prepare_creds() could not allocate heap, otherwise
+ */
+static inline const struct cred *esdfs_override_creds(
+		struct esdfs_sb_info *sbi,
+		struct esdfs_inode_info *info, int *mask)
+{
+	struct cred *creds = prepare_creds();
+	uid_t uid;
+	gid_t gid = sbi->lower_perms.gid;
+
+	if (!creds)
+		return NULL;
+
+	/* clear the umask so that the lower mode works for create cases */
+	if (mask) {
+		*mask = 0;
+		*mask = xchg(&current->fs->umask, *mask & S_IRWXUGO);
+	}
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD) &&
+			info->tree == ESDFS_TREE_DOWNLOAD) {
+		creds->fsuid = make_kuid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_uid);
+		creds->fsgid = make_kgid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_gid);
+	} else {
+		if (test_opt(sbi, GID_DERIVATION)) {
+			if (info->under_obb)
+				uid = AID_MEDIA_OBB;
+			else
+				uid = derive_uid(info, sbi->lower_perms.uid);
+		} else {
+			uid = sbi->lower_perms.uid;
+		}
+		creds->fsuid = esdfs_make_kuid(sbi, uid);
+		creds->fsgid = esdfs_make_kgid(sbi, gid);
+	}
+
+	/* this installs the new creds into current, which we must destroy */
+	return override_creds(creds);
+}
+
+static inline void esdfs_revert_creds(const struct cred *creds, int *mask)
+{
+	const struct cred *current_creds = current->cred;
+
+	/* restore the old umask */
+	if (mask)
+		*mask = xchg(&current->fs->umask, *mask & S_IRWXUGO);
+
+	/* restore the old creds into current */
+	revert_creds(creds);
+	put_cred(current_creds);	/* destroy the old temporary creds */
+}
+
+#endif	/* not _ESDFS_H_ */
Index: kernel-rpi-6_6/fs/esdfs/file.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/esdfs/file.c
@@ -0,0 +1,490 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014, 2016 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+static ssize_t esdfs_read(struct file *file, char __user *buf,
+			   size_t count, loff_t *ppos)
+{
+	int err;
+	struct file *lower_file;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	err = vfs_read(lower_file, buf, count, ppos);
+	/* update our inode atime upon a successful lower read */
+	if (err >= 0)
+		fsstack_copy_attr_atime(dentry->d_inode,
+					file_inode(lower_file));
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static ssize_t esdfs_write(struct file *file, const char __user *buf,
+			    size_t count, loff_t *ppos)
+{
+	int err;
+
+	struct file *lower_file;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	err = vfs_write(lower_file, buf, count, ppos);
+	/* update our inode times+sizes upon a successful lower write */
+	if (err >= 0) {
+		fsstack_copy_inode_size(dentry->d_inode,
+					file_inode(lower_file));
+		esdfs_copy_attr(dentry->d_inode,
+				file_inode(lower_file));
+	}
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_readdir(struct file *file, struct dir_context *ctx)
+{
+	int err;
+	struct file *lower_file = NULL;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	err = iterate_dir(lower_file, ctx);
+	file->f_pos = lower_file->f_pos;
+	if (err >= 0)		/* copy the atime */
+		fsstack_copy_attr_atime(dentry->d_inode,
+					file_inode(lower_file));
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static long esdfs_unlocked_ioctl(struct file *file, unsigned int cmd,
+				  unsigned long arg)
+{
+	long err = -ENOTTY;
+	struct file *lower_file;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (cmd == ESDFS_IOC_DIS_ACCESS) {
+		if (!capable(CAP_SYS_ADMIN)) {
+			err = -EPERM;
+			goto out;
+		}
+		set_opt(sbi, ACCESS_DISABLE);
+		err = 0;
+		goto out;
+	}
+
+	lower_file = esdfs_lower_file(file);
+
+	/* XXX: use vfs_ioctl if/when VFS exports it */
+	if (!lower_file || !lower_file->f_op)
+		goto out;
+	if (lower_file->f_op->unlocked_ioctl)
+		err = lower_file->f_op->unlocked_ioctl(lower_file, cmd, arg);
+
+	/* some ioctls can change inode attributes (EXT2_IOC_SETFLAGS) */
+	if (!err)
+		esdfs_copy_attr(file->f_path.dentry->d_inode,
+				file_inode(lower_file));
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+#ifdef CONFIG_COMPAT
+static long esdfs_compat_ioctl(struct file *file, unsigned int cmd,
+				unsigned long arg)
+{
+	long err = -ENOTTY;
+	struct file *lower_file;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+
+	/* XXX: use vfs_ioctl if/when VFS exports it */
+	if (!lower_file || !lower_file->f_op)
+		goto out;
+	if (lower_file->f_op->compat_ioctl)
+		err = lower_file->f_op->compat_ioctl(lower_file, cmd, arg);
+
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+#endif
+
+static int esdfs_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	int err = 0;
+	bool willwrite;
+	struct file *lower_file;
+	const struct vm_operations_struct *saved_vm_ops = NULL;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	/* this might be deferred to mmap's writepage */
+	willwrite = ((vma->vm_flags | VM_SHARED | VM_WRITE) == vma->vm_flags);
+
+	/*
+	 * File systems which do not implement ->writepage may use
+	 * generic_file_readonly_mmap as their ->mmap op.  If you call
+	 * generic_file_readonly_mmap with VM_WRITE, you'd get an -EINVAL.
+	 * But we cannot call the lower ->mmap op, so we can't tell that
+	 * writeable mappings won't work.  Therefore, our only choice is to
+	 * check if the lower file system supports the ->writepage, and if
+	 * not, return EINVAL (the same error that
+	 * generic_file_readonly_mmap returns in that case).
+	 */
+	lower_file = esdfs_lower_file(file);
+	if (willwrite && !lower_file->f_mapping->a_ops->writepage) {
+		err = -EINVAL;
+		esdfs_msg(file->f_mapping->host->i_sb, KERN_INFO,
+			"lower file system does not support writeable mmap\n");
+		goto out;
+	}
+
+	/*
+	 * find and save lower vm_ops.
+	 *
+	 * XXX: the VFS should have a cleaner way of finding the lower vm_ops
+	 */
+	if (!ESDFS_F(file)->lower_vm_ops) {
+		err = lower_file->f_op->mmap(lower_file, vma);
+		if (err) {
+			esdfs_msg(file->f_mapping->host->i_sb, KERN_ERR,
+				"lower mmap failed %d\n", err);
+			goto out;
+		}
+		saved_vm_ops = vma->vm_ops; /* save: came from lower ->mmap */
+	}
+
+	/*
+	 * Next 3 lines are all I need from generic_file_mmap.  I definitely
+	 * don't want its test for ->readpage which returns -ENOEXEC.
+	 */
+	file_accessed(file);
+	vma->vm_ops = &esdfs_vm_ops;
+
+	file->f_mapping->a_ops = &esdfs_aops; /* set our aops */
+	if (!ESDFS_F(file)->lower_vm_ops) /* save for our ->fault */
+		ESDFS_F(file)->lower_vm_ops = saved_vm_ops;
+
+	vma->vm_private_data = file;
+	get_file(lower_file);
+	vma->vm_file = lower_file;
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_open(struct inode *inode, struct file *file)
+{
+	int err = 0;
+	struct file *lower_file = NULL;
+	struct path lower_path;
+	struct esdfs_sb_info *sbi = ESDFS_SB(inode->i_sb);
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(inode->i_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(sbi, ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	/* don't open unhashed/deleted files */
+	if (d_unhashed(file->f_path.dentry)) {
+		err = -ENOENT;
+		goto out_err;
+	}
+
+	file->private_data =
+		kzalloc(sizeof(struct esdfs_file_info), GFP_KERNEL);
+	if (!ESDFS_F(file)) {
+		err = -ENOMEM;
+		goto out_err;
+	}
+
+	/* open lower object and link esdfs's file struct to lower's */
+	esdfs_get_lower_path(file->f_path.dentry, &lower_path);
+	lower_file = dentry_open(&lower_path, file->f_flags, current_cred());
+	path_put(&lower_path);
+	if (IS_ERR(lower_file)) {
+		err = PTR_ERR(lower_file);
+		lower_file = esdfs_lower_file(file);
+		if (lower_file) {
+			esdfs_set_lower_file(file, NULL);
+			fput(lower_file); /* fput calls dput for lower_dentry */
+		}
+	} else {
+		esdfs_set_lower_file(file, lower_file);
+	}
+
+	if (err)
+		kfree(ESDFS_F(file));
+	else
+		esdfs_copy_attr(inode, esdfs_lower_inode(inode));
+out_err:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_flush(struct file *file, fl_owner_t id)
+{
+	int err = 0;
+	struct file *lower_file = NULL;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	if (lower_file && lower_file->f_op && lower_file->f_op->flush) {
+		filemap_write_and_wait(file->f_mapping);
+		err = lower_file->f_op->flush(lower_file, id);
+	}
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/* release all lower object references & free the file info structure */
+static int esdfs_file_release(struct inode *inode, struct file *file)
+{
+	struct file *lower_file;
+
+	lower_file = esdfs_lower_file(file);
+	if (lower_file) {
+		esdfs_set_lower_file(file, NULL);
+		fput(lower_file);
+	}
+
+	kfree(ESDFS_F(file));
+	return 0;
+}
+
+static int esdfs_fsync(struct file *file, loff_t start, loff_t end,
+			int datasync)
+{
+	int err;
+	struct file *lower_file;
+	struct path lower_path;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	err = __generic_file_fsync(file, start, end, datasync);
+	if (err)
+		goto out;
+	lower_file = esdfs_lower_file(file);
+	esdfs_get_lower_path(dentry, &lower_path);
+	err = vfs_fsync_range(lower_file, start, end, datasync);
+	esdfs_put_lower_path(dentry, &lower_path);
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_fasync(int fd, struct file *file, int flag)
+{
+	int err = 0;
+	struct file *lower_file = NULL;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	if (lower_file->f_op && lower_file->f_op->fasync)
+		err = lower_file->f_op->fasync(fd, lower_file, flag);
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/*
+ * Wrapfs cannot use generic_file_llseek as ->llseek, because it would
+ * only set the offset of the upper file.  So we have to implement our
+ * own method to set both the upper and lower file offsets
+ * consistently.
+ */
+static loff_t esdfs_file_llseek(struct file *file, loff_t offset, int whence)
+{
+	int err;
+	struct file *lower_file;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+				ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	err = generic_file_llseek(file, offset, whence);
+	if (err < 0)
+		goto out;
+
+	lower_file = esdfs_lower_file(file);
+	err = generic_file_llseek(lower_file, offset, whence);
+
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/*
+ * Wrapfs read_iter, redirect modified iocb to lower read_iter
+ */
+static ssize_t
+esdfs_read_iter(struct kiocb *iocb, struct iov_iter *iter)
+{
+	int err;
+	struct file *file = iocb->ki_filp, *lower_file;
+
+	lower_file = esdfs_lower_file(file);
+	if (!lower_file->f_op->read_iter) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	get_file(lower_file); /* prevent lower_file from being released */
+	iocb->ki_filp = lower_file;
+	err = lower_file->f_op->read_iter(iocb, iter);
+	iocb->ki_filp = file;
+	fput(lower_file);
+	/* update upper inode atime as needed */
+	if (err >= 0 || err == -EIOCBQUEUED)
+		fsstack_copy_attr_atime(file->f_path.dentry->d_inode,
+					file_inode(lower_file));
+out:
+	return err;
+}
+
+/*
+ * Wrapfs write_iter, redirect modified iocb to lower write_iter
+ */
+static ssize_t
+esdfs_write_iter(struct kiocb *iocb, struct iov_iter *iter)
+{
+	int err;
+	struct file *file = iocb->ki_filp, *lower_file;
+
+	lower_file = esdfs_lower_file(file);
+	if (!lower_file->f_op->write_iter) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	get_file(lower_file); /* prevent lower_file from being released */
+	iocb->ki_filp = lower_file;
+	err = lower_file->f_op->write_iter(iocb, iter);
+	iocb->ki_filp = file;
+	fput(lower_file);
+	/* update upper inode times/sizes as needed */
+	if (err >= 0 || err == -EIOCBQUEUED) {
+		fsstack_copy_inode_size(file->f_path.dentry->d_inode,
+					file_inode(lower_file));
+		fsstack_copy_attr_times(file->f_path.dentry->d_inode,
+					file_inode(lower_file));
+	}
+out:
+	return err;
+}
+
+static ssize_t esdfs_splice_read(struct file *file, loff_t *ppos,
+              struct pipe_inode_info *pipe, size_t len,
+              unsigned int flags)
+{
+  int err;
+  struct file *lower_file;
+  struct dentry *dentry = file->f_path.dentry;
+  const struct cred *creds =
+    esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+      ESDFS_I(file->f_inode), NULL);
+  if (!creds)
+    return -ENOMEM;
+  lower_file = esdfs_lower_file(file);
+  err = vfs_splice_read(lower_file, ppos, pipe, len, flags);
+  esdfs_revert_creds(creds, NULL);
+  return err;
+}
+
+const struct file_operations esdfs_main_fops = {
+	.llseek		= generic_file_llseek,
+	.read		= esdfs_read,
+	.write		= esdfs_write,
+	.unlocked_ioctl	= esdfs_unlocked_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl	= esdfs_compat_ioctl,
+#endif
+	.mmap		= esdfs_mmap,
+	.open		= esdfs_open,
+	.flush		= esdfs_flush,
+	.release	= esdfs_file_release,
+	.fsync		= esdfs_fsync,
+	.fasync		= esdfs_fasync,
+	.read_iter	= esdfs_read_iter,
+	.write_iter	= esdfs_write_iter,
+	.splice_read    = esdfs_splice_read,
+	.splice_write   = iter_file_splice_write,
+};
+
+/* trimmed directory options */
+WRAP_DIR_ITER(esdfs_readdir) // FIXME!
+const struct file_operations esdfs_dir_fops = {
+	.llseek		= esdfs_file_llseek,
+	.read		= generic_read_dir,
+  .iterate_shared = shared_esdfs_readdir,
+	.unlocked_ioctl	= esdfs_unlocked_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl	= esdfs_compat_ioctl,
+#endif
+	.open		= esdfs_open,
+	.release	= esdfs_file_release,
+	.flush		= esdfs_flush,
+	.fsync		= esdfs_fsync,
+	.fasync		= esdfs_fasync,
+};
Index: kernel-rpi-6_6/fs/esdfs/inode.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/esdfs/inode.c
@@ -0,0 +1,550 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+#include <linux/fsnotify.h>
+
+static int esdfs_create(struct mnt_idmap *idmap, struct inode *dir,
+			struct dentry *dentry, umode_t mode, bool want_excl)
+{
+	int err;
+	struct dentry *lower_dentry;
+	struct dentry *lower_parent_dentry = NULL;
+	struct path lower_path;
+	struct inode *lower_inode;
+	int mask;
+	const struct cred *creds;
+
+	/*
+	 * Need to recheck derived permissions unified mode to prevent certain
+	 * applications from creating files at the root.
+	 */
+	if (test_opt(ESDFS_SB(dir->i_sb), DERIVE_UNIFIED) &&
+	    esdfs_check_derived_permission(dir, ESDFS_MAY_CREATE) != 0)
+		return -EACCES;
+
+	if (test_opt(ESDFS_SB(dir->i_sb), ACCESS_DISABLE))
+		return -ENOENT;
+
+	creds = esdfs_override_creds(ESDFS_SB(dir->i_sb), ESDFS_I(dir), &mask);
+	if (!creds)
+		return -ENOMEM;
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	lower_parent_dentry = lock_parent(lower_dentry);
+
+	esdfs_set_lower_mode(ESDFS_SB(dir->i_sb), ESDFS_I(dir), &mode);
+
+	lower_inode = esdfs_lower_inode(dir);
+	err = vfs_create(idmap, lower_inode, lower_dentry, mode,
+			 want_excl);
+	if (err)
+		goto out;
+
+	err = esdfs_interpose(dentry, dir->i_sb, &lower_path,
+				ESDFS_I(dir)->userid);
+	if (err)
+		goto out;
+	fsstack_copy_attr_times(dir, esdfs_lower_inode(dir));
+	fsstack_copy_inode_size(dir, lower_parent_dentry->d_inode);
+	esdfs_derive_lower_ownership(dentry, dentry->d_name.name);
+
+out:
+	unlock_dir(lower_parent_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
+
+static int esdfs_unlink(struct inode *dir, struct dentry *dentry)
+{
+	int err;
+	struct dentry *lower_dentry;
+	struct inode *lower_dir_inode;
+	struct dentry *lower_dir_dentry;
+	struct path lower_path;
+	const struct cred *creds;
+
+	creds = esdfs_override_creds(ESDFS_SB(dir->i_sb), ESDFS_I(dir), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(dir->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	dget(lower_dentry);
+
+	lower_dir_dentry = lock_parent(lower_dentry);
+
+	/* d_parent might be changed in vfs_rename */
+	if (lower_dir_dentry != lower_dentry->d_parent) {
+		err = -ENOENT;
+		goto out;
+	}
+
+	/* lower_dir_inode might be changed as well
+	 * get the new inode with new lower dir dentry
+	 */
+	lower_dir_inode = lower_dir_dentry->d_inode;
+
+	err = vfs_unlink(&nop_mnt_idmap, lower_dir_inode, lower_dentry, NULL);
+
+	/*
+	 * Note: unlinking on top of NFS can cause silly-renamed files.
+	 * Trying to delete such files results in EBUSY from NFS
+	 * below.  Silly-renamed files will get deleted by NFS later on, so
+	 * we just need to detect them here and treat such EBUSY errors as
+	 * if the upper file was successfully deleted.
+	 */
+	if (err == -EBUSY && lower_dentry->d_flags & DCACHE_NFSFS_RENAMED)
+		err = 0;
+	if (err)
+		goto out;
+	fsstack_copy_attr_times(dir, lower_dir_inode);
+	fsstack_copy_inode_size(dir, lower_dir_inode);
+	set_nlink(dentry->d_inode,
+		  esdfs_lower_inode(dentry->d_inode)->i_nlink);
+  inode_set_ctime_to_ts(dentry->d_inode, inode_get_ctime(dir));
+	d_drop(dentry); /* this is needed, else LTP fails (VFS won't do it) */
+out:
+	unlock_dir(lower_dir_dentry);
+	dput(lower_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_mkdir(struct mnt_idmap *idmap, struct inode *dir,
+		       struct dentry *dentry, umode_t mode)
+{
+	int err;
+	struct dentry *lower_dentry;
+	struct dentry *lower_parent_dentry = NULL;
+	struct path lower_path;
+	int mask;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dir->i_sb),
+					ESDFS_I(dir), &mask);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(dir->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	lower_parent_dentry = lock_parent(lower_dentry);
+
+	mode |= S_IFDIR;
+	esdfs_set_lower_mode(ESDFS_SB(dir->i_sb), ESDFS_I(dir), &mode);
+	err = vfs_mkdir(idmap, lower_parent_dentry->d_inode, lower_dentry,
+			mode);
+	if (err)
+		goto unlock_lower_parent;
+
+	err = esdfs_interpose(dentry, dir->i_sb, &lower_path,
+				ESDFS_I(dir)->userid);
+	if (err)
+		goto unlock_lower_parent;
+
+	fsstack_copy_attr_times(dir, esdfs_lower_inode(dir));
+	fsstack_copy_inode_size(dir, lower_parent_dentry->d_inode);
+	/* update number of links on parent directory */
+	set_nlink(dir, esdfs_lower_inode(dir)->i_nlink);
+	esdfs_derive_lower_ownership(dentry, dentry->d_name.name);
+
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(dir->i_sb))) {
+		unlock_dir(lower_parent_dentry);
+		err = esdfs_derive_mkdir_contents(dentry);
+		goto out;
+	}
+
+unlock_lower_parent:
+	unlock_dir(lower_parent_dentry);
+out:
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
+
+static int esdfs_rmdir(struct inode *dir, struct dentry *dentry)
+{
+	struct dentry *lower_dentry;
+	struct dentry *lower_dir_dentry;
+	int err;
+	struct path lower_path;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dir->i_sb),
+					ESDFS_I(dir), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	/* Never remove a pseudo link target.  Only the source. */
+	if (ESDFS_DENTRY_HAS_STUB(dentry))
+		esdfs_get_lower_stub_path(dentry, &lower_path);
+	else
+		esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+
+	lower_dir_dentry = lock_parent(lower_dentry);
+
+	/* d_parent might be changed in vfs_rename */
+	if (lower_dir_dentry != lower_dentry->d_parent) {
+		err = -ENOENT;
+		goto out;
+	}
+
+	err = vfs_rmdir(&nop_mnt_idmap, lower_dir_dentry->d_inode, lower_dentry);
+	if (err)
+		goto out;
+
+	d_drop(dentry);	/* drop our dentry on success (why not VFS's job?) */
+	if (dentry->d_inode)
+		clear_nlink(dentry->d_inode);
+	fsstack_copy_attr_times(dir, lower_dir_dentry->d_inode);
+	fsstack_copy_inode_size(dir, lower_dir_dentry->d_inode);
+	set_nlink(dir, lower_dir_dentry->d_inode->i_nlink);
+
+out:
+	unlock_dir(lower_dir_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/*
+ * The locking rules in esdfs_rename are complex.  We could use a simpler
+ * superblock-level name-space lock for renames and copy-ups.
+ */
+static int esdfs_rename(struct mnt_idmap *idmap,
+			struct inode *old_dir, struct dentry *old_dentry,
+			struct inode *new_dir, struct dentry *new_dentry,
+			unsigned int flags)
+{
+	int err = 0;
+	struct esdfs_sb_info *sbi = ESDFS_SB(old_dir->i_sb);
+	struct dentry *lower_old_dentry = NULL;
+	struct dentry *lower_new_dentry = NULL;
+	struct dentry *lower_old_dir_dentry = NULL;
+	struct dentry *lower_new_dir_dentry = NULL;
+	struct dentry *trap = NULL;
+	struct path lower_old_path, lower_new_path;
+	int mask;
+	const struct cred *creds;
+	struct renamedata rd;
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD)) {
+		if ((ESDFS_I(old_dir)->tree == ESDFS_TREE_DOWNLOAD
+			|| ESDFS_I(new_dir)->tree == ESDFS_TREE_DOWNLOAD)
+			&& ESDFS_I(old_dir)->tree != ESDFS_I(new_dir)->tree)
+			return -EXDEV;
+	}
+
+	if (test_opt(sbi, GID_DERIVATION)) {
+		if (ESDFS_I(old_dir)->userid != ESDFS_I(new_dir)->userid
+			|| ((ESDFS_I(old_dir)->under_obb
+			|| ESDFS_I(new_dir)->under_obb)
+			&& ESDFS_I(old_dir)->under_obb
+				!= ESDFS_I(new_dir)->under_obb))
+			return -EXDEV;
+	}
+	creds = esdfs_override_creds(sbi, ESDFS_I(new_dir), &mask);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(old_dir->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	/* Never rename to or from a pseudo hard link target. */
+	if (ESDFS_DENTRY_HAS_STUB(old_dentry))
+		esdfs_get_lower_stub_path(old_dentry, &lower_old_path);
+	else
+		esdfs_get_lower_path(old_dentry, &lower_old_path);
+	if (ESDFS_DENTRY_HAS_STUB(new_dentry))
+		esdfs_get_lower_stub_path(new_dentry, &lower_new_path);
+	else
+		esdfs_get_lower_path(new_dentry, &lower_new_path);
+	lower_old_dentry = lower_old_path.dentry;
+	lower_new_dentry = lower_new_path.dentry;
+	esdfs_get_lower_parent(old_dentry, lower_old_dentry,
+			       &lower_old_dir_dentry);
+	esdfs_get_lower_parent(new_dentry, lower_new_dentry,
+			       &lower_new_dir_dentry);
+
+	trap = lock_rename(lower_old_dir_dentry, lower_new_dir_dentry);
+	/* source should not be ancestor of target */
+	if (trap == lower_old_dentry) {
+		err = -EINVAL;
+		goto out;
+	}
+	/* target should not be ancestor of source */
+	if (trap == lower_new_dentry) {
+		err = -ENOTEMPTY;
+		goto out;
+	}
+
+	rd.old_mnt_idmap = idmap;
+	rd.old_dir = lower_old_dir_dentry->d_inode;
+	rd.old_dentry = lower_old_dentry;
+	rd.new_mnt_idmap = idmap;
+	rd.new_dir = lower_new_dir_dentry->d_inode;
+	rd.new_dentry = lower_new_dentry;
+	rd.flags = flags;
+
+	err = vfs_rename(&rd);
+	if (err)
+		goto out;
+
+	esdfs_copy_attr(new_dir, lower_new_dir_dentry->d_inode);
+	fsstack_copy_inode_size(new_dir, lower_new_dir_dentry->d_inode);
+	if (new_dir != old_dir) {
+		esdfs_copy_attr(old_dir,
+				      lower_old_dir_dentry->d_inode);
+		fsstack_copy_inode_size(old_dir,
+					lower_old_dir_dentry->d_inode);
+	}
+
+	/* Drop any old links */
+	if (ESDFS_DENTRY_HAS_STUB(old_dentry))
+		d_drop(old_dentry);
+	if (ESDFS_DENTRY_HAS_STUB(new_dentry))
+		d_drop(new_dentry);
+	esdfs_derive_lower_ownership(old_dentry, new_dentry->d_name.name);
+out:
+	unlock_rename(lower_old_dir_dentry, lower_new_dir_dentry);
+	esdfs_put_lower_parent(old_dentry, &lower_old_dir_dentry);
+	esdfs_put_lower_parent(new_dentry, &lower_new_dir_dentry);
+	esdfs_put_lower_path(old_dentry, &lower_old_path);
+	esdfs_put_lower_path(new_dentry, &lower_new_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
+
+static int esdfs_permission(struct mnt_idmap *idmap,
+			    struct inode *inode, int mask)
+{
+	struct inode *lower_inode;
+	int err;
+
+	/* First, check the upper permissions */
+	err = generic_permission(idmap, inode, mask);
+
+	/* Basic checking of the lower inode (can't override creds here) */
+	lower_inode = esdfs_lower_inode(inode);
+	if (S_ISSOCK(lower_inode->i_mode) ||
+	    S_ISLNK(lower_inode->i_mode) ||
+	    S_ISBLK(lower_inode->i_mode) ||
+	    S_ISCHR(lower_inode->i_mode) ||
+	    S_ISFIFO(lower_inode->i_mode))
+		err = -EACCES;
+
+	/* Finally, check the derived permissions */
+	if (!err && ESDFS_DERIVE_PERMS(ESDFS_SB(inode->i_sb)))
+		err = esdfs_check_derived_permission(inode, mask);
+
+	return err;
+}
+
+static int esdfs_setattr(struct mnt_idmap *idmap,
+			 struct dentry *dentry, struct iattr *ia)
+{
+	int err;
+	loff_t oldsize;
+	loff_t newsize;
+	struct dentry *lower_dentry;
+	struct inode *inode;
+	struct inode *lower_inode;
+	struct path lower_path;
+	struct iattr lower_ia;
+	const struct cred *creds;
+
+	/* We don't allow chmod or chown, so skip those */
+	ia->ia_valid &= ~(ATTR_UID | ATTR_GID | ATTR_MODE);
+	if (!ia->ia_valid)
+		return 0;
+	/* Allow touch updating timestamps. A previous permission check ensures
+	 * we have write access. Changes to mode, owner, and group are ignored
+	 */
+	ia->ia_valid |= ATTR_FORCE;
+
+	inode = dentry->d_inode;
+
+	if (test_opt(ESDFS_SB(inode->i_sb), ACCESS_DISABLE))
+		return -ENOENT;
+
+	/*
+	 * Check if user has permission to change inode.  We don't check if
+	 * this user can change the lower inode: that should happen when
+	 * calling notify_change on the lower inode.
+	 */
+	err = setattr_prepare(idmap, dentry, ia);
+	if (err)
+		return err;
+
+	creds = esdfs_override_creds(ESDFS_SB(dentry->d_inode->i_sb),
+				ESDFS_I(inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	lower_inode = esdfs_lower_inode(inode);
+
+	/* prepare our own lower struct iattr (with the lower file) */
+	memcpy(&lower_ia, ia, sizeof(lower_ia));
+	if (ia->ia_valid & ATTR_FILE)
+		lower_ia.ia_file = esdfs_lower_file(ia->ia_file);
+
+	/*
+	 * If shrinking, first truncate upper level to cancel writing dirty
+	 * pages beyond the new eof; and also if its' maxbytes is more
+	 * limiting (fail with -EFBIG before making any change to the lower
+	 * level).  There is no need to vmtruncate the upper level
+	 * afterwards in the other cases: we fsstack_copy_inode_size from
+	 * the lower level.
+	 */
+	if (ia->ia_valid & ATTR_SIZE) {
+		err = inode_newsize_ok(inode, ia->ia_size);
+		if (err)
+			goto out;
+		/*
+		 * i_size_write needs locking around it
+		 * otherwise i_size_read() may spin forever
+		 * (see include/linux/fs.h).
+		 * similar to function fsstack_copy_inode_size
+		 */
+		oldsize = i_size_read(inode);
+		newsize = ia->ia_size;
+
+#if BITS_PER_LONG == 32 && defined(CONFIG_SMP)
+		spin_lock(&inode->i_lock);
+#endif
+		i_size_write(inode, newsize);
+#if BITS_PER_LONG == 32 && defined(CONFIG_SMP)
+		spin_unlock(&inode->i_lock);
+#endif
+		if (newsize > oldsize)
+			pagecache_isize_extended(inode, oldsize, newsize);
+		truncate_pagecache(inode, newsize);
+		esdfs_truncate_share(inode->i_sb, lower_dentry->d_inode,
+					ia->ia_size);
+	}
+
+	/*
+	 * mode change is for clearing setuid/setgid bits. Allow lower fs
+	 * to interpret this in its own way.
+	 */
+	if (lower_ia.ia_valid & (ATTR_KILL_SUID | ATTR_KILL_SGID))
+		lower_ia.ia_valid &= ~ATTR_MODE;
+
+	/* notify the (possibly copied-up) lower inode */
+	/*
+	 * Note: we use lower_dentry->d_inode, because lower_inode may be
+	 * unlinked (no inode->i_sb and i_ino==0.  This happens if someone
+	 * tries to open(), unlink(), then ftruncate() a file.
+	 */
+	inode_lock(lower_dentry->d_inode);
+	err = notify_change(idmap, lower_dentry,
+			    &lower_ia, /* note: lower_ia */
+			    NULL);
+	inode_unlock(lower_dentry->d_inode);
+	if (err)
+		goto out;
+
+	/* get attributes from the lower inode */
+	esdfs_copy_attr(inode, lower_inode);
+	/*
+	 * Not running fsstack_copy_inode_size(inode, lower_inode), because
+	 * VFS should update our inode size, and notify_change on
+	 * lower_inode should update its size.
+	 */
+
+out:
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_getattr(struct mnt_idmap *idmap,
+			 const struct path *path, struct kstat *stat,
+			 u32 request_mask, unsigned int flags)
+{
+	int err;
+	struct dentry *dentry = path->dentry;
+	struct path lower_path;
+	struct kstat lower_stat;
+	struct inode *lower_inode;
+	struct inode *inode = dentry->d_inode;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(inode->i_sb),
+						ESDFS_I(inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(inode->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	esdfs_get_lower_path(dentry, &lower_path);
+
+	/* We need the lower getattr to calculate stat->blocks for us. */
+	err = vfs_getattr_nosec(&lower_path, &lower_stat, request_mask, flags);
+	if (err)
+		goto out;
+
+	lower_inode = esdfs_lower_inode(inode);
+	esdfs_copy_attr(inode, lower_inode);
+	fsstack_copy_inode_size(inode, lower_inode);
+	generic_fillattr(idmap, request_mask, inode, stat);
+
+	stat->blocks = lower_stat.blocks;
+
+out:
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+const struct inode_operations esdfs_symlink_iops = {
+	.permission     = esdfs_permission,
+	.setattr	= esdfs_setattr,
+	.getattr	= esdfs_getattr,
+};
+
+const struct inode_operations esdfs_dir_iops = {
+	.create		= esdfs_create,
+	.lookup		= esdfs_lookup,
+	.unlink		= esdfs_unlink,
+	.mkdir		= esdfs_mkdir,
+	.rmdir		= esdfs_rmdir,
+	.rename		= esdfs_rename,
+	.permission     = esdfs_permission,
+	.setattr	= esdfs_setattr,
+	.getattr	= esdfs_getattr,
+};
+
+const struct inode_operations esdfs_main_iops = {
+	.permission     = esdfs_permission,
+	.setattr	= esdfs_setattr,
+	.getattr	= esdfs_getattr,
+};
Index: kernel-rpi-6_6/fs/esdfs/lookup.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/esdfs/lookup.c
@@ -0,0 +1,473 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+struct esdfs_name_data {
+	struct dir_context ctx;
+	const struct qstr *to_find;
+	char match_name[NAME_MAX+1];
+	bool found;
+};
+
+static bool esdfs_name_match(struct dir_context *ctx, const char *name, int namelen,
+		loff_t offset, u64 ino, unsigned int d_type)
+{
+	struct esdfs_name_data *buf = container_of(ctx, struct esdfs_name_data, ctx);
+	struct qstr candidate = QSTR_INIT(name, namelen);
+
+	if (qstr_case_eq(buf->to_find, &candidate)) {
+		memcpy(buf->match_name, name, namelen);
+		buf->match_name[namelen] = 0;
+		buf->found = true;
+		return true;
+	}
+	return false;
+}
+
+int esdfs_lookup_nocase(struct path *parent,
+		const struct qstr *name,
+		struct path *path) {
+	int err = 0;
+	/* Use vfs_path_lookup to check if the dentry exists or not */
+	err = vfs_path_lookup(parent->dentry, parent->mnt, name->name, 0, path);
+	/* check for other cases */
+	if (err == -ENOENT) {
+		struct file *file;
+		const struct cred *cred = current_cred();
+
+		struct esdfs_name_data buffer = {
+			.ctx.actor = esdfs_name_match,
+			.to_find = name,
+			.found = false,
+		};
+
+		file = dentry_open(parent, O_RDONLY | O_DIRECTORY, cred);
+		if (IS_ERR(file))
+			return PTR_ERR(file);
+		err = iterate_dir(file, &buffer.ctx);
+		fput(file);
+		if (err)
+			return err;
+
+		if (buffer.found)
+			err = vfs_path_lookup(parent->dentry, parent->mnt,
+						buffer.match_name, 0, path);
+		else
+			err = -ENOENT;
+	}
+	return err;
+}
+
+struct esdfs_ci_getdents_callback {
+	struct dir_context ctx;
+	const char *name;
+	char match_name[NAME_MAX+1];
+	int found; /*-1: not found, 0: found*/
+	int count;
+};
+
+/* The dentry cache is just so we have properly sized dentries */
+static struct kmem_cache *esdfs_dentry_cachep;
+
+int esdfs_init_dentry_cache(void)
+{
+	esdfs_dentry_cachep =
+		kmem_cache_create("esdfs_dentry",
+				  sizeof(struct esdfs_dentry_info),
+				  0, SLAB_RECLAIM_ACCOUNT, NULL);
+
+	return esdfs_dentry_cachep ? 0 : -ENOMEM;
+}
+
+void esdfs_destroy_dentry_cache(void)
+{
+	if (esdfs_dentry_cachep)
+		kmem_cache_destroy(esdfs_dentry_cachep);
+}
+
+void esdfs_free_dentry_private_data(struct dentry *dentry)
+{
+	kmem_cache_free(esdfs_dentry_cachep, dentry->d_fsdata);
+	dentry->d_fsdata = NULL;
+}
+
+/* allocate new dentry private data */
+int esdfs_new_dentry_private_data(struct dentry *dentry)
+{
+	struct esdfs_dentry_info *info = ESDFS_D(dentry);
+
+	/* use zalloc to init dentry_info.lower_path */
+	info = kmem_cache_zalloc(esdfs_dentry_cachep, GFP_ATOMIC);
+	if (!info)
+		return -ENOMEM;
+
+	spin_lock_init(&info->lock);
+	dentry->d_fsdata = info;
+
+	return 0;
+}
+
+struct inode_data {
+	struct inode *lower_inode;
+	uint32_t id;
+};
+
+/* Multiple obb files can point to the same lower file */
+static int esdfs_inode_test(struct inode *inode, void *candidate_data)
+{
+	struct inode *current_lower_inode = esdfs_lower_inode(inode);
+	uint32_t current_userid = ESDFS_I(inode)->userid;
+	struct inode_data *data = (struct inode_data *)candidate_data;
+
+	if (current_lower_inode == data->lower_inode
+			&& current_userid == data->id)
+		return 1; /* found a match */
+	else
+		return 0; /* no match */
+}
+
+static int esdfs_inode_set(struct inode *inode, void *lower_inode)
+{
+	/* we do actual inode initialization in esdfs_iget */
+	return 0;
+}
+
+struct inode *esdfs_iget(struct super_block *sb, struct inode *lower_inode,
+						uint32_t id)
+{
+	struct esdfs_inode_info *info;
+	struct inode_data data;
+	struct inode *inode; /* the new inode to return */
+
+	if (!igrab(lower_inode))
+		return ERR_PTR(-ESTALE);
+	data.id = id;
+	data.lower_inode = lower_inode;
+	inode = iget5_locked(sb, /* our superblock */
+			     /*
+			      * hashval: we use inode number, but we can
+			      * also use "(unsigned long)lower_inode"
+			      * instead.
+			      */
+			     lower_inode->i_ino, /* hashval */
+			     esdfs_inode_test,	/* inode comparison function */
+			     esdfs_inode_set, /* inode init function */
+			     &data); /* data passed to test+set fxns */
+	if (!inode) {
+		iput(lower_inode);
+		return ERR_PTR(-ENOMEM);
+	}
+	/* if found a cached inode, then just return it (after iput) */
+	if (!(inode->i_state & I_NEW)) {
+		iput(lower_inode);
+		return inode;
+	}
+
+	/* initialize new inode */
+	info = ESDFS_I(inode);
+	info->tree = ESDFS_TREE_NONE;
+	info->userid = 0;
+	info->appid = 0;
+	info->under_obb = false;
+
+	inode->i_ino = lower_inode->i_ino;
+	esdfs_set_lower_inode(inode, lower_inode);
+
+	inode_inc_iversion(inode);
+
+	/* use different set of inode ops for symlinks & directories */
+	if (S_ISDIR(lower_inode->i_mode))
+		inode->i_op = &esdfs_dir_iops;
+	else if (S_ISLNK(lower_inode->i_mode))
+		inode->i_op = &esdfs_symlink_iops;
+	else
+		inode->i_op = &esdfs_main_iops;
+
+	/* use different set of file ops for directories */
+	if (S_ISDIR(lower_inode->i_mode))
+		inode->i_fop = &esdfs_dir_fops;
+	else
+		inode->i_fop = &esdfs_main_fops;
+
+	inode->i_mapping->a_ops = &esdfs_aops;
+
+	inode->i_atime.tv_sec = 0;
+	inode->i_atime.tv_nsec = 0;
+	inode->i_mtime.tv_sec = 0;
+	inode->i_mtime.tv_nsec = 0;
+  inode_set_ctime(inode, 0, 0);
+
+	/* properly initialize special inodes */
+	if (S_ISBLK(lower_inode->i_mode) || S_ISCHR(lower_inode->i_mode) ||
+	    S_ISFIFO(lower_inode->i_mode) || S_ISSOCK(lower_inode->i_mode))
+		init_special_inode(inode, lower_inode->i_mode,
+				   lower_inode->i_rdev);
+
+	/* all well, copy inode attributes */
+	esdfs_copy_lower_attr(inode, lower_inode);
+	fsstack_copy_inode_size(inode, lower_inode);
+
+	unlock_new_inode(inode);
+	return inode;
+}
+
+/*
+ * Helper interpose routine, called directly by ->lookup to handle
+ * spliced dentries
+ */
+static struct dentry *__esdfs_interpose(struct dentry *dentry,
+					struct super_block *sb,
+					struct path *lower_path,
+					uint32_t id)
+{
+	struct inode *inode;
+	struct inode *lower_inode;
+	struct super_block *lower_sb;
+	struct dentry *ret_dentry;
+
+	lower_inode = lower_path->dentry->d_inode;
+	lower_sb = esdfs_lower_super(sb);
+
+	/* check that the lower file system didn't cross a mount point */
+	if (lower_inode->i_sb != lower_sb) {
+		ret_dentry = ERR_PTR(-EXDEV);
+		goto out;
+	}
+
+	/*
+	 * We allocate our new inode below by calling esdfs_iget,
+	 * which will initialize some of the new inode's fields
+	 */
+
+	/* inherit lower inode number for esdfs's inode */
+	inode = esdfs_iget(sb, lower_inode, id);
+	if (IS_ERR(inode)) {
+		ret_dentry = ERR_CAST(inode);
+		goto out;
+	}
+
+	ret_dentry = d_splice_alias(inode, dentry);
+	dentry = ret_dentry ?: dentry;
+	if (IS_ERR(dentry))
+		goto out;
+
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(sb)))
+		esdfs_derive_perms(dentry);
+	esdfs_set_perms(inode);
+out:
+	return ret_dentry;
+}
+
+/*
+ * Connect an esdfs inode dentry/inode with several lower ones.  This is
+ * the classic stackable file system "vnode interposition" action.
+ *
+ * @dentry: esdfs's dentry which interposes on lower one
+ * @sb: esdfs's super_block
+ * @lower_path: the lower path (caller does path_get/put)
+ */
+int esdfs_interpose(struct dentry *dentry, struct super_block *sb,
+		     struct path *lower_path, uint32_t id)
+{
+	struct dentry *ret_dentry;
+
+	ret_dentry = __esdfs_interpose(dentry, sb, lower_path, id);
+	return PTR_ERR(ret_dentry);
+}
+
+/*
+ * Main driver function for esdfs's lookup.
+ *
+ * Returns: NULL (ok), ERR_PTR if an error occurred.
+ * Fills in lower_parent_path with <dentry,mnt> on success.
+ */
+static struct dentry *__esdfs_lookup(struct dentry *dentry,
+				     unsigned int flags,
+				     struct path *lower_parent_path,
+				     uint32_t id, bool use_dl)
+{
+	int err = 0;
+	struct vfsmount *lower_dir_mnt;
+	struct dentry *lower_dir_dentry = NULL;
+	struct dentry *lower_dentry;
+	const char *name;
+	struct path lower_path;
+	struct qstr dname;
+	struct dentry *ret_dentry = NULL;
+
+	/* must initialize dentry operations */
+	d_set_d_op(dentry, &esdfs_dops);
+
+	if (IS_ROOT(dentry))
+		goto out;
+
+	if (use_dl)
+		name = ESDFS_SB(dentry->d_sb)->dl_name.name;
+	else
+		name = dentry->d_name.name;
+
+	dname.name = name;
+	dname.len = strlen(name);
+
+	/* now start the actual lookup procedure */
+	lower_dir_dentry = lower_parent_path->dentry;
+	lower_dir_mnt = lower_parent_path->mnt;
+
+	/* if the access is to the Download directory, redirect
+	 * to lower path.
+	 */
+	if (use_dl) {
+		pathcpy(&lower_path, &ESDFS_SB(dentry->d_sb)->dl_path);
+		path_get(&ESDFS_SB(dentry->d_sb)->dl_path);
+	} else {
+		err = esdfs_lookup_nocase(lower_parent_path, &dname,
+					  &lower_path);
+	}
+
+	/* no error: handle positive dentries */
+	if (!err) {
+		esdfs_set_lower_path(dentry, &lower_path);
+		ret_dentry =
+			__esdfs_interpose(dentry, dentry->d_sb,
+						&lower_path, id);
+		if (IS_ERR(ret_dentry)) {
+			err = PTR_ERR(ret_dentry);
+			/* path_put underlying underlying path on error */
+			esdfs_put_reset_lower_paths(dentry);
+		}
+		goto out;
+	}
+
+	/*
+	 * We don't consider ENOENT an error, and we want to return a
+	 * negative dentry.
+	 */
+	if (err && err != -ENOENT)
+		goto out;
+
+	/* instatiate a new negative dentry */
+	/* See if the low-level filesystem might want
+	 * to use its own hash */
+	lower_dentry = d_hash_and_lookup(lower_dir_dentry, &dname);
+	if (IS_ERR(lower_dentry))
+		return lower_dentry;
+
+	if (!lower_dentry) {
+		/* We called vfs_path_lookup earlier, and did not get a negative
+		 * dentry then. Don't confuse the lower filesystem by forcing
+		 * one on it now...
+		 */
+		err = -ENOENT;
+		goto out;
+	}
+
+	lower_path.dentry = lower_dentry;
+	lower_path.mnt = mntget(lower_dir_mnt);
+	esdfs_set_lower_path(dentry, &lower_path);
+
+	/*
+	 * If the intent is to create a file, then don't return an error, so
+	 * the VFS will continue the process of making this negative dentry
+	 * into a positive one.
+	 */
+	if (flags & (LOOKUP_CREATE|LOOKUP_RENAME_TARGET))
+		err = 0;
+
+out:
+	if (err)
+		return ERR_PTR(err);
+	return ret_dentry;
+}
+
+struct dentry *esdfs_lookup(struct inode *dir, struct dentry *dentry,
+			    unsigned int flags)
+{
+	int err;
+	struct dentry *ret, *real_parent, *parent;
+	struct path lower_parent_path, old_lower_parent_path;
+	const struct cred *creds;
+	struct esdfs_sb_info *sbi = ESDFS_SB(dir->i_sb);
+	int use_dl;
+
+	parent = real_parent = dget_parent(dentry);
+
+	/* allocate dentry private data.  We free it in ->d_release */
+	err = esdfs_new_dentry_private_data(dentry);
+	if (err) {
+		ret = ERR_PTR(err);
+		goto out;
+	}
+
+	if (ESDFS_DERIVE_PERMS(sbi)) {
+		err = esdfs_derived_lookup(dentry, &parent);
+		if (err) {
+			ret = ERR_PTR(err);
+			goto out;
+		}
+	}
+
+	esdfs_get_lower_path(parent, &lower_parent_path);
+
+	creds =	esdfs_override_creds(ESDFS_SB(dir->i_sb),
+			ESDFS_I(d_inode(parent)), NULL);
+	if (!creds) {
+		ret = ERR_PTR(-EINVAL);
+		goto out_put;
+	}
+
+	/* Check if the lookup corresponds to the Download directory */
+	use_dl = esdfs_is_dl_lookup(dentry, parent);
+
+	ret = __esdfs_lookup(dentry, flags, &lower_parent_path,
+					ESDFS_I(dir)->userid,
+					use_dl);
+	if (IS_ERR(ret))
+		goto out_cred;
+	if (ret)
+		dentry = ret;
+	if (dentry->d_inode) {
+		fsstack_copy_attr_times(dentry->d_inode,
+					esdfs_lower_inode(dentry->d_inode));
+		/*
+		 * Do not modify the ownership of the lower directory if it
+		 * is the Download directory
+		 */
+		if (!use_dl)
+			esdfs_derive_lower_ownership(dentry,
+						     dentry->d_name.name);
+	}
+	/* update parent directory's atime */
+	fsstack_copy_attr_atime(parent->d_inode,
+				esdfs_lower_inode(parent->d_inode));
+
+	/*
+	 * If this is a pseudo hard link, store the real parent and ensure
+	 * that the link target directory contains any derived contents.
+	 */
+	if (parent != real_parent) {
+		esdfs_get_lower_path(real_parent, &old_lower_parent_path);
+		esdfs_set_lower_parent(dentry, old_lower_parent_path.dentry);
+		esdfs_put_lower_path(real_parent, &old_lower_parent_path);
+		esdfs_derive_mkdir_contents(dentry);
+	}
+out_cred:
+	esdfs_revert_creds(creds, NULL);
+out_put:
+	esdfs_put_lower_path(parent, &lower_parent_path);
+out:
+	dput(parent);
+	if (parent != real_parent)
+		dput(real_parent);
+	return ret;
+}
Index: kernel-rpi-6_6/fs/esdfs/main.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/esdfs/main.c
@@ -0,0 +1,728 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+#include <linux/module.h>
+#include <linux/parser.h>
+#include <linux/security.h>
+#include <linux/proc_ns.h>
+
+/*
+ * Derived from first generation "ANDROID_EMU" glue in modifed F2FS driver.
+ */
+enum {
+	Opt_lower_perms,
+	Opt_upper_perms,
+	Opt_derive_none,
+	Opt_derive_legacy,
+	Opt_derive_unified,
+	Opt_derive_multi,
+	Opt_derive_public,
+	Opt_confine,
+	Opt_noconfine,
+	Opt_gid_derivation,
+	Opt_default_normal,
+	Opt_dl_loc,
+	Opt_dl_uid,
+	Opt_dl_gid,
+	Opt_ns_fd,
+
+	/* From sdcardfs */
+	Opt_fsuid,
+	Opt_fsgid,
+	Opt_gid,
+	Opt_debug,
+	Opt_mask,
+	Opt_multiuser,
+	Opt_userid,
+
+	Opt_err,
+};
+
+static match_table_t esdfs_tokens = {
+	{Opt_lower_perms, "lower=%s"},
+	{Opt_upper_perms, "upper=%s"},
+	{Opt_derive_none, "derive=none"},
+	{Opt_derive_legacy, "derive=legacy"},
+	{Opt_derive_unified, "derive=unified"},
+	{Opt_derive_multi, "derive=multi"},
+	{Opt_derive_public, "derive=public"},
+	{Opt_confine, "confine"},
+	{Opt_noconfine, "noconfine"},
+	{Opt_gid_derivation, "derive_gid"},
+	{Opt_default_normal, "default_normal"},
+	{Opt_dl_loc, "dl_loc=%s"},
+	{Opt_dl_uid, "dl_uid=%u"},
+	{Opt_dl_gid, "dl_gid=%u"},
+	{Opt_ns_fd, "ns_fd=%d"},
+	/* compatibility with sdcardfs options */
+	{Opt_fsuid, "fsuid=%u"},
+	{Opt_fsgid, "fsgid=%u"},
+	{Opt_gid, "gid=%u"},
+	{Opt_mask, "mask=%u"},
+	{Opt_userid, "userid=%d"},
+	{Opt_multiuser, "multiuser"},
+	{Opt_gid_derivation, "derive_gid"},
+	{Opt_err, NULL},
+};
+
+struct esdfs_perms esdfs_perms_table[ESDFS_PERMS_TABLE_SIZE] = {
+	/* ESDFS_PERMS_LOWER_DEFAULT */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = AID_MEDIA_RW,
+	  .gid   = AID_MEDIA_RW,
+	  .fmask = 0664,
+	  .dmask = 0775 },
+	/* ESDFS_PERMS_UPPER_LEGACY */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = AID_ROOT,
+	  .gid   = AID_SDCARD_RW,
+	  .fmask = 0664,
+	  .dmask = 0775 },
+	/* ESDFS_PERMS_UPPER_DERIVED */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = AID_ROOT,
+	  .gid   = AID_SDCARD_R,
+	  .fmask = 0660,
+	  .dmask = 0771 },
+	/* ESDFS_PERMS_LOWER_DOWNLOAD */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = -1,
+	  .gid   = -1,
+	  .fmask = 0644,
+	  .dmask = 0711 },
+};
+
+static int parse_perms(struct esdfs_perms *perms, char *args)
+{
+	char *sep = args;
+	char *sepres;
+	int ret;
+
+	if (!sep)
+		return -EINVAL;
+
+	sepres = strsep(&sep, ":");
+	if (!sep)
+		return -EINVAL;
+	ret = kstrtou32(sepres, 0, &perms->uid);
+	if (ret)
+		return ret;
+
+	sepres = strsep(&sep, ":");
+	if (!sep)
+		return -EINVAL;
+	ret = kstrtou32(sepres, 0, &perms->gid);
+	if (ret)
+		return ret;
+
+	sepres = strsep(&sep, ":");
+	if (!sep)
+		return -EINVAL;
+	ret = kstrtou16(sepres, 8, &perms->fmask);
+	if (ret)
+		return ret;
+
+	sepres = strsep(&sep, ":");
+	ret = kstrtou16(sepres, 8, &perms->dmask);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static inline struct user_namespace *to_user_ns(struct ns_common *ns)
+{
+	return container_of(ns, struct user_namespace, ns);
+}
+
+static struct user_namespace *get_ns_from_fd(int fd)
+{
+  struct fd f = fdget(fd);
+	struct file *file;
+	struct ns_common *ns;
+	struct user_namespace *user_ns = ERR_PTR(-EINVAL);
+
+  if (!f.file)
+    return ERR_PTR(-EBADF);
+	file = f.file;
+
+	ns = get_proc_ns(file_inode(file));
+#ifdef CONFIG_USER_NS
+	if (ns->ops == &userns_operations)
+		user_ns = to_user_ns(ns);
+#endif
+	fdput(f);
+	return user_ns;
+}
+
+static int parse_options(struct super_block *sb, char *options)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(sb);
+	substring_t args[MAX_OPT_ARGS];
+	char *p;
+	int option;
+
+	if (!options)
+		return 0;
+
+	while ((p = strsep(&options, ",")) != NULL) {
+		int token;
+
+		if (!*p)
+			continue;
+		/*
+		 * Initialize args struct so we know whether arg was
+		 * found; some options take optional arguments.
+		 */
+		args[0].to = args[0].from = NULL;
+		token = match_token(p, esdfs_tokens, args);
+
+		switch (token) {
+		case Opt_lower_perms:
+			if (args->from) {
+				int ret;
+				char *perms = match_strdup(args);
+
+				ret = parse_perms(&sbi->lower_perms, perms);
+				kfree(perms);
+
+				if (ret)
+					return -EINVAL;
+			} else
+				return -EINVAL;
+			break;
+		case Opt_upper_perms:
+			if (args->from) {
+				int ret;
+				char *perms = match_strdup(args);
+
+				ret = parse_perms(&sbi->upper_perms, perms);
+				kfree(perms);
+
+				if (ret)
+					return -EINVAL;
+			} else
+				return -EINVAL;
+			break;
+		case Opt_derive_none:
+			clear_opt(sbi, DERIVE_LEGACY);
+			clear_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_derive_legacy:
+			set_opt(sbi, DERIVE_LEGACY);
+			clear_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_derive_unified:
+			clear_opt(sbi, DERIVE_LEGACY);
+			set_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			set_opt(sbi, DERIVE_CONFINE);	/* confine by default */
+			break;
+		case Opt_derive_multi:
+		case Opt_multiuser:
+			set_opt(sbi, DERIVE_LEGACY);
+			clear_opt(sbi, DERIVE_UNIFIED);
+			set_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_derive_public:
+			clear_opt(sbi, DERIVE_LEGACY);
+			set_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			set_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_confine:
+			set_opt(sbi, DERIVE_CONFINE);
+			break;
+		case Opt_noconfine:
+			clear_opt(sbi, DERIVE_CONFINE);
+			break;
+		/* for compatibility with sdcardfs options */
+		case Opt_gid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->upper_perms.raw_gid = option;
+			break;
+		case Opt_userid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->upper_perms.raw_uid = option;
+			break;
+		case Opt_mask:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->upper_perms.dmask = 0775 & ~option;
+			sbi->upper_perms.fmask = 0775 & ~option;
+			break;
+		case Opt_fsuid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_perms.raw_uid = option;
+			break;
+		case Opt_fsgid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_perms.raw_gid = option;
+			break;
+		case Opt_gid_derivation:
+			set_opt(sbi, GID_DERIVATION);
+			break;
+		case Opt_default_normal:
+			set_opt(sbi, DEFAULT_NORMAL);
+			break;
+		case Opt_dl_loc:
+			set_opt(sbi, SPECIAL_DOWNLOAD);
+			sbi->dl_loc = match_strdup(args);
+			break;
+		case Opt_dl_uid:
+			set_opt(sbi, SPECIAL_DOWNLOAD);
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_dl_perms.raw_uid = option;
+			break;
+		case Opt_dl_gid:
+			set_opt(sbi, SPECIAL_DOWNLOAD);
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_dl_perms.raw_gid = option;
+			break;
+		case Opt_ns_fd:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->ns_fd = option;
+			break;
+		default:
+			esdfs_msg(sb, KERN_ERR,
+			  "unrecognized mount option \"%s\" or missing value\n",
+			  p);
+			return -EINVAL;
+		}
+	}
+	return 0;
+}
+
+static int interpret_perms(struct esdfs_sb_info *sbi, struct esdfs_perms *perms)
+{
+	if (perms->raw_uid == -1) {
+		perms->raw_uid = perms->uid;
+	} else {
+		perms->uid = esdfs_from_local_uid(sbi, perms->raw_uid);
+		if (perms->uid == -1)
+			return -EINVAL;
+	}
+
+	if (perms->raw_gid == -1) {
+		perms->raw_gid = perms->gid;
+	} else {
+		perms->gid = esdfs_from_local_gid(sbi, perms->raw_gid);
+		if (perms->gid == -1)
+			return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * There is no need to lock the esdfs_super_info's rwsem as there is no
+ * way anyone can have a reference to the superblock at this point in time.
+ */
+static int esdfs_read_super(struct super_block *sb, const char *dev_name,
+		void *raw_data, int silent)
+{
+	int err = 0;
+	struct super_block *lower_sb;
+	struct path lower_path;
+	struct esdfs_sb_info *sbi;
+	struct inode *inode;
+	struct dentry *lower_dl_dentry, *root_dentry;
+	struct user_namespace *user_ns;
+	kuid_t dl_kuid = INVALID_UID;
+	kgid_t dl_kgid = INVALID_GID;
+
+	if (!dev_name) {
+		esdfs_msg(sb, KERN_ERR, "missing dev_name argument\n");
+		err = -EINVAL;
+		goto out;
+	}
+
+	/* parse lower path */
+	err = kern_path(dev_name, LOOKUP_FOLLOW | LOOKUP_DIRECTORY,
+			&lower_path);
+	if (err) {
+		esdfs_msg(sb, KERN_ERR,
+			"error accessing lower directory '%s'\n", dev_name);
+		goto out;
+	}
+
+	/* allocate superblock private data */
+	sb->s_fs_info = kzalloc(sizeof(struct esdfs_sb_info), GFP_KERNEL);
+	sbi = ESDFS_SB(sb);
+	if (!sbi) {
+		esdfs_msg(sb, KERN_CRIT, "read_super: out of memory\n");
+		err = -ENOMEM;
+		goto out_pput;
+	}
+	INIT_LIST_HEAD(&sbi->s_list);
+
+	/* set defaults and then parse the mount options */
+
+	sbi->ns_fd = -1;
+
+	/* make public default */
+	clear_opt(sbi, DERIVE_LEGACY);
+	set_opt(sbi, DERIVE_UNIFIED);
+	clear_opt(sbi, DERIVE_MULTI);
+	set_opt(sbi, DERIVE_PUBLIC);
+
+	memcpy(&sbi->lower_perms,
+	       &esdfs_perms_table[ESDFS_PERMS_LOWER_DEFAULT],
+	       sizeof(struct esdfs_perms));
+	if (ESDFS_DERIVE_PERMS(sbi))
+		memcpy(&sbi->upper_perms,
+		       &esdfs_perms_table[ESDFS_PERMS_UPPER_DERIVED],
+		       sizeof(struct esdfs_perms));
+	else
+		memcpy(&sbi->upper_perms,
+		       &esdfs_perms_table[ESDFS_PERMS_UPPER_LEGACY],
+		       sizeof(struct esdfs_perms));
+
+	memcpy(&sbi->lower_dl_perms,
+	       &esdfs_perms_table[ESDFS_PERMS_LOWER_DOWNLOAD],
+	       sizeof(struct esdfs_perms));
+
+	err = parse_options(sb, (char *)raw_data);
+	if (err)
+		goto out_free;
+
+	/* Initialize special namespace for lower Downloads directory */
+	sbi->dl_ns = get_user_ns(current_user_ns());
+
+	if (sbi->ns_fd == -1) {
+		sbi->base_ns = get_user_ns(current_user_ns());
+	} else {
+		user_ns = get_ns_from_fd(sbi->ns_fd);
+		if (IS_ERR(user_ns)) {
+			err = PTR_ERR(user_ns);
+			goto out_free;
+		}
+		sbi->base_ns = get_user_ns(user_ns);
+	}
+	/* interpret all parameters in given namespace */
+	err = interpret_perms(sbi, &sbi->lower_perms);
+	if (err) {
+		pr_err("esdfs: Invalid permissions for lower layer\n");
+		goto out_free;
+	}
+	err = interpret_perms(sbi, &sbi->upper_perms);
+	if (err) {
+		pr_err("esdfs: Invalid permissions for upper layer\n");
+		goto out_free;
+	}
+
+	/* Check if the downloads uid maps into a valid kuid from
+	 * the namespace of the mounting process
+	 */
+	if (sbi->lower_dl_perms.raw_uid != -1) {
+		dl_kuid = make_kuid(sbi->dl_ns,
+				    sbi->lower_dl_perms.raw_uid);
+		if (!uid_valid(dl_kuid)) {
+			pr_err("esdfs: Invalid permissions for dl_uid");
+			err = -EINVAL;
+			goto out_free;
+		}
+	}
+	if (sbi->lower_dl_perms.raw_gid != -1) {
+		dl_kgid = make_kgid(sbi->dl_ns,
+				    sbi->lower_dl_perms.raw_gid);
+		if (!gid_valid(dl_kgid)) {
+			pr_err("esdfs: Invalid permissions for dl_gid");
+			err = -EINVAL;
+			goto out_free;
+		}
+	}
+
+	/* set the lower superblock field of upper superblock */
+	lower_sb = lower_path.dentry->d_sb;
+	atomic_inc(&lower_sb->s_active);
+	esdfs_set_lower_super(sb, lower_sb);
+
+	sb->s_stack_depth = lower_sb->s_stack_depth + 1;
+	if (sb->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {
+		pr_err("esdfs: maximum fs stacking depth exceeded\n");
+		err = -EINVAL;
+		goto out_sput;
+	}
+
+	/* inherit maxbytes from lower file system */
+	sb->s_maxbytes = lower_sb->s_maxbytes;
+
+	/*
+	 * Our c/m/atime granularity is 1 ns because we may stack on file
+	 * systems whose granularity is as good.
+	 */
+	sb->s_time_gran = 1;
+
+	sb->s_op = &esdfs_sops;
+
+	/* get a new inode and allocate our root dentry */
+	inode = esdfs_iget(sb, lower_path.dentry->d_inode, 0);
+	if (IS_ERR(inode)) {
+		err = PTR_ERR(inode);
+		goto out_sput;
+	}
+	root_dentry = d_make_root(inode);
+	if (!root_dentry) {
+		err = -ENOMEM;
+		goto out_sput;
+	}
+	d_set_d_op(root_dentry, &esdfs_dops);
+
+	/* link the upper and lower dentries */
+	root_dentry->d_fsdata = NULL;
+	err = esdfs_new_dentry_private_data(root_dentry);
+	if (err)
+		goto out_freeroot;
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD)) {
+		/* parse lower path */
+		err = kern_path(sbi->dl_loc, LOOKUP_FOLLOW | LOOKUP_DIRECTORY,
+				&sbi->dl_path);
+		if (err) {
+			esdfs_msg(sb, KERN_ERR,
+				"error accessing download directory '%s'\n",
+				sbi->dl_loc);
+			goto out_freeroot;
+		}
+
+		lower_dl_dentry = sbi->dl_path.dentry;
+
+		if (!S_ISDIR(lower_dl_dentry->d_inode->i_mode)) {
+			err = -EINVAL;
+			esdfs_msg(sb, KERN_ERR,
+				"dl_loc must be a directory '%s'\n",
+				sbi->dl_loc);
+			goto out_dlput;
+		}
+
+		if (lower_dl_dentry->d_sb != lower_sb) {
+			esdfs_msg(sb, KERN_ERR,
+				"dl_loc must be in the same filesystem '%s'\n",
+				sbi->dl_loc);
+			goto out_dlput;
+		}
+
+		if (!uid_valid(dl_kuid)) {
+			dl_kuid = esdfs_make_kuid(sbi, sbi->lower_perms.uid);
+			sbi->lower_dl_perms.raw_uid = from_kuid(sbi->dl_ns,
+								dl_kuid);
+		}
+		if (!gid_valid(dl_kgid)) {
+			dl_kgid = esdfs_make_kgid(sbi, sbi->lower_perms.gid);
+			sbi->lower_dl_perms.raw_gid = from_kgid(sbi->dl_ns,
+								dl_kgid);
+		}
+		spin_lock(&lower_dl_dentry->d_lock);
+		sbi->dl_name.name = kstrndup(lower_dl_dentry->d_name.name,
+				lower_dl_dentry->d_name.len, GFP_ATOMIC);
+		sbi->dl_name.len = lower_dl_dentry->d_name.len;
+		spin_unlock(&lower_dl_dentry->d_lock);
+	}
+	/* if get here: cannot have error */
+
+	/* set the lower dentries for s_root */
+	esdfs_set_lower_path(root_dentry, &lower_path);
+
+	/*
+	 * No need to call interpose because we already have a positive
+	 * dentry, which was instantiated by d_make_root.  Just need to
+	 * d_rehash it.
+	 */
+	d_rehash(root_dentry);
+	sb->s_root = root_dentry;
+
+	if (!silent)
+		esdfs_msg(sb, KERN_INFO, "mounted on top of %s type %s\n",
+			dev_name, lower_sb->s_type->name);
+
+	if (!ESDFS_DERIVE_PERMS(sbi))
+		goto out;
+
+	/* let user know that we ignore this option in older derived modes */
+	if (ESDFS_RESTRICT_PERMS(sbi) &&
+	    memcmp(&sbi->upper_perms,
+		   &esdfs_perms_table[ESDFS_PERMS_UPPER_DERIVED],
+		   sizeof(struct esdfs_perms)))
+		esdfs_msg(sb, KERN_WARNING,
+			"'upper' mount option ignored in this derived mode\n");
+
+	/*
+	 * In Android 3.0 all user conent in the emulated storage tree was
+	 * stored in /data/media.  Android 4.2 introduced multi-user support,
+	 * which required that the primary user's content be migrated from
+	 * /data/media to /data/media/0.  The framework then uses bind mounts
+	 * to create per-process namespaces to isolate each user's tree at
+	 * /data/media/N.  This approach of having each user in a common root
+	 * is now considered "legacy" by the sdcard service.
+	 */
+	if (test_opt(sbi, DERIVE_LEGACY)) {
+		ESDFS_I(inode)->tree = ESDFS_TREE_ROOT_LEGACY;
+		sbi->obb_parent = dget(sb->s_root);
+	/*
+	 * Android 4.4 reorganized this sturcture yet again, so that the
+	 * primary user's content was again at the root.  Secondary users'
+	 * content is found in Android/user/N.  Emulated internal storage still
+	 * seems to use the legacy tree, but secondary external storage uses
+	 * this method.
+	 */
+	} else if (test_opt(sbi, DERIVE_UNIFIED))
+		ESDFS_I(inode)->tree = ESDFS_TREE_ROOT;
+	/*
+	 * Later versions of Android organize user content using quantum
+	 * entanglement, which has a low probability of being supported by
+	 * this driver.
+	 */
+	else
+		esdfs_msg(sb, KERN_WARNING,
+				"unsupported derived permissions mode\n");
+
+	/* initialize root inode */
+	esdfs_derive_perms(sb->s_root);
+	esdfs_set_perms(inode);
+
+	esdfs_add_super(sbi, sb);
+
+	goto out;
+
+out_dlput:
+	path_put(&sbi->dl_path);
+	sbi->dl_path.dentry = NULL;
+	sbi->dl_path.mnt = NULL;
+out_freeroot:
+	dput(root_dentry);
+	root_dentry = NULL;
+out_sput:
+	/* drop refs we took earlier */
+	atomic_dec(&lower_sb->s_active);
+out_free:
+	if (sbi->dl_ns)
+		put_user_ns(sbi->dl_ns);
+	if (sbi->base_ns)
+		put_user_ns(sbi->base_ns);
+	kfree(sbi->dl_loc);
+	kfree(ESDFS_SB(sb));
+	sb->s_fs_info = NULL;
+out_pput:
+	path_put(&lower_path);
+
+out:
+	return err;
+}
+
+struct esdfs_mount_private {
+	const char *dev_name;
+	void *raw_data;
+};
+
+static int __esdfs_fill_super(struct super_block *sb, void *_priv, int silent)
+{
+	struct esdfs_mount_private *priv = _priv;
+
+	return esdfs_read_super(sb, priv->dev_name, priv->raw_data, silent);
+}
+
+static struct dentry *esdfs_mount(struct file_system_type *fs_type, int flags,
+				const char *dev_name, void *raw_data)
+{
+	struct esdfs_mount_private priv = {
+		.dev_name = dev_name,
+		.raw_data = raw_data,
+	};
+
+	return mount_nodev(fs_type, flags, &priv, __esdfs_fill_super);
+}
+
+static void esdfs_kill_sb(struct super_block *sb)
+{
+	if (sb->s_fs_info && ESDFS_SB(sb)->obb_parent)
+		dput(ESDFS_SB(sb)->obb_parent);
+	if (sb->s_fs_info && ESDFS_SB(sb)->dl_ns)
+		put_user_ns(ESDFS_SB(sb)->dl_ns);
+	if (sb->s_fs_info && ESDFS_SB(sb)->base_ns)
+		put_user_ns(ESDFS_SB(sb)->base_ns);
+	if (sb->s_fs_info) {
+		kfree(ESDFS_SB(sb)->dl_loc);
+		kfree(ESDFS_SB(sb)->dl_name.name);
+		path_put(&ESDFS_SB(sb)->dl_path);
+	}
+
+	kill_anon_super(sb);
+}
+
+static struct file_system_type esdfs_fs_type = {
+	.owner		= THIS_MODULE,
+	.name		= ESDFS_NAME,
+	.mount		= esdfs_mount,
+	.kill_sb	= esdfs_kill_sb,
+	.fs_flags	= 0,
+};
+MODULE_ALIAS_FS(ESDFS_NAME);
+
+static int __init init_esdfs_fs(void)
+{
+	int err;
+
+	pr_info("Registering esdfs " ESDFS_VERSION "\n");
+
+	esdfs_init_package_list();
+
+	err = esdfs_init_inode_cache();
+	if (err)
+		goto out;
+	err = esdfs_init_dentry_cache();
+	if (err)
+		goto out;
+	err = register_filesystem(&esdfs_fs_type);
+out:
+	if (err) {
+		esdfs_destroy_inode_cache();
+		esdfs_destroy_dentry_cache();
+		esdfs_destroy_package_list();
+	}
+	return err;
+}
+
+static void __exit exit_esdfs_fs(void)
+{
+	esdfs_destroy_inode_cache();
+	esdfs_destroy_dentry_cache();
+	esdfs_destroy_package_list();
+	unregister_filesystem(&esdfs_fs_type);
+	pr_info("Completed esdfs module unload\n");
+}
+
+MODULE_AUTHOR("Erez Zadok, Filesystems and Storage Lab, Stony Brook University"
+	      " (http://www.fsl.cs.sunysb.edu/)");
+MODULE_DESCRIPTION("esdfs " ESDFS_VERSION);
+MODULE_LICENSE("GPL");
+
+module_init(init_esdfs_fs);
+module_exit(exit_esdfs_fs);
Index: kernel-rpi-6_6/fs/esdfs/mmap.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/esdfs/mmap.c
@@ -0,0 +1,98 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014, 2016 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+static vm_fault_t esdfs_fault(struct vm_fault *vmf)
+{
+	vm_fault_t err;
+	struct file *file;
+	const struct vm_operations_struct *lower_vm_ops;
+	struct esdfs_sb_info *sbi;
+	const struct cred *creds;
+	const struct vm_area_struct *vma = vmf->vma;
+
+	file = (struct file *)vma->vm_private_data;
+	sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	creds = esdfs_override_creds(sbi, ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return VM_FAULT_OOM;
+
+	lower_vm_ops = ESDFS_F(file)->lower_vm_ops;
+	BUG_ON(!lower_vm_ops);
+	err = lower_vm_ops->fault(vmf);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static void esdfs_vm_open(struct vm_area_struct *vma)
+{
+	struct file *file = (struct file *)vma->vm_private_data;
+
+	get_file(file);
+}
+
+static void esdfs_vm_close(struct vm_area_struct *vma)
+{
+	struct file *file = (struct file *)vma->vm_private_data;
+
+	fput(file);
+}
+
+static vm_fault_t esdfs_page_mkwrite(struct vm_fault *vmf)
+{
+	vm_fault_t err = 0;
+	struct file *file;
+	const struct vm_operations_struct *lower_vm_ops;
+	struct esdfs_sb_info *sbi;
+	const struct cred *creds;
+	const struct vm_area_struct *vma = vmf->vma;
+
+	file = (struct file *)vma->vm_private_data;
+	sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	creds = esdfs_override_creds(sbi, ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return VM_FAULT_OOM;
+
+	lower_vm_ops = ESDFS_F(file)->lower_vm_ops;
+	BUG_ON(!lower_vm_ops);
+	if (!lower_vm_ops->page_mkwrite)
+		goto out;
+
+	err = lower_vm_ops->page_mkwrite(vmf);
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static ssize_t esdfs_direct_IO(struct kiocb *iocb,
+				struct iov_iter *iter)
+{
+	/*
+	 * This function should never be called directly.  We need it
+	 * to exist, to get past a check in open_check_o_direct(),
+	 * which is called from do_last().
+	 */
+	return -EINVAL;
+}
+
+const struct address_space_operations esdfs_aops = {
+	.direct_IO = esdfs_direct_IO,
+};
+
+const struct vm_operations_struct esdfs_vm_ops = {
+	.fault		= esdfs_fault,
+	.page_mkwrite	= esdfs_page_mkwrite,
+	.open		= esdfs_vm_open,
+	.close		= esdfs_vm_close,
+};
Index: kernel-rpi-6_6/fs/esdfs/super.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/esdfs/super.c
@@ -0,0 +1,290 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+/*
+ * The inode cache is used with alloc_inode for both our inode info and the
+ * vfs inode.
+ */
+static struct kmem_cache *esdfs_inode_cachep;
+static LIST_HEAD(esdfs_list);
+static DEFINE_SPINLOCK(esdfs_list_lock);
+
+void esdfs_msg(struct super_block *sb, const char *level, const char *fmt, ...)
+{
+	struct va_format vaf;
+	va_list args;
+
+	va_start(args, fmt);
+	vaf.fmt = fmt;
+	vaf.va = &args;
+	printk("%sESDFS-fs (%s): %pV", level, sb->s_id, &vaf);
+	va_end(args);
+}
+
+void esdfs_add_super(struct esdfs_sb_info *sbi, struct super_block *sb)
+{
+	sbi->s_sb = sb;
+
+	spin_lock(&esdfs_list_lock);
+	list_add_tail(&sbi->s_list, &esdfs_list);
+	spin_unlock(&esdfs_list_lock);
+}
+
+static void esdfs_remove_super(struct esdfs_sb_info *sbi)
+{
+	spin_lock(&esdfs_list_lock);
+	list_del(&sbi->s_list);
+	spin_unlock(&esdfs_list_lock);
+}
+
+void esdfs_truncate_share(struct super_block *sb, struct inode *lower_inode,
+				loff_t newsize)
+{
+	struct list_head *p;
+	struct esdfs_sb_info *sbi;
+	struct super_block *lower_sb = lower_inode->i_sb;
+	struct inode *inode;
+
+	spin_lock(&esdfs_list_lock);
+	p = esdfs_list.next;
+	while (p != &esdfs_list) {
+		sbi = list_entry(p, struct esdfs_sb_info, s_list);
+		if (sbi->s_sb == sb || sbi->lower_sb != lower_sb) {
+			p = p->next;
+			continue;
+		}
+		spin_unlock(&esdfs_list_lock);
+		inode = ilookup(sbi->s_sb, lower_inode->i_ino);
+		if (inode) {
+			truncate_setsize(inode, newsize);
+			iput(inode);
+		}
+		spin_lock(&esdfs_list_lock);
+		p = p->next;
+	}
+	spin_unlock(&esdfs_list_lock);
+}
+
+/* final actions when unmounting a file system */
+static void esdfs_put_super(struct super_block *sb)
+{
+	struct esdfs_sb_info *spd;
+	struct super_block *s;
+
+	spd = ESDFS_SB(sb);
+	if (!spd)
+		return;
+
+	/* decrement lower super references */
+	s = esdfs_lower_super(sb);
+	esdfs_set_lower_super(sb, NULL);
+	atomic_dec(&s->s_active);
+
+	esdfs_remove_super(spd);
+
+	kfree(spd);
+	sb->s_fs_info = NULL;
+}
+
+static int esdfs_statfs(struct dentry *dentry, struct kstatfs *buf)
+{
+	int err;
+	struct path lower_path;
+	struct inode *inode = dentry->d_inode;
+
+	if (test_opt(ESDFS_SB(inode->i_sb), ACCESS_DISABLE))
+		return -ENOENT;
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	err = vfs_statfs(&lower_path, buf);
+	esdfs_put_lower_path(dentry, &lower_path);
+
+	/* set return buf to our f/s to avoid confusing user-level utils */
+	buf->f_type = ESDFS_SUPER_MAGIC;
+
+	return err;
+}
+
+/*
+ * @flags: numeric mount options
+ * @options: mount options string
+ */
+static int esdfs_remount_fs(struct super_block *sb, int *flags, char *options)
+{
+	int err = 0;
+
+	/*
+	 * The VFS will take care of "ro" and "rw" flags among others.  We
+	 * can safely accept a few flags (RDONLY, MANDLOCK), and honor
+	 * SILENT, but anything else left over is an error.
+	 */
+	if ((*flags & ~(MS_RDONLY | MS_MANDLOCK | MS_SILENT)) != 0) {
+		esdfs_msg(sb, KERN_ERR, "remount flags 0x%x unsupported\n",
+			*flags);
+		err = -EINVAL;
+	}
+
+	return err;
+}
+
+/*
+ * Called by iput() when the inode reference count reached zero
+ * and the inode is not hashed anywhere.  Used to clear anything
+ * that needs to be, before the inode is completely destroyed and put
+ * on the inode free list.
+ */
+static void esdfs_evict_inode(struct inode *inode)
+{
+	struct inode *lower_inode;
+
+	truncate_inode_pages(&inode->i_data, 0);
+	clear_inode(inode);
+	/*
+	 * Decrement a reference to a lower_inode, which was incremented
+	 * by our read_inode when it was created initially.
+	 */
+	lower_inode = esdfs_lower_inode(inode);
+	esdfs_set_lower_inode(inode, NULL);
+	iput(lower_inode);
+}
+
+static struct inode *esdfs_alloc_inode(struct super_block *sb)
+{
+	struct esdfs_inode_info *i;
+
+	i = kmem_cache_alloc(esdfs_inode_cachep, GFP_KERNEL);
+	if (!i)
+		return NULL;
+
+	/* memset everything up to the inode to 0 */
+	memset(i, 0, offsetof(struct esdfs_inode_info, vfs_inode));
+
+	inode_set_iversion(&i->vfs_inode, 1);
+	return &i->vfs_inode;
+}
+
+static void i_callback(struct rcu_head *head)
+{
+	struct inode *inode = container_of(head, struct inode, i_rcu);
+
+	kmem_cache_free(esdfs_inode_cachep, ESDFS_I(inode));
+}
+
+static void esdfs_destroy_inode(struct inode *inode)
+{
+	call_rcu(&inode->i_rcu, i_callback);
+}
+
+/* esdfs inode cache constructor */
+static void init_once(void *obj)
+{
+	struct esdfs_inode_info *i = obj;
+
+	inode_init_once(&i->vfs_inode);
+}
+
+int esdfs_init_inode_cache(void)
+{
+	int err = 0;
+
+	esdfs_inode_cachep =
+		kmem_cache_create("esdfs_inode_cache",
+				  sizeof(struct esdfs_inode_info), 0,
+				  SLAB_RECLAIM_ACCOUNT, init_once);
+	if (!esdfs_inode_cachep)
+		err = -ENOMEM;
+	return err;
+}
+
+/* esdfs inode cache destructor */
+void esdfs_destroy_inode_cache(void)
+{
+	if (esdfs_inode_cachep)
+		kmem_cache_destroy(esdfs_inode_cachep);
+}
+
+/*
+ * Used only in nfs, to kill any pending RPC tasks, so that subsequent
+ * code can actually succeed and won't leave tasks that need handling.
+ */
+static void esdfs_umount_begin(struct super_block *sb)
+{
+	struct super_block *lower_sb;
+
+	lower_sb = esdfs_lower_super(sb);
+	if (lower_sb && lower_sb->s_op && lower_sb->s_op->umount_begin)
+		lower_sb->s_op->umount_begin(lower_sb);
+}
+
+static int esdfs_show_options(struct seq_file *seq, struct dentry *root)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(root->d_sb);
+
+	if (memcmp(&sbi->lower_perms,
+		   &esdfs_perms_table[ESDFS_PERMS_LOWER_DEFAULT],
+		   sizeof(struct esdfs_perms)))
+		seq_printf(seq, ",lower=%u:%u:%ho:%ho",
+				sbi->lower_perms.raw_uid,
+				sbi->lower_perms.raw_gid,
+				sbi->lower_perms.fmask,
+				sbi->lower_perms.dmask);
+
+	if (memcmp(&sbi->upper_perms,
+		   &esdfs_perms_table[ESDFS_PERMS_UPPER_LEGACY],
+		   sizeof(struct esdfs_perms)))
+		seq_printf(seq, ",upper=%u:%u:%ho:%ho",
+				sbi->upper_perms.raw_uid,
+				sbi->upper_perms.raw_gid,
+				sbi->upper_perms.fmask,
+				sbi->upper_perms.dmask);
+
+	if (test_opt(sbi, DERIVE_PUBLIC))
+		seq_puts(seq, ",derive=public");
+	else if (test_opt(sbi, DERIVE_MULTI))
+		seq_puts(seq, ",derive=multi");
+	else if (test_opt(sbi, DERIVE_UNIFIED))
+		seq_puts(seq, ",derive=unified");
+	else if (test_opt(sbi, DERIVE_LEGACY))
+		seq_puts(seq, ",derive=legacy");
+	else
+		seq_puts(seq, ",derive=none");
+
+	if (test_opt(sbi, DERIVE_CONFINE))
+		seq_puts(seq, ",confine");
+	else
+		seq_puts(seq, ",noconfine");
+	if (test_opt(sbi, GID_DERIVATION))
+		seq_puts(seq, ",derive_gid");
+	if (test_opt(sbi, DEFAULT_NORMAL))
+		seq_puts(seq, ",default_normal");
+	if (test_opt(sbi, SPECIAL_DOWNLOAD)) {
+		seq_printf(seq, ",dl_loc=%s", sbi->dl_loc);
+		seq_printf(seq, ",dl_uid=%d", sbi->lower_dl_perms.raw_uid);
+		seq_printf(seq, ",dl_gid=%d", sbi->lower_dl_perms.raw_gid);
+	}
+	return 0;
+}
+
+const struct super_operations esdfs_sops = {
+	.put_super	= esdfs_put_super,
+	.statfs		= esdfs_statfs,
+	.remount_fs	= esdfs_remount_fs,
+	.evict_inode	= esdfs_evict_inode,
+	.umount_begin	= esdfs_umount_begin,
+	.show_options	= esdfs_show_options,
+	.alloc_inode	= esdfs_alloc_inode,
+	.destroy_inode	= esdfs_destroy_inode,
+	.drop_inode	= generic_delete_inode,
+};
Index: kernel-rpi-6_6/include/linux/low-mem-notify.h
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/include/linux/low-mem-notify.h
@@ -0,0 +1,22 @@
+#ifndef _LINUX_LOW_MEM_NOTIFY_H
+#define _LINUX_LOW_MEM_NOTIFY_H
+
+#include <linux/types.h>
+
+#ifdef CONFIG_LOW_MEM_NOTIFY
+extern const struct file_operations low_mem_notify_fops;
+
+void low_mem_notify(void);
+bool low_mem_check(void);
+#else
+static inline void low_mem_notify(void)
+{
+}
+
+static inline bool low_mem_check(void)
+{
+	return false;
+}
+#endif
+
+#endif
Index: kernel-rpi-6_6/include/linux/mm.h
===================================================================
--- kernel-rpi-6_6.orig/include/linux/mm.h
+++ kernel-rpi-6_6/include/linux/mm.h
@@ -198,6 +198,7 @@ static inline void __mm_zero_struct_page
 #define DEFAULT_MAX_MAP_COUNT	(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)
 
 extern int sysctl_max_map_count;
+extern int sysctl_mmap_noexec_taint;
 
 extern unsigned long sysctl_user_reserve_kbytes;
 extern unsigned long sysctl_admin_reserve_kbytes;
@@ -397,6 +398,25 @@ extern unsigned int kobjsize(const void
 # define VM_UFFD_MINOR		VM_NONE
 #endif /* CONFIG_HAVE_ARCH_USERFAULTFD_MINOR */
 
+#ifdef CONFIG_64BIT
+/* VM is sealed, in vm_flags */
+#define VM_SEALED	_BITUL(63)
+#endif
+
+/*
+ * This flag is used to connect VFIO to arch specific KVM code. It
+ * indicates that the memory under this VMA is safe for use with any
+ * non-cachable memory type inside KVM. Some VFIO devices, on some
+ * platforms, are thought to be unsafe and can cause machine crashes
+ * if KVM does not lock down the memory type.
+ */
+#ifdef CONFIG_64BIT
+#define VM_ALLOW_ANY_UNCACHED_BIT	39
+#define VM_ALLOW_ANY_UNCACHED		BIT(VM_ALLOW_ANY_UNCACHED_BIT)
+#else
+#define VM_ALLOW_ANY_UNCACHED		VM_NONE
+#endif
+
 /* Bits set in the VMA until the stack is in its final location */
 #define VM_STACK_INCOMPLETE_SETUP (VM_RAND_READ | VM_SEQ_READ | VM_STACK_EARLY)
 
@@ -1196,7 +1216,7 @@ static inline int page_mapcount(struct p
 	int mapcount = atomic_read(&page->_mapcount) + 1;
 
 	/* Handle page_has_type() pages */
-	if (mapcount < 0)
+	if (mapcount < PAGE_MAPCOUNT_RESERVE + 1)
 		mapcount = 0;
 	if (unlikely(PageCompound(page)))
 		mapcount += folio_entire_mapcount(page_folio(page));
@@ -3031,7 +3051,6 @@ static inline bool pagetable_pmd_ctor(st
 	if (!pmd_ptlock_init(ptdesc))
 		return false;
 	__folio_set_pgtable(folio);
-	ptdesc_pmd_pts_init(ptdesc);
 	lruvec_stat_add_folio(folio, NR_PAGETABLE);
 	return true;
 }
Index: kernel-rpi-6_6/mm/vmscan.c
===================================================================
--- kernel-rpi-6_6.orig/mm/vmscan.c
+++ kernel-rpi-6_6/mm/vmscan.c
@@ -641,14 +641,7 @@ unsigned long zone_reclaimable_pages(str
 	if (can_reclaim_anon_pages(NULL, zone_to_nid(zone), NULL))
 		nr += zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_ANON) +
 			zone_page_state_snapshot(zone, NR_ZONE_ACTIVE_ANON);
-	/*
-	 * If there are no reclaimable file-backed or anonymous pages,
-	 * ensure zones with sufficient free pages are not skipped.
-	 * This prevents zones like DMA32 from being ignored in reclaim
-	 * scenarios where they can still help alleviate memory pressure.
-	 */
-	if (nr == 0)
-		nr = zone_page_state_snapshot(zone, NR_FREE_PAGES);
+
 	return nr;
 }
 
@@ -1538,6 +1531,15 @@ static enum folio_references folio_check
 					   &vm_flags);
 	referenced_folio = folio_test_clear_referenced(folio);
 
+	if (lru_gen_enabled()) {
+		int gen = lru_raw_gen_from_flags(READ_ONCE(folio->flags));
+
+		VM_WARN_ON_ONCE_FOLIO(gen < ISOLATED_FOLIO_MIN, folio);
+
+		if (gen > ISOLATED_FOLIO_MIN)
+			referenced_ptes += gen - ISOLATED_FOLIO_MIN;
+	}
+
 	/*
 	 * The supposedly reclaimable folio was found to be in a VM_LOCKED vma.
 	 * Let the folio, now marked Mlocked, be moved to the unevictable list.
@@ -1754,11 +1756,6 @@ retry:
 		if (!sc->may_unmap && folio_mapped(folio))
 			goto keep_locked;
 
-		/* folio_update_gen() tried to promote this page? */
-		if (lru_gen_enabled() && !ignore_references &&
-		    folio_mapped(folio) && folio_test_referenced(folio))
-			goto keep_locked;
-
 		/*
 		 * The number of dirty pages determines if a node is marked
 		 * reclaim_congested. kswapd will stall and start writing
@@ -3223,6 +3220,8 @@ static bool can_age_anon_pages(struct pg
 
 #ifdef CONFIG_LRU_GEN
 
+static struct kernfs_node *lru_gen_admin_node;
+
 #ifdef CONFIG_LRU_GEN_ENABLED
 DEFINE_STATIC_KEY_ARRAY_TRUE(lru_gen_caps, NR_LRU_GEN_CAPS);
 #define get_cap(cap)	static_branch_likely(&lru_gen_caps[cap])
@@ -3248,7 +3247,10 @@ static bool should_clear_pmd_young(void)
 #define LRU_REFS_FLAGS	(BIT(PG_referenced) | BIT(PG_workingset))
 
 #define DEFINE_MAX_SEQ(lruvec)						\
-	unsigned long max_seq = READ_ONCE((lruvec)->lrugen.max_seq)
+	unsigned long max_seq[ANON_AND_FILE] = {			\
+		READ_ONCE((lruvec)->lrugen.max_seq[LRU_GEN_ANON]),	\
+		READ_ONCE((lruvec)->lrugen.max_seq[LRU_GEN_FILE]),	\
+	}
 
 #define DEFINE_MIN_SEQ(lruvec)						\
 	unsigned long min_seq[ANON_AND_FILE] = {			\
@@ -3301,15 +3303,20 @@ static int get_swappiness(struct lruvec
 
 static int get_nr_gens(struct lruvec *lruvec, int type)
 {
-	return lruvec->lrugen.max_seq - lruvec->lrugen.min_seq[type] + 1;
+	return lruvec->lrugen.max_seq[type] - lruvec->lrugen.min_seq[type] + 1;
 }
 
 static bool __maybe_unused seq_is_valid(struct lruvec *lruvec)
 {
-	/* see the comment on lru_gen_folio */
-	return get_nr_gens(lruvec, LRU_GEN_FILE) >= MIN_NR_GENS &&
-	       get_nr_gens(lruvec, LRU_GEN_FILE) <= get_nr_gens(lruvec, LRU_GEN_ANON) &&
-	       get_nr_gens(lruvec, LRU_GEN_ANON) <= MAX_NR_GENS;
+	int type;
+
+	for (type = ANON_AND_FILE - 1; type >= 0; type--) {
+		int nr_gens = get_nr_gens(lruvec, type);
+
+		if (nr_gens < MIN_NR_GENS || nr_gens > MAX_NR_GENS)
+			return false;
+	}
+	return true;
 }
 
 /******************************************************************************
@@ -3526,7 +3533,7 @@ static void reset_mm_stats(struct lruvec
 	lockdep_assert_held(&get_mm_list(lruvec_memcg(lruvec))->lock);
 
 	if (walk) {
-		hist = lru_hist_from_seq(walk->max_seq);
+		hist = lru_hist_from_seq(walk->scan_seq);
 
 		for (i = 0; i < NR_MM_STATS; i++) {
 			WRITE_ONCE(lruvec->mm_state.stats[hist][i],
@@ -3536,7 +3543,7 @@ static void reset_mm_stats(struct lruvec
 	}
 
 	if (NR_HIST_GENS > 1 && last) {
-		hist = lru_hist_from_seq(lruvec->mm_state.seq + 1);
+		hist = lru_hist_from_seq(lruvec->mm_state.scan_seq);
 
 		for (i = 0; i < NR_MM_STATS; i++)
 			WRITE_ONCE(lruvec->mm_state.stats[hist][i], 0);
@@ -3567,6 +3574,14 @@ static bool should_skip_mm(struct mm_str
 	return !mmget_not_zero(mm);
 }
 
+/*
+ * There is a window between when the last mm of the current iteration is
+ * claimed and when the aging process actually completes. During this
+ * window, should_run_aging() will likely still try to run aging. Use a
+ * special scan_seq value to avoid scanning based on that stale result.
+ */
+#define SCAN_ITER_FINISHING ULONG_MAX
+
 static bool iterate_mm_list(struct lruvec *lruvec, struct lru_gen_mm_walk *walk,
 			    struct mm_struct **iter)
 {
@@ -3578,9 +3593,9 @@ static bool iterate_mm_list(struct lruve
 	struct lru_gen_mm_state *mm_state = &lruvec->mm_state;
 
 	/*
-	 * mm_state->seq is incremented after each iteration of mm_list. There
-	 * are three interesting cases for this page table walker:
-	 * 1. It tries to start a new iteration with a stale max_seq: there is
+	 * mm_state->scan_seq is incremented after each iteration of mm_list.
+	 * There are three interesting cases for this page table walker:
+	 * 1. It tries to start a new iteration with a stale scan_seq: there is
 	 *    nothing left to do.
 	 * 2. It started the next iteration: it needs to reset the Bloom filter
 	 *    so that a fresh set of PTE tables can be recorded.
@@ -3589,9 +3604,9 @@ static bool iterate_mm_list(struct lruve
 	 */
 	spin_lock(&mm_list->lock);
 
-	VM_WARN_ON_ONCE(mm_state->seq + 1 < walk->max_seq);
+	VM_WARN_ON_ONCE(mm_state->scan_seq < walk->scan_seq);
 
-	if (walk->max_seq <= mm_state->seq)
+	if (walk->scan_seq < mm_state->scan_seq)
 		goto done;
 
 	if (!mm_state->head)
@@ -3603,7 +3618,7 @@ static bool iterate_mm_list(struct lruve
 	do {
 		mm_state->head = mm_state->head->next;
 		if (mm_state->head == &mm_list->fifo) {
-			WRITE_ONCE(mm_state->seq, mm_state->seq + 1);
+			WRITE_ONCE(mm_state->scan_seq, SCAN_ITER_FINISHING);
 			last = true;
 			break;
 		}
@@ -3625,7 +3640,7 @@ done:
 	spin_unlock(&mm_list->lock);
 
 	if (mm && first)
-		reset_bloom_filter(lruvec, walk->max_seq + 1);
+		reset_bloom_filter(lruvec, walk->scan_seq + 1);
 
 	if (*iter)
 		mmput_async(*iter);
@@ -3635,7 +3650,7 @@ done:
 	return last;
 }
 
-static bool iterate_mm_list_nowalk(struct lruvec *lruvec, unsigned long max_seq)
+static bool iterate_mm_list_nowalk(struct lruvec *lruvec, unsigned long scan_seq)
 {
 	bool success = false;
 	struct mem_cgroup *memcg = lruvec_memcg(lruvec);
@@ -3644,12 +3659,12 @@ static bool iterate_mm_list_nowalk(struc
 
 	spin_lock(&mm_list->lock);
 
-	VM_WARN_ON_ONCE(mm_state->seq + 1 < max_seq);
+	VM_WARN_ON_ONCE(mm_state->scan_seq < scan_seq);
 
-	if (max_seq > mm_state->seq) {
+	if (scan_seq >= mm_state->scan_seq) {
 		mm_state->head = NULL;
 		mm_state->tail = NULL;
-		WRITE_ONCE(mm_state->seq, mm_state->seq + 1);
+		WRITE_ONCE(mm_state->scan_seq, SCAN_ITER_FINISHING);
 		reset_mm_stats(lruvec, NULL, true);
 		success = true;
 	}
@@ -3664,32 +3679,31 @@ static bool iterate_mm_list_nowalk(struc
  ******************************************************************************/
 
 /*
- * A feedback loop based on Proportional-Integral-Derivative (PID) controller.
+ * To decide between two eviction targets X and Y, the target whose pages have
+ * a higher estimated cost for retention is chosen. The cost of retaining a
+ * page in X is the sum of:
  *
- * The P term is refaulted/(evicted+protected) from a tier in the generation
- * currently being evicted; the I term is the exponential moving average of the
- * P term over the generations previously evicted, using the smoothing factor
- * 1/2; the D term isn't supported.
+ *  - The cost paid so far to retain the page so far. This is estimated as the
+ *    number of pages in Y that were evicted in favor of retaining pages in X
+ *    but which have since been refaulted, divided by the original size of X
+ *    to amortize the cost across all retained pages.
+ *  - The predicted cost of evicting a page in Y instead of a page in X. This
+ *    is the refault rate of Y.
  *
- * The setpoint (SP) is always the first tier of one type; the process variable
- * (PV) is either any tier of the other type or any other tier of the same
- * type.
+ * The estimated cost of retention is then multiplied by a constant gain factor.
  *
- * The error is the difference between the SP and the PV; the correction is to
- * turn off protection when SP>PV or turn on protection when SP<PV.
- *
- * For future optimizations:
- * 1. The D term may discount the other two terms over time so that long-lived
- *    generations can resist stale information.
+ * TODO: consider evicted+protected+cur_size instead of oldest_gen_size
  */
 struct ctrl_pos {
 	unsigned long refaulted;
 	unsigned long total;
+	unsigned long num_victims;
+	unsigned long gen_size;
 	int gain;
 };
 
 static void read_ctrl_pos(struct lruvec *lruvec, int type, int tier, int gain,
-			  struct ctrl_pos *pos)
+			  bool has_victims, struct ctrl_pos *pos)
 {
 	struct lru_gen_folio *lrugen = &lruvec->lrugen;
 	int hist = lru_hist_from_seq(lrugen->min_seq[type]);
@@ -3701,55 +3715,96 @@ static void read_ctrl_pos(struct lruvec
 	if (tier)
 		pos->total += lrugen->protected[hist][type][tier - 1];
 	pos->gain = gain;
+
+	if (has_victims) {
+		pos->num_victims = atomic_long_read(&lrugen->refaulted_victims[hist][type]);
+		pos->gen_size = lrugen->oldest_gen_size[hist][type];
+	} else {
+		pos->num_victims = 0;
+		pos->gen_size = 1;
+	}
 }
 
-static void reset_ctrl_pos(struct lruvec *lruvec, int type, bool carryover)
+static void reset_histograms(struct lruvec *lruvec, int type, unsigned long seq)
 {
 	int hist, tier;
 	struct lru_gen_folio *lrugen = &lruvec->lrugen;
-	bool clear = carryover ? NR_HIST_GENS == 1 : NR_HIST_GENS > 1;
-	unsigned long seq = carryover ? lrugen->min_seq[type] : lrugen->max_seq + 1;
 
 	lockdep_assert_held(&lruvec->lru_lock);
+	hist = lru_hist_from_seq(seq);
 
-	if (!carryover && !clear)
-		return;
+	for (tier = 0; tier < MAX_NR_TIERS; tier++) {
+		atomic_long_set(&lrugen->refaulted[hist][type][tier], 0);
+		atomic_long_set(&lrugen->evicted[hist][type][tier], 0);
+		if (tier)
+			WRITE_ONCE(lrugen->protected[hist][type][tier - 1], 0);
+	}
+	atomic_long_set(&lrugen->refaulted_victims[hist][type], 0);
 
-	hist = lru_hist_from_seq(seq);
+	WRITE_ONCE(lrugen->victim_seq[hist][type], 0);
+	WRITE_ONCE(lrugen->oldest_gen_size[hist][type], 0);
+}
+
+static void reset_ctrl_pos(struct lruvec *lruvec, int type)
+{
+	int hist, tier;
+	struct lru_gen_folio *lrugen = &lruvec->lrugen;
+	unsigned long carry_from_seq = lrugen->min_seq[type];
+	unsigned long next_seq = carry_from_seq + 1;
+	unsigned long next_gen = lru_gen_from_seq(next_seq);
+	long total_nr_pages = 0;
+
+	lockdep_assert_held(&lruvec->lru_lock);
+
+	hist = lru_hist_from_seq(carry_from_seq);
 
 	for (tier = 0; tier < MAX_NR_TIERS; tier++) {
-		if (carryover) {
-			unsigned long sum;
+		unsigned long sum;
 
-			sum = lrugen->avg_refaulted[type][tier] +
-			      atomic_long_read(&lrugen->refaulted[hist][type][tier]);
-			WRITE_ONCE(lrugen->avg_refaulted[type][tier], sum / 2);
-
-			sum = lrugen->avg_total[type][tier] +
-			      atomic_long_read(&lrugen->evicted[hist][type][tier]);
-			if (tier)
-				sum += lrugen->protected[hist][type][tier - 1];
-			WRITE_ONCE(lrugen->avg_total[type][tier], sum / 2);
-		}
-
-		if (clear) {
-			atomic_long_set(&lrugen->refaulted[hist][type][tier], 0);
-			atomic_long_set(&lrugen->evicted[hist][type][tier], 0);
-			if (tier)
-				WRITE_ONCE(lrugen->protected[hist][type][tier - 1], 0);
-		}
+		sum = lrugen->avg_refaulted[type][tier] +
+		      atomic_long_read(&lrugen->refaulted[hist][type][tier]);
+		WRITE_ONCE(lrugen->avg_refaulted[type][tier], sum / 2);
+
+		sum = lrugen->avg_total[type][tier] +
+		      atomic_long_read(&lrugen->evicted[hist][type][tier]);
+		if (tier)
+			sum += lrugen->protected[hist][type][tier - 1];
+		WRITE_ONCE(lrugen->avg_total[type][tier], sum / 2);
+
+		total_nr_pages += lrugen->nr_pages[next_gen][type][tier];
 	}
+	/* nr_pages is eventually consistent, so fix up the estimate if it's negative. */
+	total_nr_pages = max(total_nr_pages, 0);
+
+	if (NR_HIST_GENS == 1)
+		reset_histograms(lruvec, type, carry_from_seq);
+
+	hist = lru_hist_from_seq(next_seq);
+	WRITE_ONCE(lrugen->victim_seq[hist][type], READ_ONCE(lrugen->min_seq[!type]));
+	WRITE_ONCE(lrugen->oldest_gen_size[hist][type], total_nr_pages);
+}
+
+/* Constant multiplier for cost calculations, to allow integer division. */
+#define COST_SHIFT 16
+
+static unsigned long retain_cost(struct ctrl_pos *retain, struct ctrl_pos *evict)
+{
+	unsigned long unnecessary_refaults, potential_refault;
+	unsigned long total_size = retain->gen_size + evict->gen_size;
+
+	unnecessary_refaults = (retain->num_victims << COST_SHIFT) / (retain->gen_size + 1);
+	potential_refault = (evict->refaulted << COST_SHIFT) / (evict->total + 1);
+	potential_refault = potential_refault * retain->gen_size / (total_size + 1);
+	return retain->gain * (unnecessary_refaults + potential_refault);
 }
 
 static bool positive_ctrl_err(struct ctrl_pos *sp, struct ctrl_pos *pv)
 {
-	/*
-	 * Return true if the PV has a limited number of refaults or a lower
-	 * refaulted/total than the SP.
-	 */
-	return pv->refaulted < MIN_LRU_BATCH ||
-	       pv->refaulted * (sp->total + MIN_LRU_BATCH) * sp->gain <=
-	       (sp->refaulted + 1) * pv->total * pv->gain;
+	if (get_cap(LRU_GEN_ADVANCE_IN_LOCKSTEP))
+		return pv->refaulted < MIN_LRU_BATCH ||
+		       pv->refaulted * (sp->total + MIN_LRU_BATCH) * sp->gain <=
+		       (sp->refaulted + 1) * pv->total * pv->gain;
+	return retain_cost(pv, sp) > retain_cost(sp, pv);
 }
 
 /******************************************************************************
@@ -3757,26 +3812,117 @@ static bool positive_ctrl_err(struct ctr
  ******************************************************************************/
 
 /* promote pages accessed through page tables */
-static int folio_update_gen(struct folio *folio, int gen)
+static int folio_update_gen_active(struct folio *folio, int gen)
 {
 	unsigned long new_flags, old_flags = READ_ONCE(folio->flags);
+	int old_gen;
 
-	VM_WARN_ON_ONCE(gen >= MAX_NR_GENS);
+	VM_WARN_ON_ONCE(gen >= (int) MAX_NR_GENS);
 	VM_WARN_ON_ONCE(!rcu_read_lock_held());
 
 	do {
+		old_gen = lru_raw_gen_from_flags(old_flags);
+		/*
+		 * ptes are created before pages are added to the lru, so we can
+		 * come across a page with no generation when walking page
+		 * tables. Newly added pages are marked active by folio_add_lru(),
+		 * so they will already be inserted into the newest generation.
+		 */
+		if (old_gen == -1)
+			break;
+
 		/* lru_gen_del_folio() has isolated this page? */
-		if (!(old_flags & LRU_GEN_MASK)) {
-			/* for shrink_folio_list() */
-			new_flags = old_flags | BIT(PG_referenced);
+		if (old_gen >= ISOLATED_FOLIO_MIN) {
+			unsigned long new_gen = min(old_gen + 1, (int) ISOLATED_FOLIO_MAX);
+
+			/* for folio_check_references() */
+			new_flags = old_flags & ~LRU_GEN_MASK;
+			new_flags |= (new_gen + 1) << LRU_GEN_PGOFF;
+			old_gen = -1;
 			continue;
 		}
 
+		if (gen < 0)
+			break;
+
 		new_flags = old_flags & ~(LRU_GEN_MASK | LRU_REFS_MASK | LRU_REFS_FLAGS);
 		new_flags |= (gen + 1UL) << LRU_GEN_PGOFF;
 	} while (!try_cmpxchg(&folio->flags, &old_flags, new_flags));
 
-	return ((old_flags & LRU_GEN_MASK) >> LRU_GEN_PGOFF) - 1;
+	return old_gen;
+}
+
+/* promote pages accessed through page tables */
+static int folio_update_gen(struct folio *folio, int active_gen,
+			    int *new_gen, bool *activate_out)
+{
+	unsigned long new_flags, old_flags = READ_ONCE(folio->flags);
+	int old_gen;
+	bool activate;
+
+	if (lru_gen_aggressive_mm()) {
+		*new_gen = active_gen;
+		if (activate_out)
+			*activate_out = true;
+		return folio_update_gen_active(folio, active_gen);
+	}
+
+	VM_WARN_ON_ONCE(active_gen >= (int) MAX_NR_GENS);
+	VM_WARN_ON_ONCE(!rcu_read_lock_held());
+
+	do {
+		old_gen = lru_raw_gen_from_flags(old_flags);
+
+		/* lru_gen_del_folio() has isolated this page? */
+		if (old_gen >= (int) ISOLATED_FOLIO_MIN) {
+			unsigned long new_gen = min(old_gen + 1, (int) ISOLATED_FOLIO_MAX);
+
+			/* for folio_check_references() */
+			new_flags = old_flags & ~LRU_GEN_MASK;
+			new_flags |= (new_gen + 1UL) << LRU_GEN_PGOFF;
+			old_gen = -1;
+			continue;
+		}
+
+		activate = old_flags & BIT(PG_referenced);
+
+		if (activate)
+			new_flags = old_flags & ~BIT(PG_referenced);
+		else
+			new_flags = old_flags | BIT(PG_referenced);
+
+		/*
+		 * ptes are created before pages are added to the lru, so we can
+		 * come across a page with no generation when walking page
+		 * tables. Set the active flag if necessary so they get inserted
+		 * into the newest generation.
+		 */
+		if (old_gen == -1) {
+			if (activate)
+				new_flags |= BIT(PG_active);
+		} else if (active_gen >= 0) {
+			int new_gen;
+
+			if (activate) {
+				new_gen = active_gen;
+			} else if (old_gen == active_gen) {
+				new_gen = old_gen;
+			} else {
+				new_gen = (old_gen + 1) % MAX_NR_GENS;
+				if (new_gen == active_gen)
+					new_gen = old_gen;
+			}
+
+			new_flags &= ~(LRU_GEN_MASK | LRU_REFS_MASK | BIT(PG_workingset));
+			new_flags |= (new_gen + 1UL) << LRU_GEN_PGOFF;
+		}
+	} while (!try_cmpxchg(&folio->flags, &old_flags, new_flags));
+
+	if (activate_out)
+		*activate_out = activate;
+
+	*new_gen = lru_raw_gen_from_flags(new_flags);
+	return old_gen;
 }
 
 /* protect pages accessed multiple times through file descriptors */
@@ -3787,7 +3933,7 @@ static int folio_inc_gen(struct lruvec *
 	int new_gen, old_gen = lru_gen_from_seq(lrugen->min_seq[type]);
 	unsigned long new_flags, old_flags = READ_ONCE(folio->flags);
 
-	VM_WARN_ON_ONCE_FOLIO(!(old_flags & LRU_GEN_MASK), folio);
+	VM_WARN_ON_ONCE_FOLIO(folio_lru_gen(folio) == -1, folio);
 
 	do {
 		new_gen = ((old_flags & LRU_GEN_MASK) >> LRU_GEN_PGOFF) - 1;
@@ -3843,7 +3989,7 @@ static void reset_batch_size(struct lruv
 		WRITE_ONCE(lrugen->nr_pages[gen][type][zone],
 			   lrugen->nr_pages[gen][type][zone] + delta);
 
-		if (lru_gen_is_active(lruvec, gen))
+		if (lru_gen_is_active(lruvec, gen, type))
 			lru += LRU_ACTIVE;
 		__update_lru_size(lruvec, lru, zone, delta);
 	}
@@ -3999,7 +4145,7 @@ static bool walk_pte_range(pmd_t *pmd, u
 	struct lru_gen_mm_walk *walk = args->private;
 	struct mem_cgroup *memcg = lruvec_memcg(walk->lruvec);
 	struct pglist_data *pgdat = lruvec_pgdat(walk->lruvec);
-	int old_gen, new_gen = lru_gen_from_seq(walk->max_seq);
+	int old_gen, new_gen, active_gen;
 
 	pte = pte_offset_map_nolock(args->mm, pmd, start & PMD_MASK, &ptl);
 	if (!pte)
@@ -4043,7 +4189,8 @@ restart:
 		      !folio_test_swapcache(folio)))
 			folio_mark_dirty(folio);
 
-		old_gen = folio_update_gen(folio, new_gen);
+		active_gen = lru_gen_from_seq(walk->max_seq[folio_is_file_lru(folio)]);
+		old_gen = folio_update_gen(folio, active_gen, &new_gen, NULL);
 		if (old_gen >= 0 && old_gen != new_gen)
 			update_batch_size(walk, folio, old_gen, new_gen);
 	}
@@ -4067,7 +4214,7 @@ static void walk_pmd_range_locked(pud_t
 	struct lru_gen_mm_walk *walk = args->private;
 	struct mem_cgroup *memcg = lruvec_memcg(walk->lruvec);
 	struct pglist_data *pgdat = lruvec_pgdat(walk->lruvec);
-	int old_gen, new_gen = lru_gen_from_seq(walk->max_seq);
+	int old_gen, new_gen, active_gen;
 
 	VM_WARN_ON_ONCE(pud_leaf(*pud));
 
@@ -4123,7 +4270,8 @@ static void walk_pmd_range_locked(pud_t
 		      !folio_test_swapcache(folio)))
 			folio_mark_dirty(folio);
 
-		old_gen = folio_update_gen(folio, new_gen);
+		active_gen = lru_gen_from_seq(walk->max_seq[folio_is_file_lru(folio)]);
+		old_gen = folio_update_gen(folio, active_gen, &new_gen, NULL);
 		if (old_gen >= 0 && old_gen != new_gen)
 			update_batch_size(walk, folio, old_gen, new_gen);
 next:
@@ -4204,7 +4352,7 @@ restart:
 			walk_pmd_range_locked(pud, addr, vma, args, bitmap, &first);
 		}
 
-		if (!walk->force_scan && !test_bloom_filter(walk->lruvec, walk->max_seq, pmd + i))
+		if (!walk->force_scan && !test_bloom_filter(walk->lruvec, walk->scan_seq, pmd + i))
 			continue;
 
 		walk->mm_stats[MM_NONLEAF_FOUND]++;
@@ -4215,7 +4363,7 @@ restart:
 		walk->mm_stats[MM_NONLEAF_ADDED]++;
 
 		/* carry over to the next generation */
-		update_bloom_filter(walk->lruvec, walk->max_seq + 1, pmd + i);
+		update_bloom_filter(walk->lruvec, walk->scan_seq + 1, pmd + i);
 	}
 
 	walk_pmd_range_locked(pud, -1, vma, args, bitmap, &first);
@@ -4280,12 +4428,12 @@ static void walk_mm(struct lruvec *lruve
 	walk->next_addr = FIRST_USER_ADDRESS;
 
 	do {
-		DEFINE_MAX_SEQ(lruvec);
+		unsigned long scan_seq = READ_ONCE(lruvec->mm_state.scan_seq);
 
 		err = -EBUSY;
 
 		/* another thread might have called inc_max_seq() */
-		if (walk->max_seq != max_seq)
+		if (walk->scan_seq != scan_seq)
 			break;
 
 		/* folio_update_gen() requires stable folio_memcg() */
@@ -4373,7 +4521,7 @@ static bool inc_min_seq(struct lruvec *l
 		}
 	}
 done:
-	reset_ctrl_pos(lruvec, type, true);
+	reset_ctrl_pos(lruvec, type);
 	WRITE_ONCE(lrugen->min_seq[type], lrugen->min_seq[type] + 1);
 
 	return true;
@@ -4385,12 +4533,13 @@ static bool try_to_inc_min_seq(struct lr
 	bool success = false;
 	struct lru_gen_folio *lrugen = &lruvec->lrugen;
 	DEFINE_MIN_SEQ(lruvec);
+	DEFINE_MAX_SEQ(lruvec);
 
 	VM_WARN_ON_ONCE(!seq_is_valid(lruvec));
 
 	/* find the oldest populated generation */
 	for (type = !can_swap; type < ANON_AND_FILE; type++) {
-		while (min_seq[type] + MIN_NR_GENS <= lrugen->max_seq) {
+		while (min_seq[type] + MIN_NR_GENS <= lrugen->max_seq[type]) {
 			gen = lru_gen_from_seq(min_seq[type]);
 
 			for (zone = 0; zone < MAX_NR_ZONES; zone++) {
@@ -4404,17 +4553,21 @@ next:
 		;
 	}
 
-	/* see the comment on lru_gen_folio */
-	if (can_swap) {
-		min_seq[LRU_GEN_ANON] = min(min_seq[LRU_GEN_ANON], min_seq[LRU_GEN_FILE]);
-		min_seq[LRU_GEN_FILE] = max(min_seq[LRU_GEN_ANON], lrugen->min_seq[LRU_GEN_FILE]);
+	if (get_cap(LRU_GEN_ADVANCE_IN_LOCKSTEP) && can_swap) {
+		unsigned long anon_gen_diff = max_seq[LRU_GEN_ANON] - min_seq[LRU_GEN_ANON];
+		unsigned long file_gen_diff = max_seq[LRU_GEN_FILE] - min_seq[LRU_GEN_FILE];
+
+		anon_gen_diff = max(anon_gen_diff, file_gen_diff);
+		min_seq[LRU_GEN_ANON] = max_seq[LRU_GEN_ANON] - anon_gen_diff;
+		min_seq[LRU_GEN_FILE] = max_seq[LRU_GEN_FILE] - anon_gen_diff;
+		min_seq[LRU_GEN_FILE] = max(min_seq[LRU_GEN_FILE], lrugen->min_seq[LRU_GEN_FILE]);
 	}
 
 	for (type = !can_swap; type < ANON_AND_FILE; type++) {
 		if (min_seq[type] == lrugen->min_seq[type])
 			continue;
 
-		reset_ctrl_pos(lruvec, type, true);
+		reset_ctrl_pos(lruvec, type);
 		WRITE_ONCE(lrugen->min_seq[type], min_seq[type]);
 		success = true;
 	}
@@ -4422,28 +4575,22 @@ next:
 	return success;
 }
 
-static void inc_max_seq(struct lruvec *lruvec, bool can_swap, bool force_scan)
+static void inc_max_seq(struct lruvec *lruvec, int type, bool can_swap, bool force_scan)
 {
-	int prev, next;
-	int type, zone;
+	int prev, next, zone;
 	struct lru_gen_folio *lrugen = &lruvec->lrugen;
 restart:
 	spin_lock_irq(&lruvec->lru_lock);
-
 	VM_WARN_ON_ONCE(!seq_is_valid(lruvec));
 
-	for (type = ANON_AND_FILE - 1; type >= 0; type--) {
-		if (get_nr_gens(lruvec, type) != MAX_NR_GENS)
-			continue;
-
+	if (get_nr_gens(lruvec, type) == MAX_NR_GENS) {
 		VM_WARN_ON_ONCE(!force_scan && (type == LRU_GEN_FILE || can_swap));
 
-		if (inc_min_seq(lruvec, type, can_swap))
-			continue;
-
-		spin_unlock_irq(&lruvec->lru_lock);
-		cond_resched();
-		goto restart;
+		if (!inc_min_seq(lruvec, type, can_swap)) {
+			spin_unlock_irq(&lruvec->lru_lock);
+			cond_resched();
+			goto restart;
+		}
 	}
 
 	/*
@@ -4452,45 +4599,47 @@ restart:
 	 * with min_seq[LRU_GEN_ANON] if swapping is constrained. And if they do
 	 * overlap, cold/hot inversion happens.
 	 */
-	prev = lru_gen_from_seq(lrugen->max_seq - 1);
-	next = lru_gen_from_seq(lrugen->max_seq + 1);
+	prev = lru_gen_from_seq(lrugen->max_seq[type] - 1);
+	next = lru_gen_from_seq(lrugen->max_seq[type] + 1);
 
-	for (type = 0; type < ANON_AND_FILE; type++) {
-		for (zone = 0; zone < MAX_NR_ZONES; zone++) {
-			enum lru_list lru = type * LRU_INACTIVE_FILE;
-			long delta = lrugen->nr_pages[prev][type][zone] -
-				     lrugen->nr_pages[next][type][zone];
+	for (zone = 0; zone < MAX_NR_ZONES; zone++) {
+		enum lru_list lru = type * LRU_INACTIVE_FILE;
+		long delta = lrugen->nr_pages[prev][type][zone] -
+			     lrugen->nr_pages[next][type][zone];
 
-			if (!delta)
-				continue;
+		if (!delta)
+			continue;
 
-			__update_lru_size(lruvec, lru, zone, delta);
-			__update_lru_size(lruvec, lru + LRU_ACTIVE, zone, -delta);
-		}
+		__update_lru_size(lruvec, lru, zone, delta);
+		__update_lru_size(lruvec, lru + LRU_ACTIVE, zone, -delta);
 	}
 
-	for (type = 0; type < ANON_AND_FILE; type++)
-		reset_ctrl_pos(lruvec, type, false);
+	if (NR_HIST_GENS > 1)
+		reset_histograms(lruvec, type, lrugen->max_seq[type] + 1);
 
-	WRITE_ONCE(lrugen->timestamps[next], jiffies);
+	WRITE_ONCE(lrugen->timestamps[next][type], jiffies);
 	/* make sure preceding modifications appear */
-	smp_store_release(&lrugen->max_seq, lrugen->max_seq + 1);
+	smp_store_release(&lrugen->max_seq[type], lrugen->max_seq[type] + 1);
 
 	spin_unlock_irq(&lruvec->lru_lock);
 }
 
-static bool try_to_inc_max_seq(struct lruvec *lruvec, unsigned long max_seq,
+static bool try_to_inc_max_seq(struct lruvec *lruvec, unsigned long *max_seq,
+			       unsigned long scan_seq, bool *should_age,
 			       struct scan_control *sc, bool can_swap, bool force_scan)
 {
 	bool success;
+	int type;
 	struct lru_gen_mm_walk *walk;
 	struct mm_struct *mm = NULL;
 	struct lru_gen_folio *lrugen = &lruvec->lrugen;
 
-	VM_WARN_ON_ONCE(max_seq > READ_ONCE(lrugen->max_seq));
+	VM_WARN_ON_ONCE(max_seq[LRU_GEN_ANON] > READ_ONCE(lrugen->max_seq[LRU_GEN_ANON]));
+	VM_WARN_ON_ONCE(max_seq[LRU_GEN_FILE] > READ_ONCE(lrugen->max_seq[LRU_GEN_FILE]));
 
-	/* see the comment in iterate_mm_list() */
-	if (max_seq <= READ_ONCE(lruvec->mm_state.seq)) {
+	/* see the comment on SCAN_ITER_FINISHING and in iterate_mm_list() */
+	if (scan_seq == SCAN_ITER_FINISHING ||
+	    scan_seq < READ_ONCE(lruvec->mm_state.scan_seq)) {
 		success = false;
 		goto done;
 	}
@@ -4502,18 +4651,20 @@ static bool try_to_inc_max_seq(struct lr
 	 * is less efficient, but it avoids bursty page faults.
 	 */
 	if (!should_walk_mmu()) {
-		success = iterate_mm_list_nowalk(lruvec, max_seq);
+		success = iterate_mm_list_nowalk(lruvec, scan_seq);
 		goto done;
 	}
 
 	walk = set_mm_walk(NULL, true);
 	if (!walk) {
-		success = iterate_mm_list_nowalk(lruvec, max_seq);
+		success = iterate_mm_list_nowalk(lruvec, scan_seq);
 		goto done;
 	}
 
 	walk->lruvec = lruvec;
-	walk->max_seq = max_seq;
+	for (type = ANON_AND_FILE - 1; type >= 0; type--)
+		walk->max_seq[type] = max_seq[type];
+	walk->scan_seq = scan_seq;
 	walk->can_swap = can_swap;
 	walk->force_scan = force_scan;
 
@@ -4523,8 +4674,16 @@ static bool try_to_inc_max_seq(struct lr
 			walk_mm(lruvec, mm, walk);
 	} while (mm);
 done:
-	if (success)
-		inc_max_seq(lruvec, can_swap, force_scan);
+	if (success) {
+		for (type = ANON_AND_FILE - 1; type >= 0; type--)
+			if (should_age[type])
+				inc_max_seq(lruvec, type, can_swap, force_scan);
+
+		VM_WARN_ON_ONCE(lruvec->mm_state.scan_seq != SCAN_ITER_FINISHING);
+		WRITE_ONCE(lruvec->mm_state.scan_seq, scan_seq + 1);
+
+		kernfs_notify(lru_gen_admin_node);
+	}
 
 	return success;
 }
@@ -4572,7 +4731,7 @@ static bool lruvec_is_sizable(struct lru
 	for (type = !can_swap; type < ANON_AND_FILE; type++) {
 		unsigned long seq;
 
-		for (seq = min_seq[type]; seq <= max_seq; seq++) {
+		for (seq = min_seq[type]; seq <= max_seq[type]; seq++) {
 			gen = lru_gen_from_seq(seq);
 
 			for (zone = 0; zone < MAX_NR_ZONES; zone++)
@@ -4587,7 +4746,7 @@ static bool lruvec_is_sizable(struct lru
 static bool lruvec_is_reclaimable(struct lruvec *lruvec, struct scan_control *sc,
 				  unsigned long min_ttl)
 {
-	int gen;
+	int gen, type;
 	unsigned long birth;
 	struct mem_cgroup *memcg = lruvec_memcg(lruvec);
 	DEFINE_MIN_SEQ(lruvec);
@@ -4598,11 +4757,15 @@ static bool lruvec_is_reclaimable(struct
 	if (!lruvec_is_sizable(lruvec, sc))
 		return false;
 
-	/* see the comment on lru_gen_folio */
-	gen = lru_gen_from_seq(min_seq[LRU_GEN_FILE]);
-	birth = READ_ONCE(lruvec->lrugen.timestamps[gen]);
+	for (type = 0; type < ANON_AND_FILE; type++) {
+		gen = lru_gen_from_seq(min_seq[type]);
+		birth = READ_ONCE(lruvec->lrugen.timestamps[gen][type]);
 
-	return time_is_before_jiffies(birth + min_ttl);
+		if (time_is_after_jiffies(birth + min_ttl))
+			return false;
+	}
+
+	return true;
 }
 
 /* to protect the working set of the last N jiffies */
@@ -4670,8 +4833,10 @@ void lru_gen_look_around(struct page_vma
 	struct mem_cgroup *memcg = folio_memcg(folio);
 	struct pglist_data *pgdat = folio_pgdat(folio);
 	struct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);
+	unsigned long scan_seq = READ_ONCE(lruvec->mm_state.scan_seq);
 	DEFINE_MAX_SEQ(lruvec);
-	int old_gen, new_gen = lru_gen_from_seq(max_seq);
+	int type = folio_is_file_lru(folio);
+	int old_gen, new_gen, active_gen = lru_gen_from_seq(max_seq[type]);
 
 	lockdep_assert_held(pvmw->ptl);
 	VM_WARN_ON_ONCE_FOLIO(folio_test_lru(folio), folio);
@@ -4710,6 +4875,7 @@ void lru_gen_look_around(struct page_vma
 
 	for (i = 0, addr = start; addr != end; i++, addr += PAGE_SIZE) {
 		unsigned long pfn;
+		bool activate;
 		pte_t ptent = ptep_get(pte + i);
 
 		pfn = get_pte_pfn(ptent, vma, addr);
@@ -4723,7 +4889,7 @@ void lru_gen_look_around(struct page_vma
 		if (!folio)
 			continue;
 
-		if (!ptep_clear_flush_young(vma, addr, pte + i))
+		if (!ptep_test_and_clear_young(vma, addr, pte + i))
 			VM_WARN_ON_ONCE(true);
 
 		young++;
@@ -4734,18 +4900,20 @@ void lru_gen_look_around(struct page_vma
 			folio_mark_dirty(folio);
 
 		if (walk) {
-			old_gen = folio_update_gen(folio, new_gen);
+			old_gen = folio_update_gen(folio, active_gen, &new_gen, NULL);
 			if (old_gen >= 0 && old_gen != new_gen)
 				update_batch_size(walk, folio, old_gen, new_gen);
 
 			continue;
 		}
 
-		old_gen = folio_lru_gen(folio);
-		if (old_gen < 0)
-			folio_set_referenced(folio);
-		else if (old_gen != new_gen)
-			folio_activate(folio);
+		old_gen = folio_update_gen(folio, -1, &new_gen, &activate);
+		if (old_gen != active_gen) {
+			if (activate)
+				folio_activate(folio);
+			else
+				folio_promote(folio);
+		}
 	}
 
 	arch_leave_lazy_mmu_mode();
@@ -4753,7 +4921,7 @@ void lru_gen_look_around(struct page_vma
 
 	/* feedback from rmap walkers to page table walkers */
 	if (suitable_to_scan(i, young))
-		update_bloom_filter(lruvec, max_seq, pvmw->pmd);
+		update_bloom_filter(lruvec, scan_seq, pvmw->pmd);
 }
 
 /******************************************************************************
@@ -4918,7 +5086,7 @@ static bool sort_folio(struct lruvec *lr
 	int tier = lru_tier_from_refs(refs);
 	struct lru_gen_folio *lrugen = &lruvec->lrugen;
 
-	VM_WARN_ON_ONCE_FOLIO(gen >= MAX_NR_GENS, folio);
+	VM_WARN_ON_ONCE_FOLIO(gen == -1, folio);
 
 	/* unevictable */
 	if (!folio_evictable(folio)) {
@@ -5001,7 +5169,6 @@ static bool isolate_folio(struct lruvec
 
 	/* for shrink_folio_list() */
 	folio_clear_reclaim(folio);
-	folio_clear_referenced(folio);
 
 	success = lru_gen_del_folio(lruvec, folio, true);
 	VM_WARN_ON_ONCE_FOLIO(!success, folio);
@@ -5095,9 +5262,9 @@ static int get_tier_idx(struct lruvec *l
 	 * This value is chosen because any other tier would have at least twice
 	 * as many refaults as the first tier.
 	 */
-	read_ctrl_pos(lruvec, type, 0, 1, &sp);
+	read_ctrl_pos(lruvec, type, 0, 1, false, &sp);
 	for (tier = 1; tier < MAX_NR_TIERS; tier++) {
-		read_ctrl_pos(lruvec, type, tier, 2, &pv);
+		read_ctrl_pos(lruvec, type, tier, 2, false, &pv);
 		if (!positive_ctrl_err(&sp, &pv))
 			break;
 	}
@@ -5117,13 +5284,13 @@ static int get_type_to_scan(struct lruve
 	 * with the first tier of the other type to determine the last tier (of
 	 * the selected type) to evict.
 	 */
-	read_ctrl_pos(lruvec, LRU_GEN_ANON, 0, gain[LRU_GEN_ANON], &sp);
-	read_ctrl_pos(lruvec, LRU_GEN_FILE, 0, gain[LRU_GEN_FILE], &pv);
+	read_ctrl_pos(lruvec, LRU_GEN_ANON, 0, gain[LRU_GEN_ANON], true, &sp);
+	read_ctrl_pos(lruvec, LRU_GEN_FILE, 0, gain[LRU_GEN_FILE], true, &pv);
 	type = positive_ctrl_err(&sp, &pv);
 
-	read_ctrl_pos(lruvec, !type, 0, gain[!type], &sp);
+	read_ctrl_pos(lruvec, !type, 0, gain[!type], true, &sp);
 	for (tier = 1; tier < MAX_NR_TIERS; tier++) {
-		read_ctrl_pos(lruvec, type, tier, gain[type], &pv);
+		read_ctrl_pos(lruvec, type, tier, gain[type], true, &pv);
 		if (!positive_ctrl_err(&sp, &pv))
 			break;
 	}
@@ -5140,16 +5307,15 @@ static int isolate_folios(struct lruvec
 	int type;
 	int scanned;
 	int tier = -1;
-	DEFINE_MIN_SEQ(lruvec);
+	unsigned long anon_num_gens = get_nr_gens(lruvec, LRU_GEN_ANON);
+	unsigned long file_num_gens = get_nr_gens(lruvec, LRU_GEN_FILE);
 
 	/*
-	 * Try to make the obvious choice first. When anon and file are both
-	 * available from the same generation, interpret swappiness 1 as file
-	 * first and 200 as anon first.
+	 * Interpret swappiness 1 as file first and 200 as anon first.
 	 */
 	if (!swappiness)
 		type = LRU_GEN_FILE;
-	else if (min_seq[LRU_GEN_ANON] < min_seq[LRU_GEN_FILE])
+	else if (get_cap(LRU_GEN_ADVANCE_IN_LOCKSTEP) && anon_num_gens > file_num_gens)
 		type = LRU_GEN_ANON;
 	else if (swappiness == 1)
 		type = LRU_GEN_FILE;
@@ -5215,14 +5381,6 @@ retry:
 			continue;
 		}
 
-		if (folio_test_reclaim(folio) &&
-		    (folio_test_dirty(folio) || folio_test_writeback(folio))) {
-			/* restore LRU_REFS_FLAGS cleared by isolate_folio() */
-			if (folio_test_workingset(folio))
-				folio_set_referenced(folio);
-			continue;
-		}
-
 		if (skip_retry || folio_test_active(folio) || folio_test_referenced(folio) ||
 		    folio_mapped(folio) || folio_test_locked(folio) ||
 		    folio_test_dirty(folio) || folio_test_writeback(folio)) {
@@ -5266,38 +5424,38 @@ retry:
 	return scanned;
 }
 
-static bool should_run_aging(struct lruvec *lruvec, unsigned long max_seq,
-			     struct scan_control *sc, bool can_swap, unsigned long *nr_to_scan)
+static bool should_run_aging(struct lruvec *lruvec, int type, unsigned long *max_seq,
+			     unsigned long *min_seq, struct scan_control *sc,
+			     unsigned long *nr_to_scan)
 {
-	int gen, type, zone;
+	int gen, end_type, zone, t_iter;
 	unsigned long old = 0;
 	unsigned long young = 0;
 	unsigned long total = 0;
+	unsigned long seq;
 	struct lru_gen_folio *lrugen = &lruvec->lrugen;
 	struct mem_cgroup *memcg = lruvec_memcg(lruvec);
-	DEFINE_MIN_SEQ(lruvec);
 
 	/* whether this lruvec is completely out of cold folios */
-	if (min_seq[!can_swap] + MIN_NR_GENS > max_seq) {
+	if (min_seq[type] + MIN_NR_GENS > max_seq[type]) {
 		*nr_to_scan = 0;
 		return true;
 	}
 
-	for (type = !can_swap; type < ANON_AND_FILE; type++) {
-		unsigned long seq;
-
-		for (seq = min_seq[type]; seq <= max_seq; seq++) {
+	end_type = get_cap(LRU_GEN_ADVANCE_IN_LOCKSTEP) ? LRU_GEN_FILE : type;
+	for (t_iter = type; t_iter <= end_type; t_iter++) {
+		for (seq = min_seq[t_iter]; seq <= max_seq[t_iter]; seq++) {
 			unsigned long size = 0;
 
 			gen = lru_gen_from_seq(seq);
 
 			for (zone = 0; zone < MAX_NR_ZONES; zone++)
-				size += max(READ_ONCE(lrugen->nr_pages[gen][type][zone]), 0L);
+				size += max(READ_ONCE(lrugen->nr_pages[gen][t_iter][zone]), 0L);
 
 			total += size;
-			if (seq == max_seq)
+			if (seq == max_seq[t_iter])
 				young += size;
-			else if (seq + MIN_NR_GENS == max_seq)
+			else if (seq + MIN_NR_GENS == max_seq[t_iter])
 				old += size;
 		}
 	}
@@ -5315,7 +5473,7 @@ static bool should_run_aging(struct lruv
 	 * stalls when the number of generations reaches MIN_NR_GENS. Hence, the
 	 * ideal number of generations is MIN_NR_GENS+1.
 	 */
-	if (min_seq[!can_swap] + MIN_NR_GENS < max_seq)
+	if (min_seq[type] + MIN_NR_GENS < max_seq[type])
 		return false;
 
 	/*
@@ -5338,24 +5496,51 @@ static bool should_run_aging(struct lruv
  * 1. Defer try_to_inc_max_seq() to workqueues to reduce latency for memcg
  *    reclaim.
  */
-static long get_nr_to_scan(struct lruvec *lruvec, struct scan_control *sc, bool can_swap)
+static long get_nr_to_scan(struct lruvec *lruvec, struct scan_control *sc,
+			   bool can_swap, int *did_age)
 {
-	unsigned long nr_to_scan;
+	unsigned long nr_to_scan[ANON_AND_FILE] = {}, ret;
 	struct mem_cgroup *memcg = lruvec_memcg(lruvec);
 	DEFINE_MAX_SEQ(lruvec);
+	DEFINE_MIN_SEQ(lruvec);
+	unsigned long scan_seq = READ_ONCE(lruvec->mm_state.scan_seq);
+	bool should_age[ANON_AND_FILE] = {};
+	int type;
+	bool max_seq_inc;
 
 	if (mem_cgroup_below_min(sc->target_mem_cgroup, memcg))
 		return -1;
 
-	if (!should_run_aging(lruvec, max_seq, sc, can_swap, &nr_to_scan))
-		return nr_to_scan;
+	for (type = !can_swap; type < ANON_AND_FILE; type++) {
+		if (did_age[type])
+			continue;
+
+		if (get_cap(LRU_GEN_ADVANCE_IN_LOCKSTEP) && can_swap && type == LRU_GEN_FILE) {
+			should_age[type] = should_age[LRU_GEN_ANON];
+			continue;
+		}
+
+		should_age[type] = should_run_aging(lruvec, type, max_seq,
+						    min_seq, sc, nr_to_scan + type);
+	}
 
 	/* skip the aging path at the default priority */
-	if (sc->priority == DEF_PRIORITY)
-		return nr_to_scan;
+	if (sc->priority == DEF_PRIORITY ||
+	    (!should_age[LRU_GEN_ANON] && !should_age[LRU_GEN_FILE]))
+		return nr_to_scan[LRU_GEN_ANON] + nr_to_scan[LRU_GEN_FILE];
+
+	max_seq_inc = try_to_inc_max_seq(lruvec, max_seq, scan_seq,
+					 should_age, sc, can_swap, false);
+	ret = 0;
+	for (type = !can_swap; type < ANON_AND_FILE; type++) {
+		if (max_seq_inc)
+			did_age[type] |= should_age[type];
+		if (!did_age[type] && !should_age[type])
+			ret += nr_to_scan[type];
+	}
 
 	/* skip this lruvec as it's low on cold folios */
-	return try_to_inc_max_seq(lruvec, max_seq, sc, can_swap, false) ? -1 : 0;
+	return ret > 0 ? ret : (max_seq_inc ? -1  : 0);
 }
 
 static bool should_abort_scan(struct lruvec *lruvec, struct scan_control *sc)
@@ -5394,6 +5579,7 @@ static bool try_to_shrink_lruvec(struct
 	long nr_to_scan;
 	unsigned long scanned = 0;
 	int swappiness = get_swappiness(lruvec, sc);
+	int did_age[ANON_AND_FILE] = {};
 
 	/* clean file folios are more likely to exist */
 	if (swappiness && !(sc->gfp_mask & __GFP_IO))
@@ -5402,7 +5588,7 @@ static bool try_to_shrink_lruvec(struct
 	while (true) {
 		int delta;
 
-		nr_to_scan = get_nr_to_scan(lruvec, sc, swappiness);
+		nr_to_scan = get_nr_to_scan(lruvec, sc, swappiness, did_age);
 		if (nr_to_scan <= 0)
 			break;
 
@@ -5610,8 +5796,8 @@ static void lru_gen_shrink_node(struct p
 
 	blk_finish_plug(&plug);
 done:
-	if (sc->nr_reclaimed > reclaimed)
-		pgdat->kswapd_failures = 0;
+	/* kswapd should never fail */
+	pgdat->kswapd_failures = 0;
 }
 
 /******************************************************************************
@@ -5790,6 +5976,12 @@ static ssize_t enabled_show(struct kobje
 	if (should_clear_pmd_young())
 		caps |= BIT(LRU_GEN_NONLEAF_YOUNG);
 
+	if (get_cap(LRU_GEN_ADVANCE_IN_LOCKSTEP))
+		caps |= BIT(LRU_GEN_ADVANCE_IN_LOCKSTEP);
+
+	if (get_cap(LRU_GEN_AGGRESSIVE_MM))
+		caps |= BIT(LRU_GEN_AGGRESSIVE_MM);
+
 	return sysfs_emit(buf, "0x%04x\n", caps);
 }
 
@@ -5823,9 +6015,173 @@ static ssize_t enabled_store(struct kobj
 
 static struct kobj_attribute lru_gen_enabled_attr = __ATTR_RW(enabled);
 
+static int print_node_mglru(struct lruvec *lruvec, char *buf, int orig_pos)
+{
+	unsigned long base_seq[2];
+	unsigned long num_gens, timestamp;
+	int i, type;
+	struct lru_gen_folio *lrugen = &lruvec->lrugen;
+
+	DEFINE_MAX_SEQ(lruvec);
+	DEFINE_MIN_SEQ(lruvec);
+
+	int print_pos = orig_pos;
+	bool needs_newline = false;
+
+	num_gens = max(max_seq[0] - min_seq[0], max_seq[1] - min_seq[1]);
+	base_seq[0] = max_seq[0] - num_gens;
+	base_seq[1] = max_seq[1] - num_gens;
+
+	for (i = 0; i <= num_gens; i++) {
+		for (type = 0; type < ANON_AND_FILE; type++) {
+			int gen, zone;
+			unsigned int msecs;
+			unsigned long seq = base_seq[type] + i;
+			long size = 0;
+
+			if (seq < min_seq[type]) {
+				continue;
+			}
+
+			if (needs_newline)
+				print_pos += snprintf(buf + print_pos, PAGE_SIZE - print_pos, "\n");
+			gen = lru_gen_from_seq(seq);
+			timestamp = READ_ONCE(lrugen->timestamps[gen][type]);
+			msecs = jiffies_to_msecs(jiffies - timestamp);
+			print_pos += snprintf(buf + print_pos, PAGE_SIZE - print_pos,
+			" %10lu %10u", seq, msecs);
+
+			for (zone = 0; zone < MAX_NR_ZONES; zone++)
+				size += READ_ONCE(lrugen->nr_pages[gen][type][zone]);
+
+			print_pos += snprintf(buf + print_pos,
+				PAGE_SIZE - print_pos, " %10lu %10lu ",
+				type == 0 ? max(size, 0L) : 0,
+				type == 1 ? max(size, 0L) : 0);
+			needs_newline = true;
+		}
+
+		print_pos += snprintf(buf + print_pos, PAGE_SIZE - print_pos, "\n");
+
+	}
+
+	return print_pos - orig_pos;
+}
+
+static ssize_t show_lru_gen_admin(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
+{
+	struct lruvec *lruvec;
+	struct mem_cgroup *memcg;
+
+	char *path = kvmalloc(PATH_MAX, GFP_KERNEL);
+	int buf_len = 0;
+
+	if (!path)
+		return -EINVAL;
+	path[0] = 0;
+	buf[0] = 0;
+	memcg = mem_cgroup_iter(NULL, NULL, NULL);
+	do {
+		int nid;
+
+		for_each_node_state(nid, N_MEMORY) {
+			lruvec = mem_cgroup_lruvec(memcg, NODE_DATA(nid));
+			if (lruvec) {
+				if (nid == first_memory_node) {
+#ifdef CONFIG_MEMCG
+					if (memcg)
+						cgroup_path(memcg->css.cgroup, path, PATH_MAX);
+					else
+						path[0] = 0;
+#endif
+					buf_len += snprintf(buf + buf_len, PAGE_SIZE - buf_len,
+						"memcg %5hu %s\n", mem_cgroup_id(memcg), path);
+				}
+
+				buf_len += snprintf(buf + buf_len, PAGE_SIZE - buf_len,
+					" node %5d\n", nid);
+				buf_len += print_node_mglru(lruvec, buf, buf_len);
+			}
+		}
+	} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)));
+
+	if (buf_len >= PAGE_SIZE)
+		buf_len = PAGE_SIZE - 1;
+	buf[buf_len] = 0;
+
+	kvfree(path);
+
+	return buf_len;
+}
+
+static int run_cmd(char cmd, int memcg_id, int nid, unsigned long *seqs,
+		   struct scan_control *sc, int swappiness, unsigned long opt);
+
+static ssize_t store_lru_gen_admin(struct kobject *kobj, struct kobj_attribute *attr,
+				const char *src, size_t len)
+{
+	void *buf;
+	char *cur, *next;
+	int err = 0;
+	struct scan_control sc = {
+		.may_writepage = true,
+		.may_unmap = true,
+		.may_swap = true,
+		.reclaim_idx = MAX_NR_ZONES - 1,
+		.gfp_mask = GFP_KERNEL,
+	};
+
+	buf = kvmalloc(len + 1, GFP_USER);
+	if (!buf)
+		return -ENOMEM;
+
+	memcpy(buf, src, len);
+
+	next = buf;
+	next[len] = '\0';
+
+	set_task_reclaim_state(current, &sc.reclaim_state);
+
+	while ((cur = strsep(&next, ",;\n"))) {
+		int n;
+		int end;
+		char cmd;
+		unsigned int memcg_id;
+		unsigned int nid;
+		unsigned long seqs[ANON_AND_FILE];
+		unsigned int swappiness = -1;
+		unsigned long nr_to_reclaim = -1;
+
+		cur = skip_spaces(cur);
+		if (!*cur)
+			continue;
+
+		n = sscanf(cur, "%c %u %u %lu %lu %n %u %n %lu %n", &cmd, &memcg_id, &nid,
+			   seqs, seqs + 1, &end, &swappiness, &end, &nr_to_reclaim, &end);
+		if (n < 5 || cur[end]) {
+			err = -EINVAL;
+			break;
+		}
+
+		err = run_cmd(cmd, memcg_id, nid, seqs, &sc, swappiness, nr_to_reclaim);
+		if (err)
+			break;
+	}
+
+	set_task_reclaim_state(current, NULL);
+	kvfree(buf);
+
+	return err ? : len;
+}
+
+static struct kobj_attribute lru_gen_admin_attr = __ATTR(
+	admin, 0444, show_lru_gen_admin, store_lru_gen_admin
+);
+
 static struct attribute *lru_gen_attrs[] = {
 	&lru_gen_min_ttl_attr.attr,
 	&lru_gen_enabled_attr.attr,
+	&lru_gen_admin_attr.attr,
 	NULL
 };
 
@@ -5889,25 +6245,24 @@ static void *lru_gen_seq_next(struct seq
 }
 
 static void lru_gen_seq_show_full(struct seq_file *m, struct lruvec *lruvec,
-				  unsigned long max_seq, unsigned long *min_seq,
-				  unsigned long seq)
+				  unsigned long *base_seq, int idx, bool is_max_seq)
 {
 	int i;
 	int type, tier;
-	int hist = lru_hist_from_seq(seq);
 	struct lru_gen_folio *lrugen = &lruvec->lrugen;
 
 	for (tier = 0; tier < MAX_NR_TIERS; tier++) {
 		seq_printf(m, "            %10d", tier);
 		for (type = 0; type < ANON_AND_FILE; type++) {
+			int hist = lru_hist_from_seq(base_seq[type] + idx);
 			const char *s = "   ";
 			unsigned long n[3] = {};
 
-			if (seq == max_seq) {
+			if (is_max_seq) {
 				s = "RT ";
 				n[0] = READ_ONCE(lrugen->avg_refaulted[type][tier]);
 				n[1] = READ_ONCE(lrugen->avg_total[type][tier]);
-			} else if (seq == min_seq[type] || NR_HIST_GENS > 1) {
+			} else if (idx == 0 || NR_HIST_GENS > 1) {
 				s = "rep";
 				n[0] = atomic_long_read(&lrugen->refaulted[hist][type][tier]);
 				n[1] = atomic_long_read(&lrugen->evicted[hist][type][tier]);
@@ -5920,34 +6275,52 @@ static void lru_gen_seq_show_full(struct
 		}
 		seq_putc(m, '\n');
 	}
+}
 
-	seq_puts(m, "                      ");
-	for (i = 0; i < NR_MM_STATS; i++) {
-		const char *s = "      ";
-		unsigned long n = 0;
-
-		if (seq == max_seq && NR_HIST_GENS == 1) {
-			s = "LOYNFA";
-			n = READ_ONCE(lruvec->mm_state.stats[hist][i]);
-		} else if (seq != max_seq && NR_HIST_GENS > 1) {
-			s = "loynfa";
-			n = READ_ONCE(lruvec->mm_state.stats[hist][i]);
-		}
+static void lru_gen_seq_show_mm_stats(struct seq_file *m, struct lruvec *lruvec)
+{
+	unsigned long seq;
+	int i;
+
+	if (lruvec->mm_state.scan_seq < NR_HIST_GENS)
+		seq = 0;
+	else
+		seq = lruvec->mm_state.scan_seq - NR_HIST_GENS + 1;
+
+	seq_puts(m, " mm_stats\n");
+	for (; seq <= lruvec->mm_state.scan_seq; seq++) {
+		int hist = lru_hist_from_seq(seq);
 
-		seq_printf(m, " %10lu%c", n, s[i]);
+		seq_printf(m, " %10lu ", seq);
+		for (i = 0; i < NR_MM_STATS; i++) {
+			const char *s = "      ";
+			unsigned long n = 0;
+			bool is_max_seq = seq == lruvec->mm_state.scan_seq;
+
+			if (is_max_seq && NR_HIST_GENS == 1) {
+				s = "LOYNFA";
+				n = READ_ONCE(lruvec->mm_state.stats[hist][i]);
+			} else if (!is_max_seq && NR_HIST_GENS > 1) {
+				s = "loynfa";
+				n = READ_ONCE(lruvec->mm_state.stats[hist][i]);
+			}
+
+			seq_printf(m, " %10lu%c", n, s[i]);
+		}
+		seq_putc(m, '\n');
 	}
-	seq_putc(m, '\n');
 }
 
 /* see Documentation/admin-guide/mm/multigen_lru.rst for details */
 static int lru_gen_seq_show(struct seq_file *m, void *v)
 {
-	unsigned long seq;
 	bool full = !debugfs_real_fops(m->file)->write;
 	struct lruvec *lruvec = v;
 	struct lru_gen_folio *lrugen = &lruvec->lrugen;
-	int nid = lruvec_pgdat(lruvec)->node_id;
+	int type, i, nid = lruvec_pgdat(lruvec)->node_id;
 	struct mem_cgroup *memcg = lruvec_memcg(lruvec);
+	unsigned long base_seq[2];
+	unsigned long num_gens;
 	DEFINE_MAX_SEQ(lruvec);
 	DEFINE_MIN_SEQ(lruvec);
 
@@ -5963,36 +6336,43 @@ static int lru_gen_seq_show(struct seq_f
 
 	seq_printf(m, " node %5d\n", nid);
 
-	if (!full)
-		seq = min_seq[LRU_GEN_ANON];
-	else if (max_seq >= MAX_NR_GENS)
-		seq = max_seq - MAX_NR_GENS + 1;
-	else
-		seq = 0;
-
-	for (; seq <= max_seq; seq++) {
-		int type, zone;
-		int gen = lru_gen_from_seq(seq);
-		unsigned long birth = READ_ONCE(lruvec->lrugen.timestamps[gen]);
-
-		seq_printf(m, " %10lu %10u", seq, jiffies_to_msecs(jiffies - birth));
+	num_gens = 0;
+	for (type = 0; type < ANON_AND_FILE; type++) {
+		if (!full)
+			base_seq[type] = min_seq[type];
+		else if (max_seq[type] >= MAX_NR_GENS)
+			base_seq[type] = max_seq[type] - MAX_NR_GENS + 1;
+		else
+			base_seq[type] = 0;
+		num_gens = max(num_gens, max_seq[type] - base_seq[type] + 1u);
+	}
 
+	for (i = 0; i < num_gens; i++) {
 		for (type = 0; type < ANON_AND_FILE; type++) {
+			unsigned long seq = base_seq[type] + i;
+			int zone, gen = lru_gen_from_seq(seq);
+			unsigned long birth = READ_ONCE(lruvec->lrugen.timestamps[gen][type]);
 			unsigned long size = 0;
 			char mark = full && seq < min_seq[type] ? 'x' : ' ';
 
+			seq_printf(m, " %10lu %10u", seq, jiffies_to_msecs(jiffies - birth));
+
 			for (zone = 0; zone < MAX_NR_ZONES; zone++)
 				size += max(READ_ONCE(lrugen->nr_pages[gen][type][zone]), 0L);
 
 			seq_printf(m, " %10lu%c", size, mark);
+
 		}
 
 		seq_putc(m, '\n');
 
 		if (full)
-			lru_gen_seq_show_full(m, lruvec, max_seq, min_seq, seq);
+			lru_gen_seq_show_full(m, lruvec, base_seq, i, i == num_gens - 1);
 	}
 
+	if (full)
+		lru_gen_seq_show_mm_stats(m, lruvec);
+
 	return 0;
 }
 
@@ -6003,46 +6383,73 @@ static const struct seq_operations lru_g
 	.show = lru_gen_seq_show,
 };
 
-static int run_aging(struct lruvec *lruvec, unsigned long seq, struct scan_control *sc,
+static int run_aging(struct lruvec *lruvec, unsigned long *seqs, struct scan_control *sc,
 		     bool can_swap, bool force_scan)
 {
 	DEFINE_MAX_SEQ(lruvec);
 	DEFINE_MIN_SEQ(lruvec);
+	unsigned long scan_seq = READ_ONCE(lruvec->mm_state.scan_seq);
+	bool should_age[ANON_AND_FILE] = {};
+	int type;
 
-	if (seq < max_seq)
+	for (type = !can_swap; type < ANON_AND_FILE; type++) {
+		if (seqs[type] > max_seq[type])
+			return -EINVAL;
+		should_age[type] = seqs[type] == max_seq[type];
+	}
+
+	if (!should_age[LRU_GEN_ANON] && !should_age[LRU_GEN_FILE])
 		return 0;
 
-	if (seq > max_seq)
-		return -EINVAL;
+	if (!force_scan) {
+		for (type = !can_swap; type < ANON_AND_FILE; type++)
+			should_age[type] &= (min_seq[type] + MAX_NR_GENS - 1 <= max_seq[type]);
 
-	if (!force_scan && min_seq[!can_swap] + MAX_NR_GENS - 1 <= max_seq)
-		return -ERANGE;
+		if (!should_age[LRU_GEN_ANON] && !should_age[LRU_GEN_FILE])
+			return -ERANGE;
+	}
 
-	try_to_inc_max_seq(lruvec, max_seq, sc, can_swap, force_scan);
+	try_to_inc_max_seq(lruvec, max_seq, scan_seq, should_age, sc, can_swap, force_scan);
 
 	return 0;
 }
 
-static int run_eviction(struct lruvec *lruvec, unsigned long seq, struct scan_control *sc,
+static int run_eviction(struct lruvec *lruvec, unsigned long *seqs, struct scan_control *sc,
 			int swappiness, unsigned long nr_to_reclaim)
 {
 	DEFINE_MAX_SEQ(lruvec);
+	int type;
+	bool has_target = false;
 
-	if (seq + MIN_NR_GENS > max_seq)
+	for (type = !swappiness; type < ANON_AND_FILE; type++)
+		has_target |= seqs[type] + MIN_NR_GENS <= max_seq[type];
+
+	if (!has_target)
 		return -EINVAL;
 
 	sc->nr_reclaimed = 0;
 
 	while (!signal_pending(current)) {
 		DEFINE_MIN_SEQ(lruvec);
+		int cur_swappiness = swappiness;
+		bool needs_work[ANON_AND_FILE] = {};
+
+		for (type = !swappiness; type < ANON_AND_FILE; type++)
+			needs_work[type] = seqs[type] >= min_seq[type];
 
-		if (seq < min_seq[!swappiness])
+		if (needs_work[LRU_GEN_ANON] && needs_work[LRU_GEN_FILE])
+			cur_swappiness = swappiness;
+		else if (needs_work[LRU_GEN_ANON])
+			cur_swappiness = 200;
+		else if (needs_work[LRU_GEN_FILE])
+			cur_swappiness = 0;
+		else
 			return 0;
 
 		if (sc->nr_reclaimed >= nr_to_reclaim)
 			return 0;
 
-		if (!evict_folios(lruvec, sc, swappiness))
+		if (!evict_folios(lruvec, sc, cur_swappiness))
 			return 0;
 
 		cond_resched();
@@ -6051,7 +6458,7 @@ static int run_eviction(struct lruvec *l
 	return -EINTR;
 }
 
-static int run_cmd(char cmd, int memcg_id, int nid, unsigned long seq,
+static int run_cmd(char cmd, int memcg_id, int nid, unsigned long *seqs,
 		   struct scan_control *sc, int swappiness, unsigned long opt)
 {
 	struct lruvec *lruvec;
@@ -6086,10 +6493,10 @@ static int run_cmd(char cmd, int memcg_i
 
 	switch (cmd) {
 	case '+':
-		err = run_aging(lruvec, seq, sc, swappiness, opt);
+		err = run_aging(lruvec, seqs, sc, swappiness, opt);
 		break;
 	case '-':
-		err = run_eviction(lruvec, seq, sc, swappiness, opt);
+		err = run_eviction(lruvec, seqs, sc, swappiness, opt);
 		break;
 	}
 done:
@@ -6141,7 +6548,7 @@ static ssize_t lru_gen_seq_write(struct
 		char cmd;
 		unsigned int memcg_id;
 		unsigned int nid;
-		unsigned long seq;
+		unsigned long seqs[ANON_AND_FILE];
 		unsigned int swappiness = -1;
 		unsigned long opt = -1;
 
@@ -6149,14 +6556,14 @@ static ssize_t lru_gen_seq_write(struct
 		if (!*cur)
 			continue;
 
-		n = sscanf(cur, "%c %u %u %lu %n %u %n %lu %n", &cmd, &memcg_id, &nid,
-			   &seq, &end, &swappiness, &end, &opt, &end);
-		if (n < 4 || cur[end]) {
+		n = sscanf(cur, "%c %u %u %lu %lu %n %u %n %lu %n", &cmd, &memcg_id, &nid,
+			   seqs, seqs + 1, &end, &swappiness, &end, &opt, &end);
+		if (n < 5 || cur[end]) {
 			err = -EINVAL;
 			break;
 		}
 
-		err = run_cmd(cmd, memcg_id, nid, seq, &sc, swappiness, opt);
+		err = run_cmd(cmd, memcg_id, nid, seqs, &sc, swappiness, opt);
 		if (err)
 			break;
 	}
@@ -6201,16 +6608,18 @@ void lru_gen_init_lruvec(struct lruvec *
 	int gen, type, zone;
 	struct lru_gen_folio *lrugen = &lruvec->lrugen;
 
-	lrugen->max_seq = MIN_NR_GENS + 1;
 	lrugen->enabled = lru_gen_enabled();
 
-	for (i = 0; i <= MIN_NR_GENS + 1; i++)
-		lrugen->timestamps[i] = jiffies;
+	for (type = ANON_AND_FILE - 1; type >= 0; type--) {
+		lrugen->max_seq[type] = MIN_NR_GENS + 1;
+		for (i = 0; i <= MIN_NR_GENS + 1; i++)
+			lrugen->timestamps[i][type] = jiffies;
+	}
 
 	for_each_gen_type_zone(gen, type, zone)
 		INIT_LIST_HEAD(&lrugen->folios[gen][type][zone]);
 
-	lruvec->mm_state.seq = MIN_NR_GENS;
+	lruvec->mm_state.scan_seq = 0;
 }
 
 #ifdef CONFIG_MEMCG
@@ -6259,11 +6668,14 @@ void lru_gen_exit_memcg(struct mem_cgrou
 
 static int __init init_lru_gen(void)
 {
+	struct kernfs_node *tmp;
 	BUILD_BUG_ON(MIN_NR_GENS + 1 >= MAX_NR_GENS);
 	BUILD_BUG_ON(BIT(LRU_GEN_WIDTH) <= MAX_NR_GENS);
 
 	if (sysfs_create_group(mm_kobj, &lru_gen_attr_group))
 		pr_err("lru_gen: failed to create sysfs group\n");
+	tmp = kernfs_find_and_get(mm_kobj->sd, "lru_gen");
+	lru_gen_admin_node = kernfs_find_and_get(tmp, "admin");
 
 	debugfs_create_file("lru_gen", 0644, NULL, NULL, &lru_gen_rw_fops);
 	debugfs_create_file("lru_gen_full", 0444, NULL, NULL, &lru_gen_ro_fops);
Index: kernel-rpi-6_6/fs/proc/task_mmu.c
===================================================================
--- kernel-rpi-6_6.orig/fs/proc/task_mmu.c
+++ kernel-rpi-6_6/fs/proc/task_mmu.c
@@ -20,11 +20,14 @@
 #include <linux/shmem_fs.h>
 #include <linux/uaccess.h>
 #include <linux/pkeys.h>
+#include <linux/random.h>
+#include <linux/mm_inline.h>
 
 #include <asm/elf.h>
 #include <asm/tlb.h>
 #include <asm/tlbflush.h>
 #include "internal.h"
+#include "../../mm/internal.h"
 
 #define SEQ_PUT_DEC(str, val) \
 		seq_put_decimal_ull_width(m, str, (val) << (PAGE_SHIFT-10), 8)
@@ -208,7 +211,7 @@ static int proc_maps_open(struct inode *
 		return -ENOMEM;
 
 	priv->inode = inode;
-	priv->mm = proc_mem_open(inode, PTRACE_MODE_READ);
+	priv->mm = proc_mem_open(file, PTRACE_MODE_READ);
 	if (IS_ERR(priv->mm)) {
 		int err = PTR_ERR(priv->mm);
 
@@ -700,6 +703,9 @@ static void show_smap_vma_flags(struct s
 #ifdef CONFIG_X86_USER_SHADOW_STACK
 		[ilog2(VM_SHADOW_STACK)] = "ss",
 #endif
+#ifdef CONFIG_64BIT
+		[ilog2(VM_SEALED)] = "sl",
+#endif
 	};
 	size_t i;
 
@@ -1025,7 +1031,7 @@ static int smaps_rollup_open(struct inod
 		goto out_free;
 
 	priv->inode = inode;
-	priv->mm = proc_mem_open(inode, PTRACE_MODE_READ);
+	priv->mm = proc_mem_open(file, PTRACE_MODE_READ);
 	if (IS_ERR(priv->mm)) {
 		ret = PTR_ERR(priv->mm);
 
@@ -1516,7 +1522,7 @@ static int pagemap_pmd_range(pmd_t *pmdp
 			flags |= PM_FILE;
 
 		for (; addr != end; addr += PAGE_SIZE, idx++) {
-			u64 cur_flags = flags;
+			unsigned long cur_flags = flags;
 			pagemap_entry_t pme;
 
 			if (page && (flags & PM_PRESENT) &&
@@ -1749,7 +1755,7 @@ static int pagemap_open(struct inode *in
 {
 	struct mm_struct *mm;
 
-	mm = proc_mem_open(inode, PTRACE_MODE_READ);
+	mm = proc_mem_open(file, PTRACE_MODE_READ);
 	if (IS_ERR(mm))
 		return PTR_ERR(mm);
 	file->private_data = mm;
@@ -1773,6 +1779,417 @@ const struct file_operations proc_pagema
 };
 #endif /* CONFIG_PROC_PAGE_MONITOR */
 
+#ifdef CONFIG_PROCESS_RECLAIM
+enum reclaim_type {
+	RECLAIM_FILE = 1,
+	RECLAIM_ANON,
+	RECLAIM_ALL,
+	/*
+	 * For safety and backwards compatability, shmem reclaim mode
+	 * is only possible by directly using 'shmem', 'all' does not
+	 * inlcude shmem.
+	 */
+	RECLAIM_SHMEM,
+};
+
+struct walk_data {
+	unsigned long nr_to_try;
+	enum reclaim_type type;
+};
+
+static int deactivate_pte_range(pmd_t *pmd, unsigned long addr,
+				unsigned long end, struct mm_walk *walk)
+{
+	pte_t *orig_pte, *pte, ptent;
+	spinlock_t *ptl;
+	struct page *page;
+	struct vm_area_struct *vma = walk->vma;
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long next = pmd_addr_end(addr, end);
+
+	ptl = pmd_trans_huge_lock(pmd, vma);
+	if (ptl) {
+		if (!pmd_present(*pmd))
+			goto huge_unlock;
+
+		if (is_huge_zero_pmd(*pmd))
+			goto huge_unlock;
+
+		page = pmd_page(*pmd);
+		if (page_mapcount(page) > 1)
+			goto huge_unlock;
+
+		if (next - addr != HPAGE_PMD_SIZE) {
+			int err;
+
+			get_page(page);
+			spin_unlock(ptl);
+			lock_page(page);
+			err = split_huge_page(page);
+			unlock_page(page);
+			put_page(page);
+			if (!err)
+				goto regular_page;
+			return 0;
+		}
+
+		pmdp_test_and_clear_young(vma, addr, pmd);
+		folio_deactivate(page_folio(page));
+huge_unlock:
+		spin_unlock(ptl);
+		return 0;
+	}
+
+regular_page:
+
+	orig_pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
+	for (pte = orig_pte; addr < end; pte++, addr += PAGE_SIZE) {
+		ptent = *pte;
+
+		if (!pte_present(ptent))
+			continue;
+
+		page = vm_normal_page(vma, addr, ptent);
+		if (!page)
+			continue;
+
+		if (PageTransCompound(page))  {
+			if (page_mapcount(page) != 1)
+				break;
+			get_page(page);
+			if (!trylock_page(page)) {
+				put_page(page);
+				break;
+			}
+			pte_unmap_unlock(orig_pte, ptl);
+			if (split_huge_page(page)) {
+				unlock_page(page);
+				put_page(page);
+				pte_offset_map_lock(mm, pmd, addr, &ptl);
+				break;
+			}
+			unlock_page(page);
+			put_page(page);
+			pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
+			pte--;
+			addr -= PAGE_SIZE;
+			continue;
+		}
+
+		VM_BUG_ON_PAGE(PageTransCompound(page), page);
+
+		if (page_mapcount(page) > 1)
+			continue;
+
+		ptep_test_and_clear_young(vma, addr, pte);
+		folio_deactivate(page_folio(page));
+	}
+	pte_unmap_unlock(orig_pte, ptl);
+	cond_resched();
+	return 0;
+}
+
+
+static int reclaim_pte_range(pmd_t *pmd, unsigned long addr,
+				unsigned long end, struct mm_walk *walk)
+{
+	pte_t *orig_pte, *pte, ptent;
+	spinlock_t *ptl;
+	LIST_HEAD(page_list);
+	struct page *page;
+	int isolated = 0;
+	struct vm_area_struct *vma = walk->vma;
+	struct walk_data *data = (struct walk_data*)walk->private;
+	enum reclaim_type type = 0;
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long next = pmd_addr_end(addr, end);
+	unsigned int batch_count = 0;
+
+	if (data)
+		type = data->type;
+
+	ptl = pmd_trans_huge_lock(pmd, vma);
+	if (ptl) {
+		if (!pmd_present(*pmd))
+			goto huge_unlock;
+
+		if (is_huge_zero_pmd(*pmd))
+			goto huge_unlock;
+
+		page = pmd_page(*pmd);
+		if (type != RECLAIM_SHMEM && page_mapcount(page) > 1)
+			goto huge_unlock;
+
+		if (!data->nr_to_try)
+			goto huge_unlock;
+		if (next - addr != HPAGE_PMD_SIZE) {
+			int err;
+
+			get_page(page);
+			spin_unlock(ptl);
+			lock_page(page);
+			err = split_huge_page(page);
+			unlock_page(page);
+			put_page(page);
+			if (!err)
+				goto regular_page;
+			return 0;
+		}
+
+		if (!isolate_lru_page(page))
+			goto huge_unlock;
+
+		/*
+		 * Reclaim the whole huge page even if it would make us go
+		 * over our limit.
+		 */
+		data->nr_to_try -= min_t(unsigned long, data->nr_to_try,
+		    thp_nr_pages(page));
+
+		/* Clear all the references to make sure it gets reclaimed */
+		pmdp_test_and_clear_young(vma, addr, pmd);
+		ClearPageReferenced(page);
+		test_and_clear_page_young(page);
+		list_add(&page->lru, &page_list);
+huge_unlock:
+		spin_unlock(ptl);
+		reclaim_pages(&page_list);
+		return 0;
+	}
+
+regular_page:
+
+	orig_pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
+	if (!orig_pte)
+		return 0;
+	for (pte = orig_pte; addr < end; pte++, addr += PAGE_SIZE) {
+		if (!data->nr_to_try)
+			break;
+		ptent = *pte;
+		if (!pte_present(ptent))
+			continue;
+
+		page = vm_normal_page(vma, addr, ptent);
+		if (!page)
+			continue;
+
+		if (++batch_count == SWAP_CLUSTER_MAX) {
+			batch_count = 0;
+			if (need_resched()) {
+				pte_unmap_unlock(orig_pte, ptl);
+				cond_resched();
+				goto regular_page;
+			}
+		}
+
+		if (PageTransCompound(page)) {
+			if (type != RECLAIM_SHMEM && page_mapcount(page) != 1)
+				break;
+			get_page(page);
+			if (!trylock_page(page)) {
+				put_page(page);
+				break;
+			}
+			pte_unmap_unlock(orig_pte, ptl);
+
+			if (split_huge_page(page)) {
+				unlock_page(page);
+				put_page(page);
+				pte_offset_map_lock(mm, pmd, addr, &ptl);
+				break;
+			}
+			unlock_page(page);
+			put_page(page);
+			pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
+			pte--;
+			addr -= PAGE_SIZE;
+			continue;
+		}
+
+		VM_BUG_ON_PAGE(PageTransCompound(page), page);
+
+		if (!PageLRU(page))
+			continue;
+
+		if (type != RECLAIM_SHMEM && page_mapcount(page) > 1)
+			continue;
+
+		if (!isolate_lru_page(page))
+			continue;
+
+		isolated++;
+		data->nr_to_try--;
+		list_add(&page->lru, &page_list);
+		/* Clear all the references to make sure it gets reclaimed */
+		ptep_test_and_clear_young(vma, addr, pte);
+		ClearPageReferenced(page);
+		test_and_clear_page_young(page);
+		if (isolated >= SWAP_CLUSTER_MAX) {
+			pte_unmap_unlock(orig_pte, ptl);
+			reclaim_pages(&page_list);
+			isolated = 0;
+			pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
+			orig_pte = pte;
+		}
+	}
+
+	pte_unmap_unlock(orig_pte, ptl);
+	reclaim_pages(&page_list);
+
+	cond_resched();
+	return 0;
+}
+
+static void reclaim_mm(struct mm_struct *mm, enum reclaim_type type, unsigned long nr_to_try)
+{
+	struct vm_area_struct *start, *vma;
+	struct mm_walk_ops reclaim_walk = {};
+	struct walk_data reclaim_data = {
+		.type = type,
+		.nr_to_try = nr_to_try,
+	};
+
+	mmap_read_lock(mm);
+
+	start = find_vma(mm, 0);
+	if (nr_to_try != ULONG_MAX) {
+		unsigned int start_idx;
+
+		/*
+		 * Try to start at a random VMA to avoid always
+		 * reclaiming the same pages.
+		 */
+		start_idx = get_random_u32() % mm->map_count;
+		for (; start_idx && start; start_idx--)
+			start = find_vma(mm, start->vm_end);
+	}
+	BUG_ON(!start);
+
+	vma = start;
+	while (reclaim_data.nr_to_try) {
+		if (is_vm_hugetlb_page(vma))
+			goto next;
+
+		if (vma->vm_flags & VM_LOCKED)
+			goto next;
+
+		if (type == RECLAIM_ANON && !vma_is_anonymous(vma))
+			goto next;
+		if ((type == RECLAIM_FILE || type == RECLAIM_SHMEM)
+				&& vma_is_anonymous(vma)) {
+			goto next;
+		}
+
+		if (vma_is_anonymous(vma) || shmem_file(vma->vm_file)) {
+			if (get_nr_swap_pages() <= 0 ||
+				get_mm_counter(mm, MM_ANONPAGES) == 0) {
+				if (type == RECLAIM_ALL)
+					goto next;
+				else
+					break;
+			}
+
+			if (shmem_file(vma->vm_file) && type != RECLAIM_SHMEM)
+				goto next;
+
+			reclaim_walk.pmd_entry = reclaim_pte_range;
+		} else {
+			reclaim_walk.pmd_entry = deactivate_pte_range;
+		}
+
+		/*
+		 * Use a random start address if we are limited in order
+		 * to avoid always hitting the same pages when we only
+		 * have a few eligible mappings.
+		 */
+		if (nr_to_try != ULONG_MAX) {
+			unsigned long idx, start;
+
+			idx = (vma->vm_end - vma->vm_start) / PAGE_SIZE;
+			idx = idx ? (get_random_u32() % idx) : 0;
+			start = vma->vm_start + PAGE_SIZE * idx;
+
+			walk_page_range(mm, start, vma->vm_end,
+			    &reclaim_walk, (void *)&reclaim_data);
+			if (start != vma->vm_start)
+				walk_page_range(mm, vma->vm_start,
+				    start, &reclaim_walk,
+				    (void *)&reclaim_data);
+		} else {
+			walk_page_range(mm, vma->vm_start, vma->vm_end,
+			    &reclaim_walk, (void *)&reclaim_data);
+		}
+
+next:
+		vma = find_vma(mm, vma->vm_end);
+		if (!vma)
+			vma = find_vma(mm, 0);
+
+		/* Already walked through all of them. */
+		if (vma == start)
+			break;
+	}
+
+	flush_tlb_mm(mm);
+	mmap_read_unlock(mm);
+}
+
+static ssize_t reclaim_write(struct file *file, const char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	struct task_struct *task;
+	char buffer[PROC_NUMBUF];
+	struct mm_struct *mm;
+	enum reclaim_type type;
+	unsigned long num;
+	char *tok, *type_buf;
+
+	memset(buffer, 0, sizeof(buffer));
+	if (count > sizeof(buffer) - 1)
+		count = sizeof(buffer) - 1;
+
+	if (copy_from_user(buffer, buf, count))
+		return -EFAULT;
+
+	type_buf = strstrip(buffer);
+	tok = strsep(&type_buf, " ");
+	if (!strcmp(tok, "file"))
+		type = RECLAIM_FILE;
+	else if (!strcmp(tok, "anon"))
+		type = RECLAIM_ANON;
+#ifdef CONFIG_SHMEM
+	else if (!strcmp(tok, "shmem"))
+		type = RECLAIM_SHMEM;
+#endif
+	else if (!strcmp(tok, "all"))
+		type = RECLAIM_ALL;
+	else
+		return -EINVAL;
+
+	tok = strsep(&type_buf, " ");
+	if (!tok || kstrtol(tok, 10, &num) < 0)
+		num = ULONG_MAX;
+
+	task = get_proc_task(file->f_path.dentry->d_inode);
+	if (!task)
+		return -ESRCH;
+
+	mm = get_task_mm(task);
+	if (mm) {
+		reclaim_mm(mm, type, num);
+		mmput(mm);
+	}
+	put_task_struct(task);
+
+	return count;
+}
+
+const struct file_operations proc_reclaim_operations = {
+	.write		= reclaim_write,
+	.llseek		= noop_llseek,
+};
+#endif
+
 #ifdef CONFIG_NUMA
 
 struct numa_maps {
Index: kernel-rpi-6_6/mm/mmap.c
===================================================================
--- kernel-rpi-6_6.orig/mm/mmap.c
+++ kernel-rpi-6_6/mm/mmap.c
@@ -137,7 +137,8 @@ void unlink_file_vma(struct vm_area_stru
 static void remove_vma(struct vm_area_struct *vma, bool unreachable)
 {
 	might_sleep();
-	vma_close(vma);
+	if (vma->vm_ops && vma->vm_ops->close)
+		vma->vm_ops->close(vma);
 	if (vma->vm_file)
 		fput(vma->vm_file);
 	mpol_put(vma_policy(vma));
@@ -1263,6 +1264,16 @@ unsigned long do_mmap(struct file *file,
 			return -EEXIST;
 	}
 
+	/*
+	 * addr is returned from get_unmapped_area,
+	 * There are two cases:
+	 * 1> MAP_FIXED == false
+	 *	unallocated memory, no need to check sealing.
+	 * 1> MAP_FIXED == true
+	 *	sealing is checked inside mmap_region when
+	 *	do_vmi_munmap is called.
+	 */
+
 	if (prot == PROT_EXEC) {
 		pkey = execute_only_pkey(mm);
 		if (pkey < 0)
@@ -1273,7 +1284,7 @@ unsigned long do_mmap(struct file *file,
 	 * to. we assume access permissions have been handled by the open
 	 * of the memory object, so we don't do any here.
 	 */
-	vm_flags |= calc_vm_prot_bits(prot, pkey) | calc_vm_flag_bits(file, flags) |
+	vm_flags |= calc_vm_prot_bits(prot, pkey) | calc_vm_flag_bits(flags) |
 			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;
 
 	if (flags & MAP_LOCKED)
@@ -1330,7 +1341,8 @@ unsigned long do_mmap(struct file *file,
 			if (path_noexec(&file->f_path)) {
 				if (vm_flags & VM_EXEC)
 					return -EPERM;
-				vm_flags &= ~VM_MAYEXEC;
+				if (sysctl_mmap_noexec_taint)
+					vm_flags &= ~VM_MAYEXEC;
 			}
 
 			if (!file->f_op->mmap)
@@ -2636,6 +2648,14 @@ int do_vmi_munmap(struct vma_iterator *v
 	if (end == start)
 		return -EINVAL;
 
+	/*
+	 * Check if memory is sealed before arch_unmap.
+	 * Prevent unmapping a sealed VMA.
+	 * can_modify_mm assumes we have acquired the lock on MM.
+	 */
+	if (unlikely(!can_modify_mm(mm, start, end)))
+		return -EPERM;
+
 	 /* arch_unmap() might do unmaps itself.  */
 	arch_unmap(mm, start, end);
 
@@ -2666,14 +2686,14 @@ int do_munmap(struct mm_struct *mm, unsi
 	return do_vmi_munmap(&vmi, mm, start, len, uf, false);
 }
 
-static unsigned long __mmap_region(struct file *file, unsigned long addr,
+unsigned long mmap_region(struct file *file, unsigned long addr,
 		unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
 		struct list_head *uf)
 {
 	struct mm_struct *mm = current->mm;
 	struct vm_area_struct *vma = NULL;
 	struct vm_area_struct *next, *prev, *merge;
-	pgoff_t pglen = PHYS_PFN(len);
+	pgoff_t pglen = len >> PAGE_SHIFT;
 	unsigned long charged = 0;
 	unsigned long end = addr + len;
 	unsigned long merge_start = addr, merge_end = end;
@@ -2697,7 +2717,10 @@ static unsigned long __mmap_region(struc
 	}
 
 	/* Unmap any existing mapping in the area */
-	if (do_vmi_munmap(&vmi, mm, addr, len, uf, false))
+	error = do_vmi_munmap(&vmi, mm, addr, len, uf, false);
+	if (error == -EPERM)
+		return error;
+	else if (error)
 		return -ENOMEM;
 
 	/*
@@ -2770,30 +2793,29 @@ cannot_expand:
 	vma->vm_page_prot = vm_get_page_prot(vm_flags);
 	vma->vm_pgoff = pgoff;
 
-	if (vma_iter_prealloc(&vmi, vma)) {
-		error = -ENOMEM;
-		goto free_vma;
-	}
-
 	if (file) {
+		if (vm_flags & VM_SHARED) {
+			error = mapping_map_writable(file->f_mapping);
+			if (error)
+				goto free_vma;
+		}
+
 		vma->vm_file = get_file(file);
-		error = mmap_file(file, vma);
+		error = call_mmap(file, vma);
 		if (error)
-			goto unmap_and_free_file_vma;
+			goto unmap_and_free_vma;
 
-		/* Drivers cannot alter the address of the VMA. */
-		WARN_ON_ONCE(addr != vma->vm_start);
 		/*
-		 * Drivers should not permit writability when previously it was
-		 * disallowed.
+		 * Expansion is handled above, merging is handled below.
+		 * Drivers should not alter the address of the VMA.
 		 */
-		VM_WARN_ON_ONCE(vm_flags != vma->vm_flags &&
-				!(vm_flags & VM_MAYWRITE) &&
-				(vma->vm_flags & VM_MAYWRITE));
+		error = -EINVAL;
+		if (WARN_ON((addr != vma->vm_start)))
+			goto close_and_free_vma;
 
 		vma_iter_config(&vmi, addr, end);
 		/*
-		 * If vm_flags changed after mmap_file(), we should try merge
+		 * If vm_flags changed after call_mmap(), we should try merge
 		 * vma again as we may succeed this time.
 		 */
 		if (unlikely(vm_flags != vma->vm_flags && prev)) {
@@ -2801,7 +2823,6 @@ cannot_expand:
 				    vma->vm_end, vma->vm_flags, NULL,
 				    vma->vm_file, vma->vm_pgoff, NULL,
 				    NULL_VM_UFFD_CTX, NULL);
-
 			if (merge) {
 				/*
 				 * ->mmap() can change vma->vm_file and fput
@@ -2815,7 +2836,7 @@ cannot_expand:
 				vma = merge;
 				/* Update vm_flags to pick up the change. */
 				vm_flags = vma->vm_flags;
-				goto file_expanded;
+				goto unmap_writable;
 			}
 		}
 
@@ -2823,15 +2844,24 @@ cannot_expand:
 	} else if (vm_flags & VM_SHARED) {
 		error = shmem_zero_setup(vma);
 		if (error)
-			goto free_iter_vma;
+			goto free_vma;
 	} else {
 		vma_set_anonymous(vma);
 	}
 
-#ifdef CONFIG_SPARC64
-	/* TODO: Fix SPARC ADI! */
-	WARN_ON_ONCE(!arch_validate_flags(vm_flags));
-#endif
+	if (map_deny_write_exec(vma, vma->vm_flags)) {
+		error = -EACCES;
+		goto close_and_free_vma;
+	}
+
+	/* Allow architectures to sanity-check the vm_flags */
+	error = -EINVAL;
+	if (!arch_validate_flags(vma->vm_flags))
+		goto close_and_free_vma;
+
+	error = -ENOMEM;
+	if (vma_iter_prealloc(&vmi, vma))
+		goto close_and_free_vma;
 
 	/* Lock the VMA since it is modified after insertion into VMA tree */
 	vma_start_write(vma);
@@ -2854,7 +2884,10 @@ cannot_expand:
 	 */
 	khugepaged_enter_vma(vma, vma->vm_flags);
 
-file_expanded:
+	/* Once vma denies write, undo our temporary denial count */
+unmap_writable:
+	if (file && vm_flags & VM_SHARED)
+		mapping_unmap_writable(file->f_mapping);
 	file = vma->vm_file;
 	ksm_add_vma(vma);
 expanded:
@@ -2884,60 +2917,34 @@ expanded:
 
 	vma_set_page_prot(vma);
 
+	validate_mm(mm);
 	return addr;
 
-unmap_and_free_file_vma:
-	fput(vma->vm_file);
-	vma->vm_file = NULL;
-
-	vma_iter_set(&vmi, vma->vm_end);
-	/* Undo any partial mapping done by a device driver. */
-	unmap_region(mm, &vmi.mas, vma, prev, next, vma->vm_start,
-		     vma->vm_end, vma->vm_end, true);
-free_iter_vma:
-	vma_iter_free(&vmi);
+close_and_free_vma:
+	if (file && vma->vm_ops && vma->vm_ops->close)
+		vma->vm_ops->close(vma);
+
+	if (file || vma->vm_file) {
+unmap_and_free_vma:
+		fput(vma->vm_file);
+		vma->vm_file = NULL;
+
+		vma_iter_set(&vmi, vma->vm_end);
+		/* Undo any partial mapping done by a device driver. */
+		unmap_region(mm, &vmi.mas, vma, prev, next, vma->vm_start,
+			     vma->vm_end, vma->vm_end, true);
+	}
+	if (file && (vm_flags & VM_SHARED))
+		mapping_unmap_writable(file->f_mapping);
 free_vma:
 	vm_area_free(vma);
 unacct_error:
 	if (charged)
 		vm_unacct_memory(charged);
+	validate_mm(mm);
 	return error;
 }
 
-unsigned long mmap_region(struct file *file, unsigned long addr,
-			  unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
-			  struct list_head *uf)
-{
-	unsigned long ret;
-	bool writable_file_mapping = false;
-
-	/* Check to see if MDWE is applicable. */
-	if (map_deny_write_exec(vm_flags, vm_flags))
-		return -EACCES;
-
-	/* Allow architectures to sanity-check the vm_flags. */
-	if (!arch_validate_flags(vm_flags))
-		return -EINVAL;
-
-	/* Map writable and ensure this isn't a sealed memfd. */
-	if (file && (vm_flags & VM_SHARED)) {
-		int error = mapping_map_writable(file->f_mapping);
-
-		if (error)
-			return error;
-		writable_file_mapping = true;
-	}
-
-	ret = __mmap_region(file, addr, len, vm_flags, pgoff, uf);
-
-	/* Clear our write mapping regardless of error. */
-	if (writable_file_mapping)
-		mapping_unmap_writable(file->f_mapping);
-
-	validate_mm(current->mm);
-	return ret;
-}
-
 static int __vm_munmap(unsigned long start, size_t len, bool unlock)
 {
 	int ret;
@@ -3077,6 +3084,14 @@ int do_vma_munmap(struct vma_iterator *v
 {
 	struct mm_struct *mm = vma->vm_mm;
 
+	/*
+	 * Check if memory is sealed before arch_unmap.
+	 * Prevent unmapping a sealed VMA.
+	 * can_modify_mm assumes we have acquired the lock on MM.
+	 */
+	if (unlikely(!can_modify_mm(mm, start, end)))
+		return -EPERM;
+
 	arch_unmap(mm, start, end);
 	return do_vmi_align_munmap(vmi, vma, mm, start, end, uf, unlock);
 }
@@ -3407,7 +3422,8 @@ struct vm_area_struct *copy_vma(struct v
 	return new_vma;
 
 out_vma_link:
-	vma_close(new_vma);
+	if (new_vma->vm_ops && new_vma->vm_ops->close)
+		new_vma->vm_ops->close(new_vma);
 
 	if (new_vma->vm_file)
 		fput(new_vma->vm_file);
Index: kernel-rpi-6_6/net/netfilter/Makefile
===================================================================
--- kernel-rpi-6_6.orig/net/netfilter/Makefile
+++ kernel-rpi-6_6/net/netfilter/Makefile
@@ -215,6 +215,7 @@ obj-$(CONFIG_NETFILTER_XT_MATCH_PHYSDEV)
 obj-$(CONFIG_NETFILTER_XT_MATCH_PKTTYPE) += xt_pkttype.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_POLICY) += xt_policy.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_QUOTA) += xt_quota.o
+obj-$(CONFIG_NETFILTER_XT_MATCH_QUOTA2) += xt_quota2.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_RATEEST) += xt_rateest.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_REALM) += xt_realm.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_RECENT) += xt_recent.o
Index: kernel-rpi-6_6/net/netfilter/xt_quota2.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/net/netfilter/xt_quota2.c
@@ -0,0 +1,397 @@
+/*
+ * xt_quota2 - enhanced xt_quota that can count upwards and in packets
+ * as a minimal accounting match.
+ * by Jan Engelhardt <jengelh@medozas.de>, 2008
+ *
+ * Originally based on xt_quota.c:
+ * 	netfilter module to enforce network quotas
+ * 	Sam Johnston <samj@samj.net>
+ *
+ *	This program is free software; you can redistribute it and/or modify
+ *	it under the terms of the GNU General Public License; either
+ *	version 2 of the License, as published by the Free Software Foundation.
+ */
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+#include <linux/skbuff.h>
+#include <linux/spinlock.h>
+#include <asm/atomic.h>
+#include <net/netlink.h>
+
+#include <linux/netfilter/x_tables.h>
+#include <linux/netfilter/xt_quota2.h>
+
+#ifdef CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG
+/* For compatibility, these definitions are copied from the
+ * deprecated header file <linux/netfilter_ipv4/ipt_ULOG.h> */
+#define ULOG_MAC_LEN	80
+#define ULOG_PREFIX_LEN	32
+
+/* Format of the ULOG packets passed through netlink */
+typedef struct ulog_packet_msg {
+	unsigned long mark;
+	long timestamp_sec;
+	long timestamp_usec;
+	unsigned int hook;
+	char indev_name[IFNAMSIZ];
+	char outdev_name[IFNAMSIZ];
+	size_t data_len;
+	char prefix[ULOG_PREFIX_LEN];
+	unsigned char mac_len;
+	unsigned char mac[ULOG_MAC_LEN];
+	unsigned char payload[0];
+} ulog_packet_msg_t;
+#endif
+
+/**
+ * @lock:	lock to protect quota writers from each other
+ */
+struct xt_quota_counter {
+	u_int64_t quota;
+	spinlock_t lock;
+	struct list_head list;
+	atomic_t ref;
+	char name[sizeof(((struct xt_quota_mtinfo2 *)NULL)->name)];
+	struct proc_dir_entry *procfs_entry;
+};
+
+#ifdef CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG
+/* Harald's favorite number +1 :D From ipt_ULOG.C */
+static int qlog_nl_event = 112;
+module_param_named(event_num, qlog_nl_event, uint, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(event_num,
+		 "Event number for NETLINK_NFLOG message. 0 disables log."
+		 "111 is what ipt_ULOG uses.");
+static struct sock *nflognl;
+#endif
+
+static LIST_HEAD(counter_list);
+static DEFINE_SPINLOCK(counter_list_lock);
+
+static struct proc_dir_entry *proc_xt_quota;
+static unsigned int quota_list_perms = S_IRUGO | S_IWUSR;
+static kuid_t quota_list_uid = KUIDT_INIT(0);
+static kgid_t quota_list_gid = KGIDT_INIT(0);
+module_param_named(perms, quota_list_perms, uint, S_IRUGO | S_IWUSR);
+
+#ifdef CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG
+static void quota2_log(unsigned int hooknum,
+		       const struct sk_buff *skb,
+		       const struct net_device *in,
+		       const struct net_device *out,
+		       const char *prefix)
+{
+	ulog_packet_msg_t *pm;
+	struct sk_buff *log_skb;
+	size_t size;
+	struct nlmsghdr *nlh;
+
+	if (!qlog_nl_event)
+		return;
+
+	size = NLMSG_SPACE(sizeof(*pm));
+	size = max(size, (size_t)NLMSG_GOODSIZE);
+	log_skb = alloc_skb(size, GFP_ATOMIC);
+	if (!log_skb) {
+		pr_err("xt_quota2: cannot alloc skb for logging\n");
+		return;
+	}
+
+	nlh = nlmsg_put(log_skb, /*pid*/0, /*seq*/0, qlog_nl_event,
+			sizeof(*pm), 0);
+	if (!nlh) {
+		pr_err("xt_quota2: nlmsg_put failed\n");
+		kfree_skb(log_skb);
+		return;
+	}
+	pm = nlmsg_data(nlh);
+	memset(pm, 0, sizeof(*pm));
+	if (skb->tstamp == 0)
+		__net_timestamp((struct sk_buff *)skb);
+	pm->hook = hooknum;
+	if (prefix != NULL)
+		strlcpy(pm->prefix, prefix, sizeof(pm->prefix));
+	if (in)
+		strlcpy(pm->indev_name, in->name, sizeof(pm->indev_name));
+	if (out)
+		strlcpy(pm->outdev_name, out->name, sizeof(pm->outdev_name));
+
+	NETLINK_CB(log_skb).dst_group = 1;
+	pr_debug("throwing 1 packets to netlink group 1\n");
+	netlink_broadcast(nflognl, log_skb, 0, 1, GFP_ATOMIC);
+}
+#else
+static void quota2_log(unsigned int hooknum,
+		       const struct sk_buff *skb,
+		       const struct net_device *in,
+		       const struct net_device *out,
+		       const char *prefix)
+{
+}
+#endif  /* if+else CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG */
+
+static ssize_t quota_proc_read(struct file *file, char __user *buf,
+			   size_t size, loff_t *ppos)
+{
+	struct xt_quota_counter *e = pde_data(file_inode(file));
+	char tmp[24];
+	size_t tmp_size;
+
+	spin_lock_bh(&e->lock);
+	tmp_size = scnprintf(tmp, sizeof(tmp), "%llu\n", e->quota);
+	spin_unlock_bh(&e->lock);
+	return simple_read_from_buffer(buf, size, ppos, tmp, tmp_size);
+}
+
+static ssize_t quota_proc_write(struct file *file, const char __user *input,
+                            size_t size, loff_t *ppos)
+{
+	struct xt_quota_counter *e = pde_data(file_inode(file));
+	char buf[sizeof("18446744073709551616")];
+
+	if (size > sizeof(buf))
+		size = sizeof(buf);
+	if (copy_from_user(buf, input, size) != 0)
+		return -EFAULT;
+	buf[sizeof(buf)-1] = '\0';
+	if (size < sizeof(buf))
+		buf[size] = '\0';
+
+	spin_lock_bh(&e->lock);
+	e->quota = simple_strtoull(buf, NULL, 0);
+	spin_unlock_bh(&e->lock);
+	return size;
+}
+
+static const struct proc_ops q2_counter_fops = {
+	.proc_read	= quota_proc_read,
+	.proc_write	= quota_proc_write,
+	.proc_lseek	= default_llseek,
+};
+
+static struct xt_quota_counter *
+q2_new_counter(const struct xt_quota_mtinfo2 *q, bool anon)
+{
+	struct xt_quota_counter *e;
+	unsigned int size;
+
+	/* Do not need all the procfs things for anonymous counters. */
+	size = anon ? offsetof(typeof(*e), list) : sizeof(*e);
+	e = kmalloc(size, GFP_KERNEL);
+	if (e == NULL)
+		return NULL;
+
+	e->quota = q->quota;
+	spin_lock_init(&e->lock);
+	if (!anon) {
+		INIT_LIST_HEAD(&e->list);
+		atomic_set(&e->ref, 1);
+		strlcpy(e->name, q->name, sizeof(e->name));
+	}
+	return e;
+}
+
+/**
+ * q2_get_counter - get ref to counter or create new
+ * @name:	name of counter
+ */
+static struct xt_quota_counter *
+q2_get_counter(const struct xt_quota_mtinfo2 *q)
+{
+	struct proc_dir_entry *p;
+	struct xt_quota_counter *e = NULL;
+	struct xt_quota_counter *new_e;
+
+	if (*q->name == '\0')
+		return q2_new_counter(q, true);
+
+	/* No need to hold a lock while getting a new counter */
+	new_e = q2_new_counter(q, false);
+	if (new_e == NULL)
+		goto out;
+
+	spin_lock_bh(&counter_list_lock);
+	list_for_each_entry(e, &counter_list, list)
+		if (strcmp(e->name, q->name) == 0) {
+			atomic_inc(&e->ref);
+			spin_unlock_bh(&counter_list_lock);
+			kfree(new_e);
+			pr_debug("xt_quota2: old counter name=%s", e->name);
+			return e;
+		}
+	e = new_e;
+	pr_debug("xt_quota2: new_counter name=%s", e->name);
+	list_add_tail(&e->list, &counter_list);
+	/* The entry having a refcount of 1 is not directly destructible.
+	 * This func has not yet returned the new entry, thus iptables
+	 * has not references for destroying this entry.
+	 * For another rule to try to destroy it, it would 1st need for this
+	 * func* to be re-invoked, acquire a new ref for the same named quota.
+	 * Nobody will access the e->procfs_entry either.
+	 * So release the lock. */
+	spin_unlock_bh(&counter_list_lock);
+
+	/* create_proc_entry() is not spin_lock happy */
+	p = e->procfs_entry = proc_create_data(e->name, quota_list_perms,
+	                      proc_xt_quota, &q2_counter_fops, e);
+
+	if (IS_ERR_OR_NULL(p)) {
+		spin_lock_bh(&counter_list_lock);
+		list_del(&e->list);
+		spin_unlock_bh(&counter_list_lock);
+		goto out;
+	}
+	proc_set_user(p, quota_list_uid, quota_list_gid);
+	return e;
+
+ out:
+	kfree(e);
+	return NULL;
+}
+
+static int quota_mt2_check(const struct xt_mtchk_param *par)
+{
+	struct xt_quota_mtinfo2 *q = par->matchinfo;
+
+	pr_debug("xt_quota2: check() flags=0x%04x", q->flags);
+
+	if (q->flags & ~XT_QUOTA_MASK)
+		return -EINVAL;
+
+	q->name[sizeof(q->name)-1] = '\0';
+	if (*q->name == '.' || strchr(q->name, '/') != NULL) {
+		printk(KERN_ERR "xt_quota.3: illegal name\n");
+		return -EINVAL;
+	}
+
+	q->master = q2_get_counter(q);
+	if (q->master == NULL) {
+		printk(KERN_ERR "xt_quota.3: memory alloc failure\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void quota_mt2_destroy(const struct xt_mtdtor_param *par)
+{
+	struct xt_quota_mtinfo2 *q = par->matchinfo;
+	struct xt_quota_counter *e = q->master;
+
+	if (*q->name == '\0') {
+		kfree(e);
+		return;
+	}
+
+	spin_lock_bh(&counter_list_lock);
+	if (!atomic_dec_and_test(&e->ref)) {
+		spin_unlock_bh(&counter_list_lock);
+		return;
+	}
+
+	list_del(&e->list);
+	spin_unlock_bh(&counter_list_lock);
+	remove_proc_entry(e->name, proc_xt_quota);
+	kfree(e);
+}
+
+static bool
+quota_mt2(const struct sk_buff *skb, struct xt_action_param *par)
+{
+	struct xt_quota_mtinfo2 *q = (void *)par->matchinfo;
+	struct xt_quota_counter *e = q->master;
+	int charge = (q->flags & XT_QUOTA_PACKET) ? 1 : skb->len;
+	bool no_change = q->flags & XT_QUOTA_NO_CHANGE;
+	bool ret = q->flags & XT_QUOTA_INVERT;
+
+	spin_lock_bh(&e->lock);
+	if (q->flags & XT_QUOTA_GROW) {
+		/*
+		 * While no_change is pointless in "grow" mode, we will
+		 * implement it here simply to have a consistent behavior.
+		 */
+		if (!no_change)
+			e->quota += charge;
+		ret = true; /* note: does not respect inversion (bug??) */
+	} else {
+		if (e->quota > charge) {
+			if (!no_change)
+				e->quota -= charge;
+			ret = !ret;
+		} else if (e->quota) {
+			/* We are transitioning, log that fact. */
+			quota2_log(xt_hooknum(par),
+				   skb,
+				   xt_in(par),
+				   xt_out(par),
+				   q->name);
+			/* we do not allow even small packets from now on */
+			e->quota = 0;
+		}
+	}
+	spin_unlock_bh(&e->lock);
+	return ret;
+}
+
+static struct xt_match quota_mt2_reg[] __read_mostly = {
+	{
+		.name       = "quota2",
+		.revision   = 3,
+		.family     = NFPROTO_IPV4,
+		.checkentry = quota_mt2_check,
+		.match      = quota_mt2,
+		.destroy    = quota_mt2_destroy,
+		.matchsize  = sizeof(struct xt_quota_mtinfo2),
+		.usersize   = offsetof(struct xt_quota_mtinfo2, master),
+		.me         = THIS_MODULE,
+	},
+	{
+		.name       = "quota2",
+		.revision   = 3,
+		.family     = NFPROTO_IPV6,
+		.checkentry = quota_mt2_check,
+		.match      = quota_mt2,
+		.destroy    = quota_mt2_destroy,
+		.matchsize  = sizeof(struct xt_quota_mtinfo2),
+		.usersize   = offsetof(struct xt_quota_mtinfo2, master),
+		.me         = THIS_MODULE,
+	},
+};
+
+static int __init quota_mt2_init(void)
+{
+	int ret;
+	pr_debug("xt_quota2: init()");
+
+#ifdef CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG
+	nflognl = netlink_kernel_create(&init_net, NETLINK_NFLOG, NULL);
+	if (!nflognl)
+		return -ENOMEM;
+#endif
+
+	proc_xt_quota = proc_mkdir("xt_quota", init_net.proc_net);
+	if (proc_xt_quota == NULL)
+		return -EACCES;
+
+	ret = xt_register_matches(quota_mt2_reg, ARRAY_SIZE(quota_mt2_reg));
+	if (ret < 0)
+		remove_proc_entry("xt_quota", init_net.proc_net);
+	pr_debug("xt_quota2: init() %d", ret);
+	return ret;
+}
+
+static void __exit quota_mt2_exit(void)
+{
+	xt_unregister_matches(quota_mt2_reg, ARRAY_SIZE(quota_mt2_reg));
+	remove_proc_entry("xt_quota", init_net.proc_net);
+}
+
+module_init(quota_mt2_init);
+module_exit(quota_mt2_exit);
+MODULE_DESCRIPTION("Xtables: countdown quota match; up counter");
+MODULE_AUTHOR("Sam Johnston <samj@samj.net>");
+MODULE_AUTHOR("Jan Engelhardt <jengelh@medozas.de>");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("ipt_quota2");
+MODULE_ALIAS("ip6t_quota2");
Index: kernel-rpi-6_6/net/netfilter/Kconfig
===================================================================
--- kernel-rpi-6_6.orig/net/netfilter/Kconfig
+++ kernel-rpi-6_6/net/netfilter/Kconfig
@@ -1525,6 +1525,29 @@ config NETFILTER_XT_MATCH_QUOTA
 	  If you want to compile it as a module, say M here and read
 	  <file:Documentation/kbuild/modules.rst>.  If unsure, say `N'.
 
+config NETFILTER_XT_MATCH_QUOTA2
+	tristate '"quota2" match support'
+	depends on NETFILTER_ADVANCED
+	help
+	  This option adds a `quota2' match, which allows to match on a
+	  byte counter correctly and not per CPU.
+	  It allows naming the quotas.
+	  This is based on http://xtables-addons.git.sourceforge.net
+
+	  If you want to compile it as a module, say M here and read
+	  <file:Documentation/kbuild/modules.txt>.  If unsure, say `N'.
+
+config NETFILTER_XT_MATCH_QUOTA2_LOG
+	bool '"quota2" Netfilter LOG support'
+	depends on NETFILTER_XT_MATCH_QUOTA2
+	default n
+	help
+	  This option allows `quota2' to log ONCE when a quota limit
+	  is passed. It logs via NETLINK using the NETLINK_NFLOG family.
+	  It logs similarly to how ipt_ULOG would without data.
+
+	  If unsure, say `N'.
+
 config NETFILTER_XT_MATCH_RATEEST
 	tristate '"rateest" match support'
 	depends on NETFILTER_ADVANCED
Index: kernel-rpi-6_6/include/linux/netfilter/xt_quota2.h
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/include/linux/netfilter/xt_quota2.h
@@ -0,0 +1,26 @@
+#ifndef _XT_QUOTA_H
+#define _XT_QUOTA_H
+#include <linux/types.h>
+
+enum xt_quota_flags {
+	XT_QUOTA_INVERT    = 1 << 0,
+	XT_QUOTA_GROW      = 1 << 1,
+	XT_QUOTA_PACKET    = 1 << 2,
+	XT_QUOTA_NO_CHANGE = 1 << 3,
+	XT_QUOTA_MASK      = 0x0F,
+};
+
+struct xt_quota_counter;
+
+struct xt_quota_mtinfo2 {
+	char name[15];
+	u_int8_t flags;
+
+	/* Comparison-invariant */
+	aligned_u64 quota;
+
+	/* Used internally by the kernel */
+	struct xt_quota_counter *master __attribute__((aligned(8)));
+};
+
+#endif /* _XT_QUOTA_H */
Index: kernel-rpi-6_6/fs/namespace.c
===================================================================
--- kernel-rpi-6_6.orig/fs/namespace.c
+++ kernel-rpi-6_6/fs/namespace.c
@@ -810,8 +810,14 @@ mountpoint:
 			goto done;
 	}
 
-	if (!new)
-		new = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);
+	if (!new) {
+		/*
+		 * We are allocating as GFP_NOFS to appease lockdep:
+		 * since we are holding i_mutex we should not try to
+		 * recurse into filesystem code.
+		 */
+		new = kmalloc(sizeof(struct mountpoint), GFP_NOFS);
+	}
 	if (!new)
 		return ERR_PTR(-ENOMEM);
 
@@ -3632,6 +3638,24 @@ int path_mount(const char *dev_name, str
 	if (!(flags & MS_NOATIME))
 		mnt_flags |= MNT_RELATIME;
 
+	/*
+	 * The nosymfollow option used to be extracted from data_page by an LSM.
+	 * It is now passed in as MS_NOSYMFOLLOW.  We need to also check in
+	 * the old place until all callers have been updated to use the flag.
+	 * Some callers will pass both for cross-kernel compatibility, so
+	 * only check if the new flag isn't already present.
+	 * TODO(b/152074038): Remove this check when all devices are on a kernel
+	 * that supports MS_NOSYMFOLLOW.
+	 */
+	if (data_page && !(flags & MS_NOSYMFOLLOW)) {
+		if (!strncmp((char *)data_page, "nosymfollow", 11) ||
+		    strstr((char *)data_page, ",nosymfollow")) {
+			WARN(1,
+			     "nosymfollow passed in mount data should be changed to the MS_NOSYMFOLLOW flag.");
+			flags |= MS_NOSYMFOLLOW;
+		}
+	}
+
 	/* Separate the per-mountpoint flags */
 	if (flags & MS_NOSUID)
 		mnt_flags |= MNT_NOSUID;
Index: kernel-rpi-6_6/net/netfilter/xt_IDLETIMER.c
===================================================================
--- kernel-rpi-6_6.orig/net/netfilter/xt_IDLETIMER.c
+++ kernel-rpi-6_6/net/netfilter/xt_IDLETIMER.c
@@ -28,6 +28,11 @@
 #include <linux/kobject.h>
 #include <linux/workqueue.h>
 #include <linux/sysfs.h>
+#include <linux/suspend.h>
+#include <net/sock.h>
+#include <net/inet_sock.h>
+
+#define NLMSG_MAX_SIZE 64
 
 struct idletimer_tg {
 	struct list_head entry;
@@ -38,15 +43,112 @@ struct idletimer_tg {
 	struct kobject *kobj;
 	struct device_attribute attr;
 
+	struct timespec64 delayed_timer_trigger;
+	struct timespec64 last_modified_timer;
+	struct timespec64 last_suspend_time;
+	struct notifier_block pm_nb;
+
+	int timeout;
 	unsigned int refcnt;
 	u8 timer_type;
+
+	bool work_pending;
+	bool send_nl_msg;
+	bool active;
+	uid_t uid;
+	bool suspend_time_valid;
 };
 
 static LIST_HEAD(idletimer_tg_list);
 static DEFINE_MUTEX(list_mutex);
+static DEFINE_SPINLOCK(timestamp_lock);
 
 static struct kobject *idletimer_tg_kobj;
 
+static bool check_for_delayed_trigger(struct idletimer_tg *timer,
+				      struct timespec64 *ts)
+{
+	bool state;
+	struct timespec64 temp;
+	spin_lock_bh(&timestamp_lock);
+	timer->work_pending = false;
+	if ((ts->tv_sec - timer->last_modified_timer.tv_sec) > timer->timeout ||
+	    timer->delayed_timer_trigger.tv_sec != 0) {
+		state = false;
+		temp.tv_sec = timer->timeout;
+		temp.tv_nsec = 0;
+		if (timer->delayed_timer_trigger.tv_sec != 0) {
+			temp = timespec64_add(timer->delayed_timer_trigger,
+					      temp);
+			ts->tv_sec = temp.tv_sec;
+			ts->tv_nsec = temp.tv_nsec;
+			timer->delayed_timer_trigger.tv_sec = 0;
+			timer->work_pending = true;
+			schedule_work(&timer->work);
+		} else {
+			temp = timespec64_add(timer->last_modified_timer, temp);
+			ts->tv_sec = temp.tv_sec;
+			ts->tv_nsec = temp.tv_nsec;
+		}
+	} else {
+		state = timer->active;
+	}
+	spin_unlock_bh(&timestamp_lock);
+	return state;
+}
+
+static void notify_netlink_uevent(const char *iface, struct idletimer_tg *timer)
+{
+	char iface_msg[NLMSG_MAX_SIZE];
+	char state_msg[NLMSG_MAX_SIZE];
+	char timestamp_msg[NLMSG_MAX_SIZE];
+	char uid_msg[NLMSG_MAX_SIZE];
+	char *envp[] = { iface_msg, state_msg, timestamp_msg, uid_msg, NULL };
+	int res;
+	struct timespec64 ts;
+	u64 time_ns;
+	bool state;
+
+	res = snprintf(iface_msg, NLMSG_MAX_SIZE, "INTERFACE=%s",
+		       iface);
+	if (NLMSG_MAX_SIZE <= res) {
+		pr_err("message too long (%d)\n", res);
+		return;
+	}
+
+	ts = ktime_to_timespec64(ktime_get_boottime());
+	state = check_for_delayed_trigger(timer, &ts);
+	res = snprintf(state_msg, NLMSG_MAX_SIZE, "STATE=%s",
+		       state ? "active" : "inactive");
+
+	if (NLMSG_MAX_SIZE <= res) {
+		pr_err("message too long (%d)\n", res);
+		return;
+	}
+
+	if (state) {
+		res = snprintf(uid_msg, NLMSG_MAX_SIZE, "UID=%u", timer->uid);
+		if (NLMSG_MAX_SIZE <= res)
+			pr_err("message too long (%d)\n", res);
+	} else {
+		res = snprintf(uid_msg, NLMSG_MAX_SIZE, "UID=");
+		if (NLMSG_MAX_SIZE <= res)
+			pr_err("message too long (%d)\n", res);
+	}
+
+	time_ns = timespec64_to_ns(&ts);
+	res = snprintf(timestamp_msg, NLMSG_MAX_SIZE, "TIME_NS=%llu", time_ns);
+	if (NLMSG_MAX_SIZE <= res) {
+		timestamp_msg[0] = '\0';
+		pr_err("message too long (%d)\n", res);
+	}
+
+	pr_debug("putting nlmsg: <%s> <%s> <%s> <%s>\n", iface_msg, state_msg,
+		 timestamp_msg, uid_msg);
+	kobject_uevent_env(idletimer_tg_kobj, KOBJ_CHANGE, envp);
+	return;
+}
+
 static
 struct idletimer_tg *__idletimer_tg_find_by_label(const char *label)
 {
@@ -67,6 +169,7 @@ static ssize_t idletimer_tg_show(struct
 	unsigned long expires = 0;
 	struct timespec64 ktimespec = {};
 	long time_diff = 0;
+	unsigned long now = jiffies;
 
 	mutex_lock(&list_mutex);
 
@@ -78,15 +181,19 @@ static ssize_t idletimer_tg_show(struct
 			time_diff = ktimespec.tv_sec;
 		} else {
 			expires = timer->timer.expires;
-			time_diff = jiffies_to_msecs(expires - jiffies) / 1000;
+			time_diff = jiffies_to_msecs(expires - now) / 1000;
 		}
 	}
 
 	mutex_unlock(&list_mutex);
 
-	if (time_after(expires, jiffies) || ktimespec.tv_sec > 0)
+	if (time_after(expires, now) || ktimespec.tv_sec > 0)
 		return sysfs_emit(buf, "%ld\n", time_diff);
 
+	if (timer->send_nl_msg)
+		return sysfs_emit(buf, "0 %d\n",
+				  jiffies_to_msecs(now - expires) / 1000);
+
 	return sysfs_emit(buf, "0\n");
 }
 
@@ -96,6 +203,9 @@ static void idletimer_tg_work(struct wor
 						  work);
 
 	sysfs_notify(idletimer_tg_kobj, NULL, timer->attr.attr.name);
+
+	if (timer->send_nl_msg)
+		notify_netlink_uevent(timer->attr.attr.name, timer);
 }
 
 static void idletimer_tg_expired(struct timer_list *t)
@@ -104,7 +214,62 @@ static void idletimer_tg_expired(struct
 
 	pr_debug("timer %s expired\n", timer->attr.attr.name);
 
+	spin_lock_bh(&timestamp_lock);
+	timer->active = false;
+	timer->work_pending = true;
 	schedule_work(&timer->work);
+	spin_unlock_bh(&timestamp_lock);
+}
+
+static int idletimer_resume(struct notifier_block *notifier,
+			    unsigned long pm_event, void *unused)
+{
+	struct timespec64 ts;
+	unsigned long time_diff, now = jiffies;
+	struct idletimer_tg *timer = container_of(notifier,
+						  struct idletimer_tg, pm_nb);
+	if (!timer)
+		return NOTIFY_DONE;
+
+	switch (pm_event) {
+	case PM_SUSPEND_PREPARE:
+		timer->last_suspend_time =
+			ktime_to_timespec64(ktime_get_boottime());
+		timer->suspend_time_valid = true;
+		break;
+	case PM_POST_SUSPEND:
+		if (!timer->suspend_time_valid)
+			break;
+		timer->suspend_time_valid = false;
+
+		spin_lock_bh(&timestamp_lock);
+		if (!timer->active) {
+			spin_unlock_bh(&timestamp_lock);
+			break;
+		}
+		/* since jiffies are not updated when suspended now represents
+		 * the time it would have suspended */
+		if (time_after(timer->timer.expires, now)) {
+			ts = ktime_to_timespec64(ktime_get_boottime());
+			ts = timespec64_sub(ts, timer->last_suspend_time);
+			time_diff = timespec64_to_jiffies(&ts);
+			if (timer->timer.expires > (time_diff + now)) {
+				mod_timer_pending(&timer->timer,
+						  (timer->timer.expires - time_diff));
+			} else {
+				del_timer(&timer->timer);
+				timer->timer.expires = 0;
+				timer->active = false;
+				timer->work_pending = true;
+				schedule_work(&timer->work);
+			}
+		}
+		spin_unlock_bh(&timestamp_lock);
+		break;
+	default:
+		break;
+	}
+	return NOTIFY_DONE;
 }
 
 static enum alarmtimer_restart idletimer_tg_alarmproc(struct alarm *alarm,
@@ -158,17 +323,34 @@ static int idletimer_tg_create(struct id
 
 	ret = sysfs_create_file(idletimer_tg_kobj, &info->timer->attr.attr);
 	if (ret < 0) {
-		pr_debug("couldn't add file to sysfs");
+		pr_debug("couldn't add file to sysfs\n");
 		goto out_free_attr;
 	}
 
 	list_add(&info->timer->entry, &idletimer_tg_list);
-
-	timer_setup(&info->timer->timer, idletimer_tg_expired, 0);
+	pr_debug("timer type value is 0.\n");
+	info->timer->timer_type = 0;
 	info->timer->refcnt = 1;
+	info->timer->send_nl_msg = false;
+	info->timer->active = true;
+	info->timer->timeout = info->timeout;
+
+	info->timer->delayed_timer_trigger.tv_sec = 0;
+	info->timer->delayed_timer_trigger.tv_nsec = 0;
+	info->timer->work_pending = false;
+	info->timer->uid = 0;
+	info->timer->last_modified_timer =
+		ktime_to_timespec64(ktime_get_boottime());
+
+	info->timer->pm_nb.notifier_call = idletimer_resume;
+	ret = register_pm_notifier(&info->timer->pm_nb);
+	if (ret)
+		printk(KERN_WARNING "[%s] Failed to register pm notifier %d\n",
+		       __func__, ret);
 
 	INIT_WORK(&info->timer->work, idletimer_tg_work);
 
+	timer_setup(&info->timer->timer, idletimer_tg_expired, 0);
 	mod_timer(&info->timer->timer,
 		  msecs_to_jiffies(info->timeout * 1000) + jiffies);
 
@@ -186,7 +368,7 @@ static int idletimer_tg_create_v1(struct
 {
 	int ret;
 
-	info->timer = kmalloc(sizeof(*info->timer), GFP_KERNEL);
+	info->timer = kzalloc(sizeof(*info->timer), GFP_KERNEL);
 	if (!info->timer) {
 		ret = -ENOMEM;
 		goto out;
@@ -207,7 +389,7 @@ static int idletimer_tg_create_v1(struct
 
 	ret = sysfs_create_file(idletimer_tg_kobj, &info->timer->attr.attr);
 	if (ret < 0) {
-		pr_debug("couldn't add file to sysfs");
+		pr_debug("couldn't add file to sysfs\n");
 		goto out_free_attr;
 	}
 
@@ -215,9 +397,25 @@ static int idletimer_tg_create_v1(struct
 	kobject_uevent(idletimer_tg_kobj,KOBJ_ADD);
 
 	list_add(&info->timer->entry, &idletimer_tg_list);
-	pr_debug("timer type value is %u", info->timer_type);
+	pr_debug("timer type value is %u\n", info->timer_type);
 	info->timer->timer_type = info->timer_type;
 	info->timer->refcnt = 1;
+	info->timer->send_nl_msg = (info->send_nl_msg != 0);
+	info->timer->active = true;
+	info->timer->timeout = info->timeout;
+
+	info->timer->delayed_timer_trigger.tv_sec = 0;
+	info->timer->delayed_timer_trigger.tv_nsec = 0;
+	info->timer->work_pending = false;
+	info->timer->uid = 0;
+	info->timer->last_modified_timer =
+		ktime_to_timespec64(ktime_get_boottime());
+
+	info->timer->pm_nb.notifier_call = idletimer_resume;
+	ret = register_pm_notifier(&info->timer->pm_nb);
+	if (ret)
+		printk(KERN_WARNING "[%s] Failed to register pm notifier %d\n",
+		       __func__, ret);
 
 	INIT_WORK(&info->timer->work, idletimer_tg_work);
 
@@ -231,7 +429,7 @@ static int idletimer_tg_create_v1(struct
 	} else {
 		timer_setup(&info->timer->timer, idletimer_tg_expired, 0);
 		mod_timer(&info->timer->timer,
-				msecs_to_jiffies(info->timeout * 1000) + jiffies);
+			  msecs_to_jiffies(info->timeout * 1000) + jiffies);
 	}
 
 	return 0;
@@ -244,6 +442,41 @@ out:
 	return ret;
 }
 
+static void reset_timer(struct idletimer_tg * const info_timer,
+			const __u32 info_timeout,
+			struct sk_buff *skb)
+{
+	unsigned long now = jiffies;
+	bool timer_prev;
+
+	spin_lock_bh(&timestamp_lock);
+	timer_prev = info_timer->active;
+	info_timer->active = true;
+	/* timer_prev is used to guard overflow problem in time_before*/
+	if (!timer_prev || time_before(info_timer->timer.expires, now)) {
+		pr_debug("Starting Checkentry timer (Expired, Jiffies): %lu, %lu\n",
+			 info_timer->timer.expires, now);
+
+		/* Stores the uid resposible for waking up the radio */
+		if (skb && (skb->sk)) {
+			info_timer->uid = from_kuid_munged(current_user_ns(),
+							   sock_i_uid(skb_to_full_sk(skb)));
+		}
+
+		/* checks if there is a pending inactive notification*/
+		if (info_timer->work_pending)
+			info_timer->delayed_timer_trigger = info_timer->last_modified_timer;
+		else {
+			info_timer->work_pending = true;
+			schedule_work(&info_timer->work);
+		}
+	}
+
+	info_timer->last_modified_timer = ktime_to_timespec64(ktime_get_boottime());
+	mod_timer(&info_timer->timer, msecs_to_jiffies(info_timeout * 1000) + now);
+	spin_unlock_bh(&timestamp_lock);
+}
+
 /*
  * The actual xt_tables plugin.
  */
@@ -251,12 +484,21 @@ static unsigned int idletimer_tg_target(
 					 const struct xt_action_param *par)
 {
 	const struct idletimer_tg_info *info = par->targinfo;
+	unsigned long now = jiffies;
 
 	pr_debug("resetting timer %s, timeout period %u\n",
 		 info->label, info->timeout);
 
-	mod_timer(&info->timer->timer,
-		  msecs_to_jiffies(info->timeout * 1000) + jiffies);
+	info->timer->active = true;
+
+	if (time_before(info->timer->timer.expires, now)) {
+		schedule_work(&info->timer->work);
+		pr_debug("Starting timer %s (Expired, Jiffies): %lu, %lu\n",
+			 info->label, info->timer->timer.expires, now);
+	}
+
+	/* TODO: Avoid modifying timers on each packet */
+	reset_timer(info->timer, info->timeout, skb);
 
 	return XT_CONTINUE;
 }
@@ -268,6 +510,7 @@ static unsigned int idletimer_tg_target_
 					 const struct xt_action_param *par)
 {
 	const struct idletimer_tg_info_v1 *info = par->targinfo;
+	unsigned long now = jiffies;
 
 	pr_debug("resetting timer %s, timeout period %u\n",
 		 info->label, info->timeout);
@@ -276,8 +519,16 @@ static unsigned int idletimer_tg_target_
 		ktime_t tout = ktime_set(info->timeout, 0);
 		alarm_start_relative(&info->timer->alarm, tout);
 	} else {
-		mod_timer(&info->timer->timer,
-				msecs_to_jiffies(info->timeout * 1000) + jiffies);
+		info->timer->active = true;
+
+		if (time_before(info->timer->timer.expires, now)) {
+			schedule_work(&info->timer->work);
+			pr_debug("Starting timer %s (Expired, Jiffies): %lu, %lu\n",
+				 info->label, info->timer->timer.expires, now);
+		}
+
+		/* TODO: Avoid modifying timers on each packet */
+		reset_timer(info->timer, info->timeout, skb);
 	}
 
 	return XT_CONTINUE;
@@ -321,9 +572,7 @@ static int idletimer_tg_checkentry(const
 	info->timer = __idletimer_tg_find_by_label(info->label);
 	if (info->timer) {
 		info->timer->refcnt++;
-		mod_timer(&info->timer->timer,
-			  msecs_to_jiffies(info->timeout * 1000) + jiffies);
-
+		reset_timer(info->timer, info->timeout, NULL);
 		pr_debug("increased refcnt of timer %s to %u\n",
 			 info->label, info->timer->refcnt);
 	} else {
@@ -346,9 +595,6 @@ static int idletimer_tg_checkentry_v1(co
 
 	pr_debug("checkentry targinfo%s\n", info->label);
 
-	if (info->send_nl_msg)
-		return -EOPNOTSUPP;
-
 	ret = idletimer_tg_helper((struct idletimer_tg_info *)info);
 	if(ret < 0)
 	{
@@ -361,6 +607,11 @@ static int idletimer_tg_checkentry_v1(co
 		return -EINVAL;
 	}
 
+	if (info->send_nl_msg > 1) {
+		pr_debug("invalid value for send_nl_msg\n");
+		return -EINVAL;
+	}
+
 	mutex_lock(&list_mutex);
 
 	info->timer = __idletimer_tg_find_by_label(info->label);
@@ -383,8 +634,7 @@ static int idletimer_tg_checkentry_v1(co
 				alarm_start_relative(&info->timer->alarm, tout);
 			}
 		} else {
-				mod_timer(&info->timer->timer,
-					msecs_to_jiffies(info->timeout * 1000) + jiffies);
+			reset_timer(info->timer, info->timeout, NULL);
 		}
 		pr_debug("increased refcnt of timer %s to %u\n",
 			 info->label, info->timer->refcnt);
@@ -422,8 +672,10 @@ static void idletimer_tg_destroy(const s
 	mutex_unlock(&list_mutex);
 
 	timer_shutdown_sync(&info->timer->timer);
+  unregister_pm_notifier(&info->timer->pm_nb);
 	cancel_work_sync(&info->timer->work);
 	sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr);
+  unregister_pm_notifier(&info->timer->pm_nb);
 	kfree(info->timer->attr.attr.name);
 	kfree(info->timer);
 }
@@ -453,6 +705,7 @@ static void idletimer_tg_destroy_v1(cons
 	} else {
 		timer_shutdown_sync(&info->timer->timer);
 	}
+  unregister_pm_notifier(&info->timer->pm_nb);
 	cancel_work_sync(&info->timer->work);
 	sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr);
 	kfree(info->timer->attr.attr.name);
@@ -565,3 +818,4 @@ MODULE_DESCRIPTION("Xtables: idle time m
 MODULE_LICENSE("GPL v2");
 MODULE_ALIAS("ipt_IDLETIMER");
 MODULE_ALIAS("ip6t_IDLETIMER");
+MODULE_ALIAS("arpt_IDLETIMER");
Index: kernel-rpi-6_6/include/linux/pkglist.h
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/include/linux/pkglist.h
@@ -0,0 +1,38 @@
+#ifndef _PKGLIST_H_
+#define _PKGLIST_H_
+
+#include <linux/dcache.h>
+#include <linux/uidgid.h>
+
+#define QSTR_LITERAL(string) QSTR_INIT(string, sizeof(string)-1)
+
+static inline bool str_case_eq(const char *s1, const char *s2)
+{
+	return !strcasecmp(s1, s2);
+}
+
+static inline bool str_n_case_eq(const char *s1, const char *s2, size_t len)
+{
+	return !strncasecmp(s1, s2, len);
+}
+
+static inline bool qstr_case_eq(const struct qstr *q1, const struct qstr *q2)
+{
+	return q1->len == q2->len && str_case_eq(q1->name, q2->name);
+}
+
+#define BY_NAME		BIT(0)
+#define BY_USERID	BIT(1)
+
+struct pkg_list {
+	struct list_head list;
+	void (*update)(int flags, const struct qstr *name, uint32_t userid);
+};
+
+kuid_t pkglist_get_appid(const char *key);
+kgid_t pkglist_get_ext_gid(const char *key);
+bool pkglist_user_is_excluded(const char *key, uint32_t user);
+kuid_t pkglist_get_allowed_appid(const char *key, uint32_t user);
+void pkglist_register_update_listener(struct pkg_list *pkg);
+void pkglist_unregister_update_listener(struct pkg_list *pkg);
+#endif
Index: kernel-rpi-6_6/include/uapi/linux/magic.h
===================================================================
--- kernel-rpi-6_6.orig/include/uapi/linux/magic.h
+++ kernel-rpi-6_6/include/uapi/linux/magic.h
@@ -62,6 +62,8 @@
 #define REISER2FS_SUPER_MAGIC_STRING	"ReIsEr2Fs"
 #define REISER2FS_JR_SUPER_MAGIC_STRING	"ReIsEr3Fs"
 
+#define ESDFS_SUPER_MAGIC	0x00035df5
+
 #define SMB_SUPER_MAGIC		0x517B
 #define CIFS_SUPER_MAGIC	0xFF534D42      /* the first four bytes of SMB PDUs */
 #define SMB2_SUPER_MAGIC	0xFE534D42
Index: kernel-rpi-6_6/kernel/sysctl.c
===================================================================
--- kernel-rpi-6_6.orig/kernel/sysctl.c
+++ kernel-rpi-6_6/kernel/sysctl.c
@@ -1623,6 +1623,20 @@ static struct ctl_table kern_table[] = {
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec,
 	},
+	{
+		.procname	= "iowait_reset_ticks",
+		.data		= &sysctl_iowait_reset_ticks,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
+		.procname	= "iowait_apply_ticks",
+		.data		= &sysctl_iowait_apply_ticks,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
 #ifdef CONFIG_PROC_SYSCTL
 	{
 		.procname	= "tainted",
@@ -2220,6 +2234,17 @@ static struct ctl_table vm_table[] = {
 		.mode		= 0644,
 		.proc_handler	= proc_doulongvec_minmax,
 	},
+#ifdef CONFIG_DISK_BASED_SWAP
+  {
+    .procname = "disk_based_swap",
+    .data   = &sysctl_disk_based_swap,
+    .maxlen   = sizeof(sysctl_disk_based_swap),
+    .mode   = 0644,
+    .proc_handler = proc_dointvec_minmax,
+    .extra1   = SYSCTL_ZERO,
+    .extra2   = SYSCTL_ONE,
+  },
+#endif
 	{
 		.procname	= "admin_reserve_kbytes",
 		.data		= &sysctl_admin_reserve_kbytes,
@@ -2248,6 +2273,15 @@ static struct ctl_table vm_table[] = {
 		.extra1		= (void *)&mmap_rnd_compat_bits_min,
 		.extra2		= (void *)&mmap_rnd_compat_bits_max,
 	},
+	{
+		.procname	= "mmap_noexec_taint",
+		.data		= &sysctl_mmap_noexec_taint,
+		.maxlen		= sizeof(sysctl_mmap_noexec_taint),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
 #endif
 	{ }
 };
Index: kernel-rpi-6_6/include/linux/sched/sysctl.h
===================================================================
--- kernel-rpi-6_6.orig/include/linux/sched/sysctl.h
+++ kernel-rpi-6_6/include/linux/sched/sysctl.h
@@ -19,6 +19,9 @@ enum sched_tunable_scaling {
 	SCHED_TUNABLESCALING_END,
 };
 
+extern unsigned int sysctl_iowait_reset_ticks;
+extern unsigned int sysctl_iowait_apply_ticks;
+
 #define NUMA_BALANCING_DISABLED		0x0
 #define NUMA_BALANCING_NORMAL		0x1
 #define NUMA_BALANCING_MEMORY_TIERING	0x2
Index: kernel-rpi-6_6/kernel/sched/core.c
===================================================================
--- kernel-rpi-6_6.orig/kernel/sched/core.c
+++ kernel-rpi-6_6/kernel/sched/core.c
@@ -149,6 +149,9 @@ __read_mostly int sysctl_resched_latency
  */
 const_debug unsigned int sysctl_sched_nr_migrate = SCHED_NR_MIGRATE_BREAK;
 
+unsigned int sysctl_iowait_reset_ticks = 20;
+unsigned int sysctl_iowait_apply_ticks = 10;
+
 __read_mostly int scheduler_running;
 
 #ifdef CONFIG_SCHED_CORE
@@ -9331,7 +9334,8 @@ int cpuset_cpumask_can_shrink(const stru
 	return ret;
 }
 
-int task_can_attach(struct task_struct *p)
+int task_can_attach(struct task_struct *p,
+          const struct cpumask *cs_effective_cpus)
 {
 	int ret = 0;
 
@@ -9344,9 +9348,22 @@ int task_can_attach(struct task_struct *
 	 * success of set_cpus_allowed_ptr() on all attached tasks
 	 * before cpus_mask may be changed.
 	 */
-	if (p->flags & PF_NO_SETAFFINITY)
+	if (p->flags & PF_NO_SETAFFINITY) {
 		ret = -EINVAL;
-
+    goto out;
+  }
+	if (dl_task(p) && !cpumask_intersects(task_rq(p)->rd->span,
+                cs_effective_cpus)) {
+    int cpu = cpumask_any_and(cpu_active_mask, cs_effective_cpus);
+
+    if (unlikely(cpu >= nr_cpu_ids))
+      return -EINVAL;
+    if (p && !p->dl.dl_bw)
+      ret = dl_bw_alloc(cpu, p->dl.dl_bw);
+    else
+      ret = dl_bw_check_overflow(cpu);
+  }
+out:
 	return ret;
 }
 
@@ -10824,6 +10841,27 @@ static int cpu_uclamp_max_show(struct se
 	cpu_uclamp_print(sf, UCLAMP_MAX);
 	return 0;
 }
+
+static int cpu_uclamp_ls_write_u64(struct cgroup_subsys_state *css,
+                                  struct cftype *cftype, u64 ls)
+{
+       struct task_group *tg;
+
+       if (ls > 1)
+               return -EINVAL;
+       tg = css_tg(css);
+       tg->latency_sensitive = (unsigned int) ls;
+
+       return 0;
+}
+
+static u64 cpu_uclamp_ls_read_u64(struct cgroup_subsys_state *css,
+                                 struct cftype *cft)
+{
+       struct task_group *tg = css_tg(css);
+
+       return (u64) tg->latency_sensitive;
+}
 #endif /* CONFIG_UCLAMP_TASK_GROUP */
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -11296,6 +11334,12 @@ static struct cftype cpu_legacy_files[]
 		.seq_show = cpu_uclamp_max_show,
 		.write = cpu_uclamp_max_write,
 	},
+  {
+   .name = "uclamp.latency_sensitive",
+   .flags = CFTYPE_NOT_ON_ROOT,
+   .read_u64 = cpu_uclamp_ls_read_u64,
+   .write_u64 = cpu_uclamp_ls_write_u64,
+  },
 #endif
 	{ }	/* Terminate */
 };
@@ -11512,6 +11556,12 @@ static struct cftype cpu_files[] = {
 		.seq_show = cpu_uclamp_max_show,
 		.write = cpu_uclamp_max_write,
 	},
+  {
+    .name = "uclamp.latency_sensitive",
+    .flags = CFTYPE_NOT_ON_ROOT,
+    .read_u64 = cpu_uclamp_ls_read_u64,
+    .write_u64 = cpu_uclamp_ls_write_u64,
+  },
 #endif
 	{ }	/* terminate */
 };
Index: kernel-rpi-6_6/kernel/sched/cpufreq_schedutil.c
===================================================================
--- kernel-rpi-6_6.orig/kernel/sched/cpufreq_schedutil.c
+++ kernel-rpi-6_6/kernel/sched/cpufreq_schedutil.c
@@ -20,6 +20,7 @@ struct sugov_policy {
 	struct list_head	tunables_hook;
 
 	raw_spinlock_t		update_lock;
+  u64     last_update;
 	u64			last_freq_update_time;
 	s64			freq_update_delay_ns;
 	unsigned int		next_freq;
@@ -178,9 +179,13 @@ static bool sugov_iowait_reset(struct su
 			       bool set_iowait_boost)
 {
 	s64 delta_ns = time - sg_cpu->last_update;
+  unsigned int ticks = TICK_NSEC;
+
+  if (sysctl_iowait_reset_ticks)
+    ticks = sysctl_iowait_reset_ticks * TICK_NSEC;
 
 	/* Reset boost only if a tick has elapsed since last request */
-	if (delta_ns <= TICK_NSEC)
+	if (delta_ns <= ticks)
 		return false;
 
 	sg_cpu->iowait_boost = set_iowait_boost ? IOWAIT_BOOST_MIN : 0;
@@ -255,6 +260,7 @@ static void sugov_iowait_apply(struct su
 			       unsigned long max_cap)
 {
 	unsigned long boost;
+  struct sugov_policy *sg_policy = sg_cpu->sg_policy;
 
 	/* No boost currently required */
 	if (!sg_cpu->iowait_boost)
@@ -264,7 +270,9 @@ static void sugov_iowait_apply(struct su
 	if (sugov_iowait_reset(sg_cpu, time, false))
 		return;
 
-	if (!sg_cpu->iowait_boost_pending) {
+	if (!sg_cpu->iowait_boost_pending &&
+         (!sysctl_iowait_apply_ticks ||
+          (time - sg_policy->last_update > (sysctl_iowait_apply_ticks * TICK_NSEC)))) {
 		/*
 		 * No boost pending; reduce the boost value.
 		 */
@@ -454,6 +462,7 @@ sugov_update_shared(struct update_util_d
 		if (!sugov_update_next_freq(sg_policy, time, next_f))
 			goto unlock;
 
+    sg_policy->last_update = time;
 		if (sg_policy->policy->fast_switch_enabled)
 			cpufreq_driver_fast_switch(sg_policy->policy, next_f);
 		else
Index: kernel-rpi-6_6/kernel/sched/sched.h
===================================================================
--- kernel-rpi-6_6.orig/kernel/sched/sched.h
+++ kernel-rpi-6_6/kernel/sched/sched.h
@@ -10,6 +10,7 @@
 #include <linux/sched/cpufreq.h>
 #include <linux/sched/deadline.h>
 #include <linux/sched.h>
+#include <linux/sched/latsense.h>
 #include <linux/sched/loadavg.h>
 #include <linux/sched/mm.h>
 #include <linux/sched/rseq_api.h>
@@ -410,6 +411,7 @@ struct task_group {
 	struct uclamp_se	uclamp_req[UCLAMP_CNT];
 	/* Effective clamp values used for a task group */
 	struct uclamp_se	uclamp[UCLAMP_CNT];
+  unsigned int            latency_sensitive;
 #endif
 
 };
@@ -2534,6 +2536,9 @@ extern const_debug unsigned int sysctl_s
 
 extern unsigned int sysctl_sched_base_slice;
 
+extern unsigned int sysctl_iowait_reset_ticks;
+extern unsigned int sysctl_iowait_apply_ticks;
+
 #ifdef CONFIG_SCHED_DEBUG
 extern int sysctl_resched_latency_warn_ms;
 extern int sysctl_resched_latency_warn_once;
@@ -3117,6 +3122,11 @@ out:
 	return clamp(util, min_util, max_util);
 }
 
+static inline bool uclamp_boosted(struct task_struct *p)
+{
+  return uclamp_eff_value(p, UCLAMP_MIN) > 0;
+}
+
 /* Is the rq being capped/throttled by uclamp_max? */
 static inline bool uclamp_rq_is_capped(struct rq *rq)
 {
@@ -3160,7 +3170,7 @@ unsigned long uclamp_rq_util_with(struct
 {
 	return util;
 }
-
+static inline bool uclamp_boosted(struct task_struct *p) {return false;}
 static inline bool uclamp_rq_is_capped(struct rq *rq) { return false; }
 
 static inline bool uclamp_is_used(void)
@@ -3187,7 +3197,33 @@ static inline bool uclamp_rq_is_idle(str
 	return false;
 }
 #endif /* CONFIG_UCLAMP_TASK */
+#ifdef CONFIG_UCLAMP_TASK_GROUP
+static inline bool uclamp_latency_sensitive(struct task_struct *p)
+{
+       struct cgroup_subsys_state *css = task_css(p, cpu_cgrp_id);
+       struct task_group *tg;
 
+#ifdef CONFIG_PROC_LATSENSE
+       /* Over CGroup interface with task-interface. */
+       if (p->proc_latency_sensitive)
+               return true;
+#endif
+
+       if (!css)
+               return false;
+       tg = container_of(css, struct task_group, css);
+
+       return tg->latency_sensitive;
+}
+#else
+static inline bool uclamp_latency_sensitive(struct task_struct *p)
+{
+#ifdef CONFIG_PROC_LATSENSE
+       return !!p->proc_latency_sensitive;
+#endif
+       return false;
+}
+#endif /* CONFIG_UCLAMP_TASK_GROUP */
 #ifdef CONFIG_HAVE_SCHED_AVG_IRQ
 static inline unsigned long cpu_util_irq(struct rq *rq)
 {
Index: kernel-rpi-6_6/mm/Kconfig
===================================================================
--- kernel-rpi-6_6.orig/mm/Kconfig
+++ kernel-rpi-6_6/mm/Kconfig
@@ -680,6 +680,17 @@ config PAGE_REPORTING
 	  those pages to another entity, such as a hypervisor, so that the
 	  memory can be freed within the host for other uses.
 
+config PROCESS_RECLAIM
+	bool "Enable process reclaim"
+	depends on PROC_FS && MMU
+	help
+	 It allows to reclaim pages of the process by /proc/pid/reclaim.
+
+	 (echo file > /proc/PID/reclaim) reclaims file-backed pages only.
+	 (echo anon > /proc/PID/reclaim) reclaims anonymous pages only.
+	 (echo all > /proc/PID/reclaim) reclaims all pages.
+
+	 Any other value is ignored.
 #
 # support for page migration
 #
@@ -778,6 +789,23 @@ config DEFAULT_MMAP_MIN_ADDR
 	  This value can be changed after boot using the
 	  /proc/sys/vm/mmap_min_addr tunable.
 
+config MMAP_NOEXEC_TAINT
+	int "Turns on tainting of mmap()d files from noexec mountpoints"
+	default 1 if MMU
+	default 0 if !MMU
+	help
+	  By default, the ability to change the protections of a virtual
+	  memory area to allow execution depend on if the vma has the
+	  VM_MAYEXEC flag.  When mapping regions from files, VM_MAYEXEC
+	  will be unset if the containing mountpoint is mounted MNT_NOEXEC.
+	  By setting the value to 0, any mmap()d region may be later
+	  mprotect()d with PROT_EXEC.
+
+	  If unsure, keep the value set to 1.
+
+	  This value can be changed after boot using the
+	  /proc/sys/vm/mmap_noexec_taint tunable.
+
 config ARCH_SUPPORTS_MEMORY_FAILURE
 	bool
 
@@ -1279,8 +1307,8 @@ config PER_VMA_LOCK
 	  handling page faults instead of taking mmap_lock.
 
 config LOCK_MM_AND_FIND_VMA
-	bool
-	depends on !STACK_GROWSUP
+       bool
+       depends on !STACK_GROWSUP
 
 source "mm/damon/Kconfig"
 
Index: kernel-rpi-6_6/fs/proc/base.c
===================================================================
--- kernel-rpi-6_6.orig/fs/proc/base.c
+++ kernel-rpi-6_6/fs/proc/base.c
@@ -98,6 +98,7 @@
 #include <linux/resctrl.h>
 #include <linux/cn_proc.h>
 #include <linux/ksm.h>
+#include <linux/cpufreq_times.h>
 #include <trace/events/oom.h>
 #include "internal.h"
 #include "fd.h"
@@ -186,6 +187,65 @@ struct pid_entry {
 		NULL, &proc_pid_attr_operations,	\
 		{ .lsm = LSM })
 
+#if IS_ENABLED(CONFIG_PROC_MEM_RESTRICT_OPEN_READ_ALL)
+DEFINE_STATIC_KEY_TRUE_RO(proc_mem_restrict_open_read_all);
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_open_read_ptracer);
+#elif IS_ENABLED(CONFIG_PROC_MEM_RESTRICT_OPEN_READ_PTRACE)
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_open_read_all);
+DEFINE_STATIC_KEY_TRUE_RO(proc_mem_restrict_open_read_ptracer);
+#else
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_open_read_all);
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_open_read_ptracer);
+#endif
+
+#if IS_ENABLED(CONFIG_PROC_MEM_RESTRICT_OPEN_WRITE_ALL)
+DEFINE_STATIC_KEY_TRUE_RO(proc_mem_restrict_open_write_all);
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_open_write_ptracer);
+#elif IS_ENABLED(CONFIG_PROC_MEM_RESTRICT_OPEN_WRITE_PTRACE)
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_open_write_all);
+DEFINE_STATIC_KEY_TRUE_RO(proc_mem_restrict_open_write_ptracer);
+#else
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_open_write_all);
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_open_write_ptracer);
+#endif
+
+#if IS_ENABLED(CONFIG_PROC_MEM_RESTRICT_WRITE_ALL)
+DEFINE_STATIC_KEY_TRUE_RO(proc_mem_restrict_write_all);
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_write_ptracer);
+#elif IS_ENABLED(CONFIG_PROC_MEM_RESTRICT_WRITE_PTRACE)
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_write_all);
+DEFINE_STATIC_KEY_TRUE_RO(proc_mem_restrict_write_ptracer);
+#else
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_write_all);
+DEFINE_STATIC_KEY_FALSE_RO(proc_mem_restrict_write_ptracer);
+#endif
+
+#define DEFINE_EARLY_PROC_MEM_RESTRICT(name)					\
+static int __init early_proc_mem_restrict_##name(char *buf)			\
+{										\
+	if (!buf)								\
+		return -EINVAL;							\
+										\
+	if (strcmp(buf, "all") == 0) {						\
+		static_key_enable(&proc_mem_restrict_##name##_all.key);		\
+		static_key_disable(&proc_mem_restrict_##name##_ptracer.key);	\
+	} else if (strcmp(buf, "ptracer") == 0) {				\
+		static_key_disable(&proc_mem_restrict_##name##_all.key);	\
+		static_key_enable(&proc_mem_restrict_##name##_ptracer.key);	\
+	} else if (strcmp(buf, "off") == 0) {					\
+		static_key_disable(&proc_mem_restrict_##name##_all.key);	\
+		static_key_disable(&proc_mem_restrict_##name##_ptracer.key);	\
+	} else									\
+		pr_warn("%s: ignoring unknown option '%s'\n",			\
+			"proc_mem.restrict_" #name, buf);			\
+	return 0;								\
+}										\
+early_param("proc_mem.restrict_" #name, early_proc_mem_restrict_##name)
+
+DEFINE_EARLY_PROC_MEM_RESTRICT(open_read);
+DEFINE_EARLY_PROC_MEM_RESTRICT(open_write);
+DEFINE_EARLY_PROC_MEM_RESTRICT(write);
+
 /*
  * Count the number of hardlinks for the pid_entry table, excluding the .
  * and .. links.
@@ -828,12 +888,71 @@ static const struct file_operations proc
 };
 
 
-struct mm_struct *proc_mem_open(struct inode *inode, unsigned int mode)
+static void report_mem_rw_reject(const char *action, struct task_struct *task)
 {
-	struct task_struct *task = get_proc_task(inode);
+	pr_warn_ratelimited("Denied %s of /proc/%d/mem (%s) by pid %d (%s)\n",
+			    action, task_pid_nr(task), task->comm,
+			    task_pid_nr(current), current->comm);
+}
+
+static int __mem_open_access_permitted(struct file *file, struct task_struct *task)
+{
+	bool is_ptracer;
+
+	rcu_read_lock();
+	is_ptracer = current == ptrace_parent(task);
+	rcu_read_unlock();
+
+	if (file->f_mode & FMODE_WRITE) {
+		/* Deny if writes are unconditionally disabled via param */
+		if (static_branch_maybe(CONFIG_PROC_MEM_RESTRICT_OPEN_WRITE_DEFAULT,
+					&proc_mem_restrict_open_write_all)) {
+			report_mem_rw_reject("all open-for-write", task);
+			return -EACCES;
+		}
+
+		/* Deny if writes are allowed only for ptracers via param */
+		if (static_branch_maybe(CONFIG_PROC_MEM_RESTRICT_OPEN_WRITE_PTRACE_DEFAULT,
+					&proc_mem_restrict_open_write_ptracer) &&
+		    !is_ptracer) {
+			report_mem_rw_reject("non-ptracer open-for-write", task);
+			return -EACCES;
+		}
+	}
+
+	if (file->f_mode & FMODE_READ) {
+		/* Deny if reads are unconditionally disabled via param */
+		if (static_branch_maybe(CONFIG_PROC_MEM_RESTRICT_OPEN_READ_DEFAULT,
+					&proc_mem_restrict_open_read_all)) {
+			report_mem_rw_reject("all open-for-read", task);
+			return -EACCES;
+		}
+
+		/* Deny if reads are allowed only for ptracers via param */
+		if (static_branch_maybe(CONFIG_PROC_MEM_RESTRICT_OPEN_READ_PTRACE_DEFAULT,
+					&proc_mem_restrict_open_read_ptracer) &&
+		    !is_ptracer) {
+			report_mem_rw_reject("non-ptracer open-for-read", task);
+			return -EACCES;
+		}
+	}
+
+	return 0; /* R/W are not restricted */
+}
+
+struct mm_struct *proc_mem_open(struct file  *file, unsigned int mode)
+{
+	struct task_struct *task = get_proc_task(file_inode(file));
 	struct mm_struct *mm = ERR_PTR(-ESRCH);
+	int ret;
 
 	if (task) {
+		ret = __mem_open_access_permitted(file, task);
+		if (ret) {
+			put_task_struct(task);
+			return ERR_PTR(ret);
+		}
+
 		mm = mm_access(task, mode | PTRACE_MODE_FSCREDS);
 		put_task_struct(task);
 
@@ -850,7 +969,7 @@ struct mm_struct *proc_mem_open(struct i
 
 static int __mem_open(struct inode *inode, struct file *file, unsigned int mode)
 {
-	struct mm_struct *mm = proc_mem_open(inode, mode);
+	struct mm_struct *mm = proc_mem_open(file, mode);
 
 	if (IS_ERR(mm))
 		return PTR_ERR(mm);
@@ -869,6 +988,46 @@ static int mem_open(struct inode *inode,
 	return ret;
 }
 
+static bool __mem_rw_current_is_ptracer(struct file *file)
+{
+	struct inode *inode = file_inode(file);
+	struct task_struct *task = get_proc_task(inode);
+	struct mm_struct *mm = NULL;
+	int is_ptracer = false, has_mm_access = false;
+
+	if (task) {
+		rcu_read_lock();
+		is_ptracer = current == ptrace_parent(task);
+		rcu_read_unlock();
+
+		mm = mm_access(task, PTRACE_MODE_READ_FSCREDS);
+		if (mm && file->private_data == mm) {
+			has_mm_access = true;
+			mmput(mm);
+		}
+
+		put_task_struct(task);
+	}
+
+	return is_ptracer && has_mm_access;
+}
+
+static bool __mem_rw_block_writes(struct file *file)
+{
+	/* Block if writes are disabled via param proc_mem.restrict_write=all */
+	if (static_branch_maybe(CONFIG_PROC_MEM_RESTRICT_WRITE_DEFAULT,
+				&proc_mem_restrict_write_all))
+		return true;
+
+	/* Block with an exception only for ptracers */
+	if (static_branch_maybe(CONFIG_PROC_MEM_RESTRICT_WRITE_PTRACE_DEFAULT,
+				&proc_mem_restrict_write_ptracer) &&
+	    !__mem_rw_current_is_ptracer(file))
+		return true;
+
+	return false;
+}
+
 static bool proc_mem_foll_force(struct file *file, struct mm_struct *mm)
 {
 	struct task_struct *task;
@@ -895,6 +1054,7 @@ static ssize_t mem_rw(struct file *file,
 			size_t count, loff_t *ppos, int write)
 {
 	struct mm_struct *mm = file->private_data;
+	struct task_struct *task = NULL;
 	unsigned long addr = *ppos;
 	ssize_t copied;
 	char *page;
@@ -903,6 +1063,15 @@ static ssize_t mem_rw(struct file *file,
 	if (!mm)
 		return 0;
 
+	if (write && __mem_rw_block_writes(file)) {
+		task = get_proc_task(file->f_inode);
+		if (task) {
+			report_mem_rw_reject("write call", task);
+			put_task_struct(task);
+		}
+		return -EACCES;
+	}
+
 	page = (char *)__get_free_page(GFP_KERNEL);
 	if (!page)
 		return -ENOMEM;
@@ -3338,6 +3507,9 @@ static const struct pid_entry tgid_base_
 	REG("mounts",     S_IRUGO, proc_mounts_operations),
 	REG("mountinfo",  S_IRUGO, proc_mountinfo_operations),
 	REG("mountstats", S_IRUSR, proc_mountstats_operations),
+#ifdef CONFIG_PROCESS_RECLAIM
+	REG("reclaim",    S_IWUGO, proc_reclaim_operations),
+#endif
 #ifdef CONFIG_PROC_PAGE_MONITOR
 	REG("clear_refs", S_IWUSR, proc_clear_refs_operations),
 	REG("smaps",      S_IRUGO, proc_pid_smaps_operations),
@@ -3398,6 +3570,9 @@ static const struct pid_entry tgid_base_
 #ifdef CONFIG_LIVEPATCH
 	ONE("patch_state",  S_IRUSR, proc_pid_patch_state),
 #endif
+#ifdef CONFIG_CPU_FREQ_TIMES
+	ONE("time_in_state", 0444, proc_time_in_state_show),
+#endif
 #ifdef CONFIG_STACKLEAK_METRICS
 	ONE("stack_depth", S_IRUGO, proc_stack_depth),
 #endif
@@ -3750,6 +3925,12 @@ static const struct pid_entry tid_base_s
 	ONE("ksm_merging_pages",  S_IRUSR, proc_pid_ksm_merging_pages),
 	ONE("ksm_stat",  S_IRUSR, proc_pid_ksm_stat),
 #endif
+#ifdef CONFIG_PROC_LATSENSE
+	REG("latency_sensitive",  S_IRUGO|S_IWUGO, proc_tid_latsense_operations),
+#endif
+#ifdef CONFIG_CPU_FREQ_TIMES
+	ONE("time_in_state", 0444, proc_time_in_state_show),
+#endif
 };
 
 static int proc_tid_base_readdir(struct file *file, struct dir_context *ctx)
Index: kernel-rpi-6_6/mm/util.c
===================================================================
--- kernel-rpi-6_6.orig/mm/util.c
+++ kernel-rpi-6_6/mm/util.c
@@ -814,6 +814,7 @@ int sysctl_overcommit_memory __read_most
 int sysctl_overcommit_ratio __read_mostly = 50;
 unsigned long sysctl_overcommit_kbytes __read_mostly;
 int sysctl_max_map_count __read_mostly = DEFAULT_MAX_MAP_COUNT;
+int sysctl_mmap_noexec_taint __read_mostly = CONFIG_MMAP_NOEXEC_TAINT;
 unsigned long sysctl_user_reserve_kbytes __read_mostly = 1UL << 17; /* 128MB */
 unsigned long sysctl_admin_reserve_kbytes __read_mostly = 1UL << 13; /* 8MB */
 
Index: kernel-rpi-6_6/mm/Makefile
===================================================================
--- kernel-rpi-6_6.orig/mm/Makefile
+++ kernel-rpi-6_6/mm/Makefile
@@ -46,6 +46,10 @@ ifdef CONFIG_CROSS_MEMORY_ATTACH
 mmu-$(CONFIG_MMU)	+= process_vm_access.o
 endif
 
+ifdef CONFIG_64BIT
+mmu-$(CONFIG_MMU) += mseal.o
+endif
+
 obj-y			:= filemap.o mempool.o oom_kill.o fadvise.o \
 			   maccess.o page-writeback.o folio-compat.o \
 			   readahead.o swap.o truncate.o vmscan.o shmem.o \
Index: kernel-rpi-6_6/net/core/dev_ioctl.c
===================================================================
--- kernel-rpi-6_6.orig/net/core/dev_ioctl.c
+++ kernel-rpi-6_6/net/core/dev_ioctl.c
@@ -9,6 +9,7 @@
 #include <linux/wireless.h>
 #include <linux/if_bridge.h>
 #include <net/dsa_stubs.h>
+#include <net/sock.h>
 #include <net/wext.h>
 
 #include "dev.h"
@@ -776,7 +777,7 @@ int dev_ioctl(struct net *net, unsigned
 	case SIOCBRADDIF:
 	case SIOCBRDELIF:
 	case SIOCSHWTSTAMP:
-		if (!ns_capable(net->user_ns, CAP_NET_ADMIN))
+		if (!android_ns_capable(net, CAP_NET_ADMIN))
 			return -EPERM;
 		fallthrough;
 	case SIOCBONDSLAVEINFOQUERY:
Index: kernel-rpi-6_6/net/xfrm/xfrm_algo.c
===================================================================
--- kernel-rpi-6_6.orig/net/xfrm/xfrm_algo.c
+++ kernel-rpi-6_6/net/xfrm/xfrm_algo.c
@@ -237,7 +237,7 @@ static struct xfrm_algo_desc aalg_list[]
 
 	.uinfo = {
 		.auth = {
-			.icv_truncbits = 96,
+			.icv_truncbits = IS_ENABLED(CONFIG_ANDROID) ? 128 : 96,
 			.icv_fullbits = 256,
 		}
 	},
Index: kernel-rpi-6_6/net/xfrm/xfrm_sysctl.c
===================================================================
--- kernel-rpi-6_6.orig/net/xfrm/xfrm_sysctl.c
+++ kernel-rpi-6_6/net/xfrm/xfrm_sysctl.c
@@ -58,7 +58,8 @@ int __net_init xfrm_sysctl_init(struct n
 
 	/* Don't export sysctls to unprivileged users */
 	if (net->user_ns != &init_user_ns) {
-		table[0].procname = NULL;
+    table[0] = table[3];
+		table[1].procname = NULL;
 		table_size = 0;
 	}
 
Index: kernel-rpi-6_6/kernel/cgroup/cgroup-v1.c
===================================================================
--- kernel-rpi-6_6.orig/kernel/cgroup/cgroup-v1.c
+++ kernel-rpi-6_6/kernel/cgroup/cgroup-v1.c
@@ -513,7 +513,8 @@ static ssize_t __cgroup1_procs_write(str
 	tcred = get_task_cred(task);
 	if (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&
 	    !uid_eq(cred->euid, tcred->uid) &&
-	    !uid_eq(cred->euid, tcred->suid))
+	    !uid_eq(cred->euid, tcred->suid) &&
+      !ns_capable(tcred->user_ns, CAP_SYS_NICE))
 		ret = -EACCES;
 	put_cred(tcred);
 	if (ret)
Index: kernel-rpi-6_6/mm/page_alloc.c
===================================================================
--- kernel-rpi-6_6.orig/mm/page_alloc.c
+++ kernel-rpi-6_6/mm/page_alloc.c
@@ -204,27 +204,6 @@ EXPORT_SYMBOL(node_states);
 
 gfp_t gfp_allowed_mask __read_mostly = GFP_BOOT_MASK;
 
-#define ALLOC_IN_CMA_THRESHOLD_MAX 16
-#define ALLOC_IN_CMA_THRESHOLD_DEFAULT 12
-
-static unsigned long _alloc_in_cma_threshold __read_mostly
-				= ALLOC_IN_CMA_THRESHOLD_DEFAULT;
-
-static int __init alloc_in_cma_threshold_setup(char *buf)
-{
-	unsigned long res;
-
-	if (kstrtoul(buf, 10, &res) < 0 ||
-	    res > ALLOC_IN_CMA_THRESHOLD_MAX) {
-		pr_err("Bad alloc_cma_threshold value\n");
-		return 0;
-	}
-	_alloc_in_cma_threshold = res;
-	pr_info("Setting alloc_in_cma_threshold to %lu\n", res);
-	return 0;
-}
-early_param("alloc_in_cma_threshold", alloc_in_cma_threshold_setup);
-
 /*
  * A cached value of the page's pageblock's migratetype, used when the page is
  * put on a pcplist. Used to avoid the pageblock migratetype lookup when
@@ -621,7 +600,9 @@ void destroy_large_folio(struct folio *f
 		return;
 	}
 
-	folio_unqueue_deferred_split(folio);
+	if (folio_test_large_rmappable(folio))
+		folio_undo_large_rmappable(folio);
+
 	mem_cgroup_uncharge(folio);
 	free_the_page(&folio->page, folio_order(folio));
 }
@@ -1021,11 +1002,10 @@ static int free_tail_page_prepare(struct
 		}
 		break;
 	case 2:
-		/* the second tail page: deferred_list overlaps ->mapping */
-		if (unlikely(!list_empty(&folio->_deferred_list))) {
-			bad_page(page, "on deferred list");
-			goto out;
-		}
+		/*
+		 * the second tail page: ->mapping is
+		 * deferred_list.next -- ignore value.
+		 */
 		break;
 	default:
 		if (page->mapping != TAIL_MAPPING) {
@@ -1103,27 +1083,12 @@ static __always_inline bool free_pages_p
 	int bad = 0;
 	bool skip_kasan_poison = should_skip_kasan_poison(page, fpi_flags);
 	bool init = want_init_on_free();
-	struct folio *folio = page_folio(page);
 
 	VM_BUG_ON_PAGE(PageTail(page), page);
 
 	trace_mm_page_free(page, order);
 	kmsan_free_page(page, order);
 
-	/*
-	 * In rare cases, when truncation or holepunching raced with
-	 * munlock after VM_LOCKED was cleared, Mlocked may still be
-	 * found set here.  This does not indicate a problem, unless
-	 * "unevictable_pgs_cleared" appears worryingly large.
-	 */
-	if (unlikely(folio_test_mlocked(folio))) {
-		long nr_pages = folio_nr_pages(folio);
-
-		__folio_clear_mlocked(folio);
-		zone_stat_mod_folio(folio, NR_MLOCK, -nr_pages);
-		count_vm_events(UNEVICTABLE_PGCLEARED, nr_pages);
-	}
-
 	if (unlikely(PageHWPoison(page)) && !order) {
 		/*
 		 * Do not let hwpoison pages hit pcplists/buddy
@@ -2130,13 +2095,12 @@ __rmqueue(struct zone *zone, unsigned in
 	if (IS_ENABLED(CONFIG_CMA)) {
 		/*
 		 * Balance movable allocations between regular and CMA areas by
-		 * allocating from CMA when over more than a given proportion of
-		 * the zone's free memory is in the CMA area.
+		 * allocating from CMA when over half of the zone's free memory
+		 * is in the CMA area.
 		 */
 		if (alloc_flags & ALLOC_CMA &&
 		    zone_page_state(zone, NR_FREE_CMA_PAGES) >
-		    zone_page_state(zone, NR_FREE_PAGES) / ALLOC_IN_CMA_THRESHOLD_MAX
-		    * _alloc_in_cma_threshold) {
+		    zone_page_state(zone, NR_FREE_PAGES) / 2) {
 			page = __rmqueue_cma_fallback(zone, order);
 			if (page)
 				return page;
@@ -2694,12 +2658,12 @@ struct page *rmqueue_buddy(struct zone *
 			page = __rmqueue(zone, order, migratetype, alloc_flags);
 
 			/*
-			 * If the allocation fails, allow OOM handling and
-			 * order-0 (atomic) allocs access to HIGHATOMIC
-			 * reserves as failing now is worse than failing a
-			 * high-order atomic allocation in the future.
+			 * If the allocation fails, allow OOM handling access
+			 * to HIGHATOMIC reserves as failing now is worse than
+			 * failing a high-order atomic allocation in the
+			 * future.
 			 */
-			if (!page && (alloc_flags & (ALLOC_OOM|ALLOC_NON_BLOCK)))
+			if (!page && (alloc_flags & ALLOC_OOM))
 				page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
 
 			if (!page) {
@@ -3851,9 +3815,9 @@ should_reclaim_retry(gfp_t gfp_mask, uns
 	else
 		(*no_progress_loops)++;
 
-	if (*no_progress_loops > MAX_RECLAIM_RETRIES)
+	if (*no_progress_loops > MAX_RECLAIM_RETRIES) {
 		goto out;
-
+	}
 
 	/*
 	 * Keep reclaiming pages while there is a chance this will lead
@@ -4338,8 +4302,7 @@ unsigned long __alloc_pages_bulk(gfp_t g
 	gfp = alloc_gfp;
 
 	/* Find an allowed local zone that meets the low watermark. */
-	z = ac.preferred_zoneref;
-	for_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {
+	for_each_zone_zonelist_nodemask(zone, z, ac.zonelist, ac.highest_zoneidx, ac.nodemask) {
 		unsigned long mark;
 
 		if (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&
@@ -4501,8 +4464,12 @@ struct folio *__folio_alloc(gfp_t gfp, u
 		nodemask_t *nodemask)
 {
 	struct page *page = __alloc_pages(gfp | __GFP_COMP, order,
-					preferred_nid, nodemask);
-	return page_rmappable_folio(page);
+			preferred_nid, nodemask);
+	struct folio *folio = (struct folio *)page;
+
+	if (folio && order > 1)
+		folio_prep_large_rmappable(folio);
+	return folio;
 }
 EXPORT_SYMBOL(__folio_alloc);
 
Index: kernel-rpi-6_6/fs/proc/internal.h
===================================================================
--- kernel-rpi-6_6.orig/fs/proc/internal.h
+++ kernel-rpi-6_6/fs/proc/internal.h
@@ -146,6 +146,7 @@ unsigned name_to_int(const struct qstr *
  * array.c
  */
 extern const struct file_operations proc_tid_children_operations;
+extern const struct file_operations proc_tid_latsense_operations;
 
 extern void proc_task_name(struct seq_file *m, struct task_struct *p,
 			   bool escape);
@@ -216,6 +217,7 @@ struct pde_opener {
 extern const struct inode_operations proc_link_inode_operations;
 extern const struct inode_operations proc_pid_link_inode_operations;
 extern const struct super_operations proc_sops;
+extern const struct file_operations proc_reclaim_operations;
 
 void proc_init_kmemcache(void);
 void proc_invalidate_siblings_dcache(struct hlist_head *inodes, spinlock_t *lock);
@@ -295,7 +297,7 @@ struct proc_maps_private {
 #endif
 } __randomize_layout;
 
-struct mm_struct *proc_mem_open(struct inode *inode, unsigned int mode);
+struct mm_struct *proc_mem_open(struct file *file, unsigned int mode);
 
 extern const struct file_operations proc_pid_maps_operations;
 extern const struct file_operations proc_pid_numa_maps_operations;
Index: kernel-rpi-6_6/include/linux/cpufreq_times.h
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/include/linux/cpufreq_times.h
@@ -0,0 +1,42 @@
+/* drivers/cpufreq/cpufreq_times.c
+ *
+ * Copyright (C) 2018 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef _LINUX_CPUFREQ_TIMES_H
+#define _LINUX_CPUFREQ_TIMES_H
+
+#include <linux/cpufreq.h>
+#include <linux/pid.h>
+
+#ifdef CONFIG_CPU_FREQ_TIMES
+void cpufreq_task_times_init(struct task_struct *p);
+void cpufreq_task_times_alloc(struct task_struct *p);
+void cpufreq_task_times_exit(struct task_struct *p);
+int proc_time_in_state_show(struct seq_file *m, struct pid_namespace *ns,
+			    struct pid *pid, struct task_struct *p);
+void cpufreq_acct_update_power(struct task_struct *p, u64 cputime);
+void cpufreq_times_create_policy(struct cpufreq_policy *policy);
+void cpufreq_times_record_transition(struct cpufreq_policy *policy,
+                                     unsigned int new_freq);
+#else
+static inline void cpufreq_task_times_init(struct task_struct *p) {}
+static inline void cpufreq_task_times_alloc(struct task_struct *p) {}
+static inline void cpufreq_task_times_exit(struct task_struct *p) {}
+static inline void cpufreq_acct_update_power(struct task_struct *p,
+					     u64 cputime) {}
+static inline void cpufreq_times_create_policy(struct cpufreq_policy *policy) {}
+static inline void cpufreq_times_record_transition(
+	struct cpufreq_policy *policy, unsigned int new_freq) {}
+#endif /* CONFIG_CPU_FREQ_TIMES */
+#endif /* _LINUX_CPUFREQ_TIMES_H */
Index: kernel-rpi-6_6/fs/proc/latsense.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/fs/proc/latsense.c
@@ -0,0 +1,94 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright 2019 Google, Inc.
+ *
+ * Support for setting tasks as latency sensitive
+ * using /proc/pid/tasks/tid/latency_sensitive interface.
+ */
+
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/sched/task.h>
+#include <linux/sched/latsense.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/fs_struct.h>
+
+#include "internal.h"
+
+/*
+ * Print out latsense related information:
+ */
+static int sched_latsense_show(struct seq_file *m, void *v)
+{
+	struct inode *inode = m->private;
+	struct task_struct *p;
+
+	p = get_proc_task(inode);
+	if (!p)
+		return -ESRCH;
+
+	seq_printf(m, "%d\n", !!proc_sched_get_latency_sensitive(p));
+
+	put_task_struct(p);
+
+	return 0;
+}
+
+static ssize_t
+sched_latsense_write(struct file *file, const char __user *buf,
+	    size_t count, loff_t *offset)
+{
+	struct inode *inode = file_inode(file);
+	struct task_struct *p;
+	char buffer[PROC_NUMBUF];
+	int val;
+	int err;
+
+	memset(buffer, 0, sizeof(buffer));
+	if (count > sizeof(buffer) - 1)
+		count = sizeof(buffer) - 1;
+	if (copy_from_user(buffer, buf, count))
+		return -EFAULT;
+
+	err = kstrtoint(strstrip(buffer), 0, &val);
+	if (err < 0)
+		return err;
+
+	if (val != 0 && val != 1)
+		return -EINVAL;
+
+	p = get_proc_task(inode);
+	if (!p)
+		return -ESRCH;
+
+	err = proc_sched_set_latency_sensitive(p, val);
+	if (err)
+		count = err;
+
+	put_task_struct(p);
+
+	return count;
+}
+
+static int sched_latsense_open(struct inode *inode, struct file *filp)
+{
+	int ret;
+
+	ret = single_open(filp, sched_latsense_show, NULL);
+	if (!ret) {
+		struct seq_file *m = filp->private_data;
+
+		m->private = inode;
+	}
+	return ret;
+}
+
+const struct file_operations proc_tid_latsense_operations = {
+	.open		= sched_latsense_open,
+	.read		= seq_read,
+	.write		= sched_latsense_write,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
Index: kernel-rpi-6_6/fs/proc/Makefile
===================================================================
--- kernel-rpi-6_6.orig/fs/proc/Makefile
+++ kernel-rpi-6_6/fs/proc/Makefile
@@ -27,6 +27,7 @@ proc-y	+= softirqs.o
 proc-y	+= namespaces.o
 proc-y	+= self.o
 proc-y	+= thread_self.o
+proc-$(CONFIG_PROC_LATSENSE)  += latsense.o
 proc-$(CONFIG_PROC_SYSCTL)	+= proc_sysctl.o
 proc-$(CONFIG_NET)		+= proc_net.o
 proc-$(CONFIG_PROC_KCORE)	+= kcore.o
Index: kernel-rpi-6_6/fs/proc/Kconfig
===================================================================
--- kernel-rpi-6_6.orig/fs/proc/Kconfig
+++ kernel-rpi-6_6/fs/proc/Kconfig
@@ -108,3 +108,9 @@ config PROC_PID_ARCH_STATUS
 config PROC_CPU_RESCTRL
 	def_bool n
 	depends on PROC_FS
+
+config PROC_LATSENSE
+	def_bool y
+	depends on PROC_FS && UCLAMP_TASK
+	help
+	Enable /proc/pid/tasks/tid latency sensitive scheduler attribute
Index: kernel-rpi-6_6/include/linux/sched/latsense.h
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/include/linux/sched/latsense.h
@@ -0,0 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _LINUX_SCHED_LATSENSE_H
+#define _LINUX_SCHED_LATSENSE_H
+
+extern int proc_sched_set_latency_sensitive(struct task_struct *p, int val);
+extern int proc_sched_get_latency_sensitive(struct task_struct *p);
+
+#endif /* _LINUX_SCHED_LATSENSE_H */
Index: kernel-rpi-6_6/include/linux/sched.h
===================================================================
--- kernel-rpi-6_6.orig/include/linux/sched.h
+++ kernel-rpi-6_6/include/linux/sched.h
@@ -824,6 +824,10 @@ struct task_struct {
 
 	struct sched_statistics         stats;
 
+#ifdef CONFIG_PROC_LATSENSE
+  int proc_latency_sensitive;
+#endif
+
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	/* List of struct preempt_notifier: */
 	struct hlist_head		preempt_notifiers;
@@ -1029,6 +1033,10 @@ struct task_struct {
 	u64				stimescaled;
 #endif
 	u64				gtime;
+#ifdef CONFIG_CPU_FREQ_TIMES
+  u64       *time_in_state;
+  unsigned int        max_state;
+#endif
 	struct prev_cputime		prev_cputime;
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 	struct vtime			vtime;
@@ -1861,7 +1869,7 @@ current_restore_flags(unsigned long orig
 }
 
 extern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);
-extern int task_can_attach(struct task_struct *p);
+extern int task_can_attach(struct task_struct *p, const struct cpumask *cs_effective_cpus);
 extern int dl_bw_alloc(int cpu, u64 dl_bw);
 extern void dl_bw_free(int cpu, u64 dl_bw);
 #ifdef CONFIG_SMP
Index: kernel-rpi-6_6/kernel/cgroup/cpuset.c
===================================================================
--- kernel-rpi-6_6.orig/kernel/cgroup/cpuset.c
+++ kernel-rpi-6_6/kernel/cgroup/cpuset.c
@@ -2559,7 +2559,7 @@ static int cpuset_can_attach(struct cgro
 	mems_updated = !nodes_equal(cs->effective_mems, oldcs->effective_mems);
 
 	cgroup_taskset_for_each(task, css, tset) {
-		ret = task_can_attach(task);
+		ret = task_can_attach(task, cs->effective_cpus);
 		if (ret)
 			goto out_unlock;
 
@@ -3424,7 +3424,7 @@ static int cpuset_can_fork(struct task_s
 	if (ret)
 		goto out_unlock;
 
-	ret = task_can_attach(task);
+	ret = task_can_attach(task, cs->effective_cpus);
 	if (ret)
 		goto out_unlock;
 
Index: kernel-rpi-6_6/include/linux/rmap.h
===================================================================
--- kernel-rpi-6_6.orig/include/linux/rmap.h
+++ kernel-rpi-6_6/include/linux/rmap.h
@@ -14,6 +14,10 @@
 #include <linux/pagemap.h>
 #include <linux/memremap.h>
 
+extern bool isolate_lru_page(struct page *page);
+extern void putback_lru_page(struct page *page);
+extern unsigned long reclaim_pages(struct list_head *page_list);
+
 /*
  * The anon_vma heads a list of private "related" vmas, to scan if
  * an anonymous page pointing to this anon_vma needs to be unmapped:
Index: kernel-rpi-6_6/kernel/sched/latsense.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/kernel/sched/latsense.c
@@ -0,0 +1,17 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "sched.h"
+
+int proc_sched_set_latency_sensitive(struct task_struct *p, int val)
+{
+	if (val != 0 && val != 1)
+		return -EINVAL;
+
+	p->proc_latency_sensitive = val;
+
+	return 0;
+}
+
+int proc_sched_get_latency_sensitive(struct task_struct *p)
+{
+	return p->proc_latency_sensitive;
+}
Index: kernel-rpi-6_6/kernel/sched/build_utility.c
===================================================================
--- kernel-rpi-6_6.orig/kernel/sched/build_utility.c
+++ kernel-rpi-6_6/kernel/sched/build_utility.c
@@ -108,3 +108,7 @@
 #ifdef CONFIG_SCHED_AUTOGROUP
 # include "autogroup.c"
 #endif
+
+#ifdef CONFIG_PROC_LATSENSE
+# include "latsense.c"
+#endif
Index: kernel-rpi-6_6/fs/nfs/nfs4proc.c
===================================================================
--- kernel-rpi-6_6.orig/fs/nfs/nfs4proc.c
+++ kernel-rpi-6_6/fs/nfs/nfs4proc.c
@@ -7765,7 +7765,7 @@ static int nfs4_xattr_set_nfs4_dacl(cons
 
 static int nfs4_xattr_get_nfs4_dacl(const struct xattr_handler *handler,
 				    struct dentry *unused, struct inode *inode,
-				    const char *key, void *buf, size_t buflen)
+				    const char *key, void *buf, size_t buflen, int flags)
 {
 	return nfs4_proc_get_acl(inode, buf, buflen, NFS4ACL_DACL);
 }
@@ -7788,7 +7788,7 @@ static int nfs4_xattr_set_nfs4_sacl(cons
 
 static int nfs4_xattr_get_nfs4_sacl(const struct xattr_handler *handler,
 				    struct dentry *unused, struct inode *inode,
-				    const char *key, void *buf, size_t buflen)
+				    const char *key, void *buf, size_t buflen, int flags)
 {
 	return nfs4_proc_get_acl(inode, buf, buflen, NFS4ACL_SACL);
 }
@@ -7816,7 +7816,7 @@ static int nfs4_xattr_set_nfs4_label(con
 
 static int nfs4_xattr_get_nfs4_label(const struct xattr_handler *handler,
 				     struct dentry *unused, struct inode *inode,
-				     const char *key, void *buf, size_t buflen)
+				     const char *key, void *buf, size_t buflen, int flags)
 {
 	if (security_ismaclabel(key))
 		return nfs4_get_security_label(inode, buf, buflen);
Index: kernel-rpi-6_6/mm/mseal.c
===================================================================
--- /dev/null
+++ kernel-rpi-6_6/mm/mseal.c
@@ -0,0 +1,312 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *  Implement mseal() syscall.
+ *
+ *  Copyright (c) 2023,2024 Google, Inc.
+ *
+ *  Author: Jeff Xu <jeffxu@chromium.org>
+ */
+
+#include <linux/mempolicy.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/mm_inline.h>
+#include <linux/mmu_context.h>
+#include <linux/syscalls.h>
+#include <linux/sched.h>
+#include "internal.h"
+
+static inline void set_vma_sealed(struct vm_area_struct *vma)
+{
+	vm_flags_set(vma, VM_SEALED);
+}
+
+static bool is_madv_discard(int behavior)
+{
+	switch (behavior) {
+	case MADV_FREE:
+	case MADV_DONTNEED:
+	case MADV_DONTNEED_LOCKED:
+	case MADV_REMOVE:
+	case MADV_DONTFORK:
+	case MADV_WIPEONFORK:
+		return true;
+	}
+
+	return false;
+}
+
+static bool is_ro_anon(struct vm_area_struct *vma)
+{
+	/* check anonymous mapping. */
+	if (vma->vm_file || vma->vm_flags & VM_SHARED)
+		return false;
+
+	/*
+	 * check for non-writable:
+	 * PROT=RO or PKRU is not writeable.
+	 */
+	if (!(vma->vm_flags & VM_WRITE) ||
+		!arch_vma_access_permitted(vma, true, false, false))
+		return true;
+
+	return false;
+}
+
+/*
+ * Check if the vmas of a memory range are allowed to be modified.
+ * the memory ranger can have a gap (unallocated memory).
+ * return true, if it is allowed.
+ */
+bool can_modify_mm(struct mm_struct *mm, unsigned long start, unsigned long end)
+{
+	struct vm_area_struct *vma;
+
+	VMA_ITERATOR(vmi, mm, start);
+
+	/* going through each vma to check. */
+	for_each_vma_range(vmi, vma, end) {
+		if (unlikely(!can_modify_vma(vma)))
+			return false;
+	}
+
+	/* Allow by default. */
+	return true;
+}
+
+/*
+ * Check if a vma is allowed to be modified by madvise.
+ */
+bool can_modify_vma_madv(struct vm_area_struct *vma, int behavior)
+{
+	if (!is_madv_discard(behavior))
+		return true;
+
+	if (unlikely(!can_modify_vma(vma) && is_ro_anon(vma)))
+		return false;
+
+	/* Allow by default. */
+	return true;
+}
+
+/*
+ * mseal_fixup is almost same as mlock_fixup
+ */
+static int mseal_fixup(struct vma_iterator *vmi, struct vm_area_struct *vma,
+		struct vm_area_struct **prev, unsigned long start,
+		unsigned long end, vm_flags_t newflags)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	pgoff_t pgoff;
+	int ret = 0;
+	vm_flags_t oldflags = vma->vm_flags;
+
+	if (newflags == oldflags)
+		goto out;
+
+	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
+	*prev = vma_merge(vmi, mm, *prev, start, end, newflags,
+			vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
+			vma->vm_userfaultfd_ctx, anon_vma_name(vma));
+	if (*prev) {
+		vma = *prev;
+		goto success;
+	}
+
+	if (start != vma->vm_start) {
+		ret = split_vma(vmi, vma, start, 1);
+		if (ret)
+			goto out;
+	}
+
+	if (end != vma->vm_end) {
+		ret = split_vma(vmi, vma, end, 0);
+		if (ret)
+			goto out;
+	}
+
+success:
+	set_vma_sealed(vma);
+
+out:
+	*prev = vma;
+	return ret;
+}
+
+
+/*
+ * Check for do_mseal:
+ * 1> start is part of a valid vma.
+ * 2> end is part of a valid vma.
+ * 3> No gap (unallocated address) between start and end.
+ * 4> map is sealable.
+ */
+static int check_mm_seal(unsigned long start, unsigned long end)
+{
+	struct vm_area_struct *vma;
+	unsigned long nstart = start;
+
+	VMA_ITERATOR(vmi, current->mm, start);
+
+	/* going through each vma to check. */
+	for_each_vma_range(vmi, vma, end) {
+		if (vma->vm_start > nstart)
+			/* unallocated memory found. */
+			return -ENOMEM;
+
+		if (vma->vm_end >= end)
+			return 0;
+
+		nstart = vma->vm_end;
+	}
+
+	return -ENOMEM;
+}
+
+/*
+ * Apply sealing.
+ */
+static int apply_mm_seal(unsigned long start, unsigned long end)
+{
+	unsigned long nstart;
+	struct vm_area_struct *vma, *prev;
+
+	VMA_ITERATOR(vmi, current->mm, start);
+
+	vma = vma_iter_load(&vmi);
+	/*
+	 * Note: check_mm_seal should already checked ENOMEM case.
+	 * so vma should not be null, same for the other ENOMEM cases.
+	 */
+	prev = vma_prev(&vmi);
+	if (start > vma->vm_start)
+		prev = vma;
+
+	nstart = start;
+	for_each_vma_range(vmi, vma, end) {
+		int error;
+		unsigned long tmp;
+		vm_flags_t newflags;
+
+		newflags = vma->vm_flags | VM_SEALED;
+		tmp = vma->vm_end;
+		if (tmp > end)
+			tmp = end;
+		error = mseal_fixup(&vmi, vma, &prev, nstart, tmp, newflags);
+		if (error)
+			return error;
+		nstart = vma_iter_end(&vmi);
+	}
+
+	return 0;
+}
+
+/*
+ * mseal(2) seals the VM's meta data from
+ * selected syscalls.
+ *
+ * addr/len: VM address range.
+ *
+ *  The address range by addr/len must meet:
+ *   start (addr) must be in a valid VMA.
+ *   end (addr + len) must be in a valid VMA.
+ *   no gap (unallocated memory) between start and end.
+ *   start (addr) must be page aligned.
+ *
+ *  len: len will be page aligned implicitly.
+ *
+ *   Below VMA operations are blocked after sealing.
+ *   1> Unmapping, moving to another location, and shrinking
+ *	the size, via munmap() and mremap(), can leave an empty
+ *	space, therefore can be replaced with a VMA with a new
+ *	set of attributes.
+ *   2> Moving or expanding a different vma into the current location,
+ *	via mremap().
+ *   3> Modifying a VMA via mmap(MAP_FIXED).
+ *   4> Size expansion, via mremap(), does not appear to pose any
+ *	specific risks to sealed VMAs. It is included anyway because
+ *	the use case is unclear. In any case, users can rely on
+ *	merging to expand a sealed VMA.
+ *   5> mprotect and pkey_mprotect.
+ *   6> Some destructive madvice() behavior (e.g. MADV_DONTNEED)
+ *      for anonymous memory, when users don't have write permission to the
+ *	memory. Those behaviors can alter region contents by discarding pages,
+ *	effectively a memset(0) for anonymous memory.
+ *
+ *  flags: reserved.
+ *
+ * return values:
+ *  zero: success.
+ *  -EINVAL:
+ *   invalid input flags.
+ *   start address is not page aligned.
+ *   Address arange (start + len) overflow.
+ *  -ENOMEM:
+ *   addr is not a valid address (not allocated).
+ *   end (start + len) is not a valid address.
+ *   a gap (unallocated memory) between start and end.
+ *  -EPERM:
+ *  - In 32 bit architecture, sealing is not supported.
+ * Note:
+ *  user can call mseal(2) multiple times, adding a seal on an
+ *  already sealed memory is a no-action (no error).
+ *
+ *  unseal() is not supported.
+ */
+static int do_mseal(unsigned long start, size_t len_in, unsigned long flags)
+{
+	size_t len;
+	int ret = 0;
+	unsigned long end;
+	struct mm_struct *mm = current->mm;
+
+	ret = can_do_mseal(flags);
+	if (ret)
+		return ret;
+
+	start = untagged_addr(start);
+	if (!PAGE_ALIGNED(start))
+		return -EINVAL;
+
+	len = PAGE_ALIGN(len_in);
+	/* Check to see whether len was rounded up from small -ve to zero. */
+	if (len_in && !len)
+		return -EINVAL;
+
+	end = start + len;
+	if (end < start)
+		return -EINVAL;
+
+	if (end == start)
+		return 0;
+
+	if (mmap_write_lock_killable(mm))
+		return -EINTR;
+
+	/*
+	 * First pass, this helps to avoid
+	 * partial sealing in case of error in input address range,
+	 * e.g. ENOMEM error.
+	 */
+	ret = check_mm_seal(start, end);
+	if (ret)
+		goto out;
+
+	/*
+	 * Second pass, this should success, unless there are errors
+	 * from vma_modify_flags, e.g. merge/split error, or process
+	 * reaching the max supported VMAs, however, those cases shall
+	 * be rare.
+	 */
+	ret = apply_mm_seal(start, end);
+
+out:
+	mmap_write_unlock(current->mm);
+	return ret;
+}
+
+SYSCALL_DEFINE3(mseal, unsigned long, start, size_t, len, unsigned long,
+		flags)
+{
+	return do_mseal(start, len, flags);
+}
Index: kernel-rpi-6_6/mm/internal.h
===================================================================
--- kernel-rpi-6_6.orig/mm/internal.h
+++ kernel-rpi-6_6/mm/internal.h
@@ -40,7 +40,7 @@ struct folio_batch;
  * when we specify __GFP_NOWARN.
  */
 #define WARN_ON_ONCE_GFP(cond, gfp)	({				\
-	static bool __section(".data..once") __warned;			\
+	static bool __section(".data.once") __warned;			\
 	int __ret_warn_once = !!(cond);					\
 									\
 	if (unlikely(!(gfp & __GFP_NOWARN) && __ret_warn_once && !__warned)) { \
@@ -83,51 +83,6 @@ static inline void *folio_raw_mapping(st
 	return (void *)(mapping & ~PAGE_MAPPING_FLAGS);
 }
 
-/*
- * This is a file-backed mapping, and is about to be memory mapped - invoke its
- * mmap hook and safely handle error conditions. On error, VMA hooks will be
- * mutated.
- *
- * @file: File which backs the mapping.
- * @vma:  VMA which we are mapping.
- *
- * Returns: 0 if success, error otherwise.
- */
-static inline int mmap_file(struct file *file, struct vm_area_struct *vma)
-{
-	int err = call_mmap(file, vma);
-
-	if (likely(!err))
-		return 0;
-
-	/*
-	 * OK, we tried to call the file hook for mmap(), but an error
-	 * arose. The mapping is in an inconsistent state and we most not invoke
-	 * any further hooks on it.
-	 */
-	vma->vm_ops = &vma_dummy_vm_ops;
-
-	return err;
-}
-
-/*
- * If the VMA has a close hook then close it, and since closing it might leave
- * it in an inconsistent state which makes the use of any hooks suspect, clear
- * them down by installing dummy empty hooks.
- */
-static inline void vma_close(struct vm_area_struct *vma)
-{
-	if (vma->vm_ops && vma->vm_ops->close) {
-		vma->vm_ops->close(vma);
-
-		/*
-		 * The mapping is in an inconsistent state, and no further hooks
-		 * may be invoked upon it.
-		 */
-		vma->vm_ops = &vma_dummy_vm_ops;
-	}
-}
-
 void __acct_reclaim_writeback(pg_data_t *pgdat, struct folio *folio,
 						int nr_throttled);
 static inline void acct_reclaim_writeback(struct folio *folio)
@@ -153,6 +108,7 @@ void folio_rotate_reclaimable(struct fol
 bool __folio_end_writeback(struct folio *folio);
 void deactivate_file_folio(struct folio *folio);
 void folio_activate(struct folio *folio);
+void folio_promote(struct folio *folio);
 
 void free_pgtables(struct mmu_gather *tlb, struct ma_state *mas,
 		   struct vm_area_struct *start_vma, unsigned long floor,
@@ -458,30 +414,7 @@ static inline void folio_set_order(struc
 #endif
 }
 
-bool __folio_unqueue_deferred_split(struct folio *folio);
-static inline bool folio_unqueue_deferred_split(struct folio *folio)
-{
-	if (folio_order(folio) <= 1 || !folio_test_large_rmappable(folio))
-		return false;
-
-	/*
-	 * At this point, there is no one trying to add the folio to
-	 * deferred_list. If folio is not in deferred_list, it's safe
-	 * to check without acquiring the split_queue_lock.
-	 */
-	if (data_race(list_empty(&folio->_deferred_list)))
-		return false;
-
-	return __folio_unqueue_deferred_split(folio);
-}
-
-static inline struct folio *page_rmappable_folio(struct page *page)
-{
-	struct folio *folio = (struct folio *)page;
-
-	folio_prep_large_rmappable(folio);
-	return folio;
-}
+void folio_undo_large_rmappable(struct folio *folio);
 
 static inline void prep_compound_head(struct page *page, unsigned int order)
 {
@@ -491,8 +424,6 @@ static inline void prep_compound_head(st
 	atomic_set(&folio->_entire_mapcount, -1);
 	atomic_set(&folio->_nr_pages_mapped, 0);
 	atomic_set(&folio->_pincount, 0);
-	if (order > 1)
-		INIT_LIST_HEAD(&folio->_deferred_list);
 }
 
 static inline void prep_compound_tail(struct page *head, int tail_idx)
@@ -1229,4 +1160,67 @@ struct vma_prepare {
 	struct vm_area_struct *remove;
 	struct vm_area_struct *remove2;
 };
+
+#ifdef CONFIG_64BIT
+static inline int can_do_mseal(unsigned long flags)
+{
+	if (flags)
+		return -EINVAL;
+
+	return 0;
+}
+
+bool can_modify_mm(struct mm_struct *mm, unsigned long start,
+		unsigned long end);
+bool can_modify_mm_madv(struct mm_struct *mm, unsigned long start,
+		unsigned long end, int behavior);
+
+static inline bool vma_is_sealed(struct vm_area_struct *vma)
+{
+	return (vma->vm_flags & VM_SEALED);
+}
+
+/*
+ * check if a vma is sealed for modification.
+ * return true, if modification is allowed.
+ */
+static inline bool can_modify_vma(struct vm_area_struct *vma)
+{
+	if (unlikely(vma_is_sealed(vma)))
+		return false;
+
+	return true;
+}
+
+bool can_modify_vma_madv(struct vm_area_struct *vma, int behavior);
+
+#else
+static inline int can_do_mseal(unsigned long flags)
+{
+	return -EPERM;
+}
+
+static inline bool can_modify_mm(struct mm_struct *mm, unsigned long start,
+		unsigned long end)
+{
+	return true;
+}
+
+static inline bool can_modify_mm_madv(struct mm_struct *mm, unsigned long start,
+		unsigned long end, int behavior)
+{
+	return true;
+}
+
+static inline bool can_modify_vma(struct vm_area_struct *vma)
+{
+	return true;
+}
+
+static inline bool can_modify_vma_madv(struct vm_area_struct *vma, int behavior)
+{
+	return true;
+}
+
+#endif
 #endif	/* __MM_INTERNAL_H */
Index: kernel-rpi-6_6/include/linux/mm_inline.h
===================================================================
--- kernel-rpi-6_6.orig/include/linux/mm_inline.h
+++ kernel-rpi-6_6/include/linux/mm_inline.h
@@ -108,6 +108,14 @@ static inline bool lru_gen_enabled(void)
 
 	return static_branch_likely(&lru_gen_caps[LRU_GEN_CORE]);
 }
+
+static inline bool lru_gen_aggressive_mm(void)
+{
+	DECLARE_STATIC_KEY_TRUE(lru_gen_caps[NR_LRU_GEN_CAPS]);
+
+	return static_branch_likely(&lru_gen_caps[LRU_GEN_AGGRESSIVE_MM]);
+}
+
 #else
 static inline bool lru_gen_enabled(void)
 {
@@ -115,6 +123,13 @@ static inline bool lru_gen_enabled(void)
 
 	return static_branch_unlikely(&lru_gen_caps[LRU_GEN_CORE]);
 }
+
+static inline bool lru_gen_aggressive_mm(void)
+{
+	DECLARE_STATIC_KEY_FALSE(lru_gen_caps[NR_LRU_GEN_CAPS]);
+
+	return static_branch_likely(&lru_gen_caps[LRU_GEN_AGGRESSIVE_MM]);
+}
 #endif
 
 static inline bool lru_gen_in_fault(void)
@@ -154,16 +169,28 @@ static inline int folio_lru_refs(struct
 	return ((flags & LRU_REFS_MASK) >> LRU_REFS_PGOFF) + workingset;
 }
 
+static inline int lru_raw_gen_from_flags(unsigned long flags)
+{
+	return ((flags & LRU_GEN_MASK) >> LRU_GEN_PGOFF) - 1;
+}
+
+#define ISOLATED_FOLIO_MIN MAX_NR_GENS
+#define ISOLATED_FOLIO_MAX (MAX_NR_GENS + 2)
+
 static inline int folio_lru_gen(struct folio *folio)
 {
-	unsigned long flags = READ_ONCE(folio->flags);
+	int raw_gen = lru_raw_gen_from_flags(READ_ONCE(folio->flags));
 
-	return ((flags & LRU_GEN_MASK) >> LRU_GEN_PGOFF) - 1;
+	BUILD_BUG_ON(order_base_2(ISOLATED_FOLIO_MAX + 1) != LRU_GEN_WIDTH);
+
+	if (raw_gen >= ISOLATED_FOLIO_MIN)
+		return -1;
+	return raw_gen;
 }
 
-static inline bool lru_gen_is_active(struct lruvec *lruvec, int gen)
+static inline bool lru_gen_is_active(struct lruvec *lruvec, int gen, int type)
 {
-	unsigned long max_seq = lruvec->lrugen.max_seq;
+	unsigned long max_seq = lruvec->lrugen.max_seq[type];
 
 	VM_WARN_ON_ONCE(gen >= MAX_NR_GENS);
 
@@ -193,7 +220,7 @@ static inline void lru_gen_update_size(s
 
 	/* addition */
 	if (old_gen < 0) {
-		if (lru_gen_is_active(lruvec, new_gen))
+		if (lru_gen_is_active(lruvec, new_gen, type))
 			lru += LRU_ACTIVE;
 		__update_lru_size(lruvec, lru, zone, delta);
 		return;
@@ -201,20 +228,21 @@ static inline void lru_gen_update_size(s
 
 	/* deletion */
 	if (new_gen < 0) {
-		if (lru_gen_is_active(lruvec, old_gen))
+		if (lru_gen_is_active(lruvec, old_gen, type))
 			lru += LRU_ACTIVE;
 		__update_lru_size(lruvec, lru, zone, -delta);
 		return;
 	}
 
 	/* promotion */
-	if (!lru_gen_is_active(lruvec, old_gen) && lru_gen_is_active(lruvec, new_gen)) {
+	if (!lru_gen_is_active(lruvec, old_gen, type) && lru_gen_is_active(lruvec, new_gen, type)) {
 		__update_lru_size(lruvec, lru, zone, -delta);
 		__update_lru_size(lruvec, lru + LRU_ACTIVE, zone, delta);
 	}
 
 	/* demotion requires isolation, e.g., lru_deactivate_fn() */
-	VM_WARN_ON_ONCE(lru_gen_is_active(lruvec, old_gen) && !lru_gen_is_active(lruvec, new_gen));
+	VM_WARN_ON_ONCE(lru_gen_is_active(lruvec, old_gen, type) &&
+			!lru_gen_is_active(lruvec, new_gen, type));
 }
 
 static inline bool lru_gen_add_folio(struct lruvec *lruvec, struct folio *folio, bool reclaiming)
@@ -243,12 +271,13 @@ static inline bool lru_gen_add_folio(str
 	 *    oldest generation otherwise. See lru_gen_is_active().
 	 */
 	if (folio_test_active(folio))
-		seq = lrugen->max_seq;
-	else if ((type == LRU_GEN_ANON && !folio_test_swapcache(folio)) ||
+		seq = lrugen->max_seq[type];
+	else if ((lru_gen_aggressive_mm() &&
+		  type == LRU_GEN_ANON && !folio_test_swapcache(folio)) ||
 		 (folio_test_reclaim(folio) &&
 		  (folio_test_dirty(folio) || folio_test_writeback(folio))))
-		seq = lrugen->max_seq - 1;
-	else if (reclaiming || lrugen->min_seq[type] + MIN_NR_GENS >= lrugen->max_seq)
+		seq = lrugen->max_seq[type] - 1;
+	else if (reclaiming || lrugen->min_seq[type] + MIN_NR_GENS >= lrugen->max_seq[type])
 		seq = lrugen->min_seq[type];
 	else
 		seq = lrugen->min_seq[type] + 1;
@@ -271,6 +300,7 @@ static inline bool lru_gen_add_folio(str
 static inline bool lru_gen_del_folio(struct lruvec *lruvec, struct folio *folio, bool reclaiming)
 {
 	unsigned long flags;
+	int type = folio_is_file_lru(folio);
 	int gen = folio_lru_gen(folio);
 
 	if (gen < 0)
@@ -280,7 +310,8 @@ static inline bool lru_gen_del_folio(str
 	VM_WARN_ON_ONCE_FOLIO(folio_test_unevictable(folio), folio);
 
 	/* for folio_migrate_flags() */
-	flags = !reclaiming && lru_gen_is_active(lruvec, gen) ? BIT(PG_active) : 0;
+	flags = !reclaiming && lru_gen_is_active(lruvec, gen, type) ? BIT(PG_active) : 0;
+	flags |= (ISOLATED_FOLIO_MIN + 1UL) << LRU_GEN_PGOFF;
 	flags = set_mask_bits(&folio->flags, LRU_GEN_MASK, flags);
 	gen = ((flags & LRU_GEN_MASK) >> LRU_GEN_PGOFF) - 1;
 
@@ -297,6 +328,11 @@ static inline bool lru_gen_enabled(void)
 	return false;
 }
 
+static inline bool lru_gen_aggressive_mm(void)
+{
+	return false;
+}
+
 static inline bool lru_gen_in_fault(void)
 {
 	return false;
@@ -312,6 +348,13 @@ static inline bool lru_gen_del_folio(str
 	return false;
 }
 
+static inline int lru_raw_gen_from_flags(unsigned long flags)
+{
+	return 0;
+}
+
+#define ISOLATED_FOLIO_MIN 0
+
 #endif /* CONFIG_LRU_GEN */
 
 static __always_inline
Index: kernel-rpi-6_6/include/linux/mmzone.h
===================================================================
--- kernel-rpi-6_6.orig/include/linux/mmzone.h
+++ kernel-rpi-6_6/include/linux/mmzone.h
@@ -389,6 +389,9 @@ enum {
 	LRU_GEN_CORE,
 	LRU_GEN_MM_WALK,
 	LRU_GEN_NONLEAF_YOUNG,
+	// Skip 3 to present ABI consistent with 5.15
+	LRU_GEN_ADVANCE_IN_LOCKSTEP = 4,
+	LRU_GEN_AGGRESSIVE_MM,
 	NR_LRU_GEN_CAPS
 };
 
@@ -403,25 +406,19 @@ enum {
 #endif
 
 /*
- * The youngest generation number is stored in max_seq for both anon and file
- * types as they are aged on an equal footing. The oldest generation numbers are
- * stored in min_seq[] separately for anon and file types as clean file pages
- * can be evicted regardless of swap constraints.
- *
- * Normally anon and file min_seq are in sync. But if swapping is constrained,
- * e.g., out of swap space, file min_seq is allowed to advance and leave anon
- * min_seq behind.
+ * The youngest generation numbers are stored in max_seq[], and the oldest
+ * generation numbers are stored in min_seq[].
  *
  * The number of pages in each generation is eventually consistent and therefore
  * can be transiently negative when reset_batch_size() is pending.
  */
 struct lru_gen_folio {
 	/* the aging increments the youngest generation number */
-	unsigned long max_seq;
+	unsigned long max_seq[ANON_AND_FILE];
 	/* the eviction increments the oldest generation numbers */
 	unsigned long min_seq[ANON_AND_FILE];
 	/* the birth time of each generation in jiffies */
-	unsigned long timestamps[MAX_NR_GENS];
+	unsigned long timestamps[MAX_NR_GENS][ANON_AND_FILE];
 	/* the multi-gen LRU lists, lazily sorted on eviction */
 	struct list_head folios[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES];
 	/* the multi-gen LRU sizes, eventually consistent */
@@ -432,9 +429,26 @@ struct lru_gen_folio {
 	unsigned long avg_total[ANON_AND_FILE][MAX_NR_TIERS];
 	/* the first tier doesn't need protection, hence the minus one */
 	unsigned long protected[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS - 1];
+	/*
+	 * When min_seq is incremented, records the min_seq of the opposite
+	 * type. Used for tracking refaulted_victims.
+	 */
+	unsigned long victim_seq[NR_HIST_GENS][ANON_AND_FILE];
+	/* the approximate size of the new oldest generation when min_seq is incremented */
+	unsigned long oldest_gen_size[NR_HIST_GENS][ANON_AND_FILE];
+
 	/* can be modified without holding the LRU lock */
 	atomic_long_t evicted[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS];
 	atomic_long_t refaulted[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS];
+	/*
+	 * A refaulted victim of a given page is a page that was evicted
+	 * to save the given page, but which later gets refaulted. This
+	 * indicates an incorrect decision by the eviction logic. For the
+	 * oldest generation of a given type, refautled_victims estimate
+	 * the number of refaulted victims of the opposite type.
+	 */
+	atomic_long_t refaulted_victims[NR_HIST_GENS][ANON_AND_FILE];
+
 	/* whether the multi-gen LRU is enabled */
 	bool enabled;
 #ifdef CONFIG_MEMCG
@@ -461,8 +475,8 @@ enum {
 #define NR_BLOOM_FILTERS	2
 
 struct lru_gen_mm_state {
-	/* set to max_seq after each iteration */
-	unsigned long seq;
+	/* the current scan's seqno, incremented after each iteration */
+	unsigned long scan_seq;
 	/* where the current iteration continues after */
 	struct list_head *head;
 	/* where the last iteration ended before */
@@ -477,7 +491,9 @@ struct lru_gen_mm_walk {
 	/* the lruvec under reclaim */
 	struct lruvec *lruvec;
 	/* unstable max_seq from lru_gen_folio */
-	unsigned long max_seq;
+	unsigned long max_seq[ANON_AND_FILE];
+	/* scan_seq of the scan this walk is part of */
+	unsigned long scan_seq;
 	/* the next address within an mm to scan */
 	unsigned long next_addr;
 	/* to batch promoted pages */
Index: kernel-rpi-6_6/include/linux/mman.h
===================================================================
--- kernel-rpi-6_6.orig/include/linux/mman.h
+++ kernel-rpi-6_6/include/linux/mman.h
@@ -2,7 +2,6 @@
 #ifndef _LINUX_MMAN_H
 #define _LINUX_MMAN_H
 
-#include <linux/fs.h>
 #include <linux/mm.h>
 #include <linux/percpu_counter.h>
 
@@ -95,7 +94,7 @@ static inline void vm_unacct_memory(long
 #endif
 
 #ifndef arch_calc_vm_flag_bits
-#define arch_calc_vm_flag_bits(file, flags) 0
+#define arch_calc_vm_flag_bits(flags) 0
 #endif
 
 #ifndef arch_validate_prot
@@ -152,12 +151,12 @@ calc_vm_prot_bits(unsigned long prot, un
  * Combine the mmap "flags" argument into "vm_flags" used internally.
  */
 static inline unsigned long
-calc_vm_flag_bits(struct file *file, unsigned long flags)
+calc_vm_flag_bits(unsigned long flags)
 {
 	return _calc_vm_trans(flags, MAP_GROWSDOWN,  VM_GROWSDOWN ) |
 	       _calc_vm_trans(flags, MAP_LOCKED,     VM_LOCKED    ) |
 	       _calc_vm_trans(flags, MAP_SYNC,	     VM_SYNC      ) |
-		arch_calc_vm_flag_bits(file, flags);
+	       arch_calc_vm_flag_bits(flags);
 }
 
 unsigned long vm_commit_limit(void);
@@ -188,31 +187,16 @@ static inline bool arch_memory_deny_writ
  *
  *	d)	mmap(PROT_READ | PROT_EXEC)
  *		mmap(PROT_READ | PROT_EXEC | PROT_BTI)
- *
- * This is only applicable if the user has set the Memory-Deny-Write-Execute
- * (MDWE) protection mask for the current process.
- *
- * @old specifies the VMA flags the VMA originally possessed, and @new the ones
- * we propose to set.
- *
- * Return: false if proposed change is OK, true if not ok and should be denied.
  */
-static inline bool map_deny_write_exec(unsigned long old, unsigned long new)
+static inline bool map_deny_write_exec(struct vm_area_struct *vma,  unsigned long vm_flags)
 {
-	/* If MDWE is disabled, we have nothing to deny. */
 	if (!test_bit(MMF_HAS_MDWE, &current->mm->flags))
 		return false;
 
-	/* If the new VMA is not executable, we have nothing to deny. */
-	if (!(new & VM_EXEC))
-		return false;
-
-	/* Under MDWE we do not accept newly writably executable VMAs... */
-	if (new & VM_WRITE)
+	if ((vm_flags & VM_EXEC) && (vm_flags & VM_WRITE))
 		return true;
 
-	/* ...nor previously non-executable VMAs becoming executable. */
-	if (!(old & VM_EXEC))
+	if (!(vma->vm_flags & VM_EXEC) && (vm_flags & VM_EXEC))
 		return true;
 
 	return false;
Index: kernel-rpi-6_6/mm/mprotect.c
===================================================================
--- kernel-rpi-6_6.orig/mm/mprotect.c
+++ kernel-rpi-6_6/mm/mprotect.c
@@ -32,6 +32,7 @@
 #include <linux/sched/sysctl.h>
 #include <linux/userfaultfd_k.h>
 #include <linux/memory-tiers.h>
+#include <uapi/linux/mman.h>
 #include <asm/cacheflush.h>
 #include <asm/mmu_context.h>
 #include <asm/tlbflush.h>
@@ -584,6 +585,9 @@ mprotect_fixup(struct vma_iterator *vmi,
 	pgoff_t pgoff;
 	int error;
 
+	if (!can_modify_vma(vma))
+		return -EPERM;
+
 	if (newflags == oldflags) {
 		*pprev = vma;
 		return 0;
@@ -791,7 +795,7 @@ static int do_mprotect_pkey(unsigned lon
 			break;
 		}
 
-		if (map_deny_write_exec(vma->vm_flags, newflags)) {
+		if (map_deny_write_exec(vma, newflags)) {
 			error = -EACCES;
 			break;
 		}
Index: kernel-rpi-6_6/arch/arm64/include/asm/mman.h
===================================================================
--- kernel-rpi-6_6.orig/arch/arm64/include/asm/mman.h
+++ kernel-rpi-6_6/arch/arm64/include/asm/mman.h
@@ -3,8 +3,6 @@
 #define __ASM_MMAN_H__
 
 #include <linux/compiler.h>
-#include <linux/fs.h>
-#include <linux/shmem_fs.h>
 #include <linux/types.h>
 #include <uapi/asm/mman.h>
 
@@ -23,21 +21,19 @@ static inline unsigned long arch_calc_vm
 }
 #define arch_calc_vm_prot_bits(prot, pkey) arch_calc_vm_prot_bits(prot, pkey)
 
-static inline unsigned long arch_calc_vm_flag_bits(struct file *file,
-						   unsigned long flags)
+static inline unsigned long arch_calc_vm_flag_bits(unsigned long flags)
 {
 	/*
 	 * Only allow MTE on anonymous mappings as these are guaranteed to be
 	 * backed by tags-capable memory. The vm_flags may be overridden by a
 	 * filesystem supporting MTE (RAM-based).
 	 */
-	if (system_supports_mte() &&
-	    ((flags & MAP_ANONYMOUS) || shmem_file(file)))
+	if (system_supports_mte() && (flags & MAP_ANONYMOUS))
 		return VM_MTE_ALLOWED;
 
 	return 0;
 }
-#define arch_calc_vm_flag_bits(file, flags) arch_calc_vm_flag_bits(file, flags)
+#define arch_calc_vm_flag_bits(flags) arch_calc_vm_flag_bits(flags)
 
 static inline bool arch_validate_prot(unsigned long prot,
 	unsigned long addr __always_unused)
Index: kernel-rpi-6_6/mm/huge_memory.c
===================================================================
--- kernel-rpi-6_6.orig/mm/huge_memory.c
+++ kernel-rpi-6_6/mm/huge_memory.c
@@ -78,7 +78,18 @@ bool hugepage_vma_check(struct vm_area_s
 	if (!vma->vm_mm)		/* vdso */
 		return false;
 
-	if (thp_disabled_by_hw() || vma_thp_disabled(vma, vm_flags))
+	/*
+	 * Explicitly disabled through madvise or prctl, or some
+	 * architectures may disable THP for some mappings, for
+	 * example, s390 kvm.
+	 * */
+	if ((vm_flags & VM_NOHUGEPAGE) ||
+	    test_bit(MMF_DISABLE_THP, &vma->vm_mm->flags))
+		return false;
+	/*
+	 * If the hardware/firmware marked hugepage support disabled.
+	 */
+	if (transparent_hugepage_flags & (1 << TRANSPARENT_HUGEPAGE_UNSUPPORTED))
 		return false;
 
 	/* khugepaged doesn't collapse DAX vma, but page fault is fine. */
@@ -569,8 +580,8 @@ struct deferred_split *get_deferred_spli
 
 void folio_prep_large_rmappable(struct folio *folio)
 {
-	if (!folio || !folio_test_large(folio))
-		return;
+	VM_BUG_ON_FOLIO(folio_order(folio) < 2, folio);
+	INIT_LIST_HEAD(&folio->_deferred_list);
 	folio_set_large_rmappable(folio);
 }
 
@@ -2720,10 +2731,9 @@ int split_huge_page_to_list(struct page
 	/* Prevent deferred_split_scan() touching ->_refcount */
 	spin_lock(&ds_queue->split_queue_lock);
 	if (folio_ref_freeze(folio, 1 + extra_pins)) {
-		if (folio_order(folio) > 1 &&
-		    !list_empty(&folio->_deferred_list)) {
+		if (!list_empty(&folio->_deferred_list)) {
 			ds_queue->split_queue_len--;
-			list_del_init(&folio->_deferred_list);
+			list_del(&folio->_deferred_list);
 		}
 		spin_unlock(&ds_queue->split_queue_lock);
 		if (mapping) {
@@ -2767,38 +2777,26 @@ out:
 	return ret;
 }
 
-/*
- * __folio_unqueue_deferred_split() is not to be called directly:
- * the folio_unqueue_deferred_split() inline wrapper in mm/internal.h
- * limits its calls to those folios which may have a _deferred_list for
- * queueing THP splits, and that list is (racily observed to be) non-empty.
- *
- * It is unsafe to call folio_unqueue_deferred_split() until folio refcount is
- * zero: because even when split_queue_lock is held, a non-empty _deferred_list
- * might be in use on deferred_split_scan()'s unlocked on-stack list.
- *
- * If memory cgroups are enabled, split_queue_lock is in the mem_cgroup: it is
- * therefore important to unqueue deferred split before changing folio memcg.
- */
-bool __folio_unqueue_deferred_split(struct folio *folio)
+void folio_undo_large_rmappable(struct folio *folio)
 {
 	struct deferred_split *ds_queue;
 	unsigned long flags;
-	bool unqueued = false;
 
-	WARN_ON_ONCE(folio_ref_count(folio));
-	WARN_ON_ONCE(!mem_cgroup_disabled() && !folio_memcg(folio));
+	/*
+	 * At this point, there is no one trying to add the folio to
+	 * deferred_list. If folio is not in deferred_list, it's safe
+	 * to check without acquiring the split_queue_lock.
+	 */
+	if (data_race(list_empty(&folio->_deferred_list)))
+		return;
 
 	ds_queue = get_deferred_split_queue(folio);
 	spin_lock_irqsave(&ds_queue->split_queue_lock, flags);
 	if (!list_empty(&folio->_deferred_list)) {
 		ds_queue->split_queue_len--;
-		list_del_init(&folio->_deferred_list);
-		unqueued = true;
+		list_del(&folio->_deferred_list);
 	}
 	spin_unlock_irqrestore(&ds_queue->split_queue_lock, flags);
-
-	return unqueued;	/* useful for debug warnings */
 }
 
 void deferred_split_folio(struct folio *folio)
@@ -2809,19 +2807,17 @@ void deferred_split_folio(struct folio *
 #endif
 	unsigned long flags;
 
-	/*
-	 * Order 1 folios have no space for a deferred list, but we also
-	 * won't waste much memory by not adding them to the deferred list.
-	 */
-	if (folio_order(folio) <= 1)
-		return;
+	VM_BUG_ON_FOLIO(folio_order(folio) < 2, folio);
 
 	/*
-	 * Exclude swapcache: originally to avoid a corrupt deferred split
-	 * queue. Nowadays that is fully prevented by mem_cgroup_swapout();
-	 * but if page reclaim is already handling the same folio, it is
-	 * unnecessary to handle it again in the shrinker, so excluding
-	 * swapcache here may still be a useful optimization.
+	 * The try_to_unmap() in page reclaim path might reach here too,
+	 * this may cause a race condition to corrupt deferred split queue.
+	 * And, if page reclaim is already handling the same folio, it is
+	 * unnecessary to handle it again in shrinker.
+	 *
+	 * Check the swapcache flag to determine if the folio is being
+	 * handled by page reclaim since THP swap would add the folio into
+	 * swap cache before calling try_to_unmap().
 	 */
 	if (folio_test_swapcache(folio))
 		return;
Index: kernel-rpi-6_6/mm/swap.c
===================================================================
--- kernel-rpi-6_6.orig/mm/swap.c
+++ kernel-rpi-6_6/mm/swap.c
@@ -68,6 +68,7 @@ struct cpu_fbatches {
 	struct folio_batch lru_lazyfree;
 #ifdef CONFIG_SMP
 	struct folio_batch activate;
+	struct folio_batch promote;
 #endif
 };
 static DEFINE_PER_CPU(struct cpu_fbatches, cpu_fbatches) = {
@@ -89,6 +90,14 @@ static void __page_cache_release(struct
 		__folio_clear_lru_flags(folio);
 		unlock_page_lruvec_irqrestore(lruvec, flags);
 	}
+	/* See comment on folio_test_mlocked in release_pages() */
+	if (unlikely(folio_test_mlocked(folio))) {
+		long nr_pages = folio_nr_pages(folio);
+
+		__folio_clear_mlocked(folio);
+		zone_stat_mod_folio(folio, NR_MLOCK, -nr_pages);
+		count_vm_events(UNEVICTABLE_PGCLEARED, nr_pages);
+	}
 }
 
 static void __folio_put_small(struct folio *folio)
@@ -332,13 +341,26 @@ static void folio_activate_fn(struct lru
 	}
 }
 
+static void folio_promote_fn(struct lruvec *lruvec, struct folio *folio)
+{
+	if (!folio_test_unevictable(folio)) {
+		lruvec_del_folio(lruvec, folio);
+		lruvec_add_folio(lruvec, folio);
+	}
+}
+
 #ifdef CONFIG_SMP
-static void folio_activate_drain(int cpu)
+static void folio_update_drain(int cpu)
 {
 	struct folio_batch *fbatch = &per_cpu(cpu_fbatches.activate, cpu);
 
 	if (folio_batch_count(fbatch))
 		folio_batch_move_lru(fbatch, folio_activate_fn);
+
+	fbatch = &per_cpu(cpu_fbatches.promote, cpu);
+
+	if (folio_batch_count(fbatch))
+		folio_batch_move_lru(fbatch, folio_promote_fn);
 }
 
 void folio_activate(struct folio *folio)
@@ -355,6 +377,19 @@ void folio_activate(struct folio *folio)
 	}
 }
 
+void folio_promote(struct folio *folio)
+{
+	if (folio_test_lru(folio) && !folio_test_unevictable(folio)) {
+		struct folio_batch *fbatch;
+
+		folio_get(folio);
+		local_lock(&cpu_fbatches.lock);
+		fbatch = this_cpu_ptr(&cpu_fbatches.promote);
+		folio_batch_add_and_move(fbatch, folio, folio_promote_fn);
+		local_unlock(&cpu_fbatches.lock);
+	}
+}
+
 #else
 static inline void folio_activate_drain(int cpu)
 {
@@ -371,6 +406,18 @@ void folio_activate(struct folio *folio)
 		folio_set_lru(folio);
 	}
 }
+
+void folio_promote(struct folio *folio)
+{
+	struct lruvec *lruvec;
+
+	if (folio_test_clear_lru(folio)) {
+		lruvec = folio_lruvec_lock_irq(folio);
+		folio_promote_fn(lruvec, folio);
+		unlock_page_lruvec_irq(lruvec);
+		folio_set_lru(folio);
+	}
+}
 #endif
 
 static void __lru_cache_activate_folio(struct folio *folio)
@@ -500,7 +547,8 @@ void folio_add_lru(struct folio *folio)
 
 	/* see the comment in lru_gen_add_folio() */
 	if (lru_gen_enabled() && !folio_test_unevictable(folio) &&
-	    lru_gen_in_fault() && !(current->flags & PF_MEMALLOC))
+	    lru_gen_in_fault() && !(current->flags & PF_MEMALLOC) &&
+	    lru_gen_aggressive_mm())
 		folio_set_active(folio);
 
 	folio_get(folio);
@@ -599,6 +647,7 @@ static void lru_deactivate_fn(struct lru
 		lruvec_del_folio(lruvec, folio);
 		folio_clear_active(folio);
 		folio_clear_referenced(folio);
+		folio_test_clear_young(folio);
 		lruvec_add_folio(lruvec, folio);
 
 		__count_vm_events(PGDEACTIVATE, nr_pages);
@@ -666,7 +715,7 @@ void lru_add_drain_cpu(int cpu)
 	if (folio_batch_count(fbatch))
 		folio_batch_move_lru(fbatch, lru_lazyfree_fn);
 
-	folio_activate_drain(cpu);
+	folio_update_drain(cpu);
 }
 
 /**
@@ -790,6 +839,7 @@ static bool cpu_needs_drain(unsigned int
 		folio_batch_count(&fbatches->lru_deactivate) ||
 		folio_batch_count(&fbatches->lru_lazyfree) ||
 		folio_batch_count(&fbatches->activate) ||
+		folio_batch_count(&fbatches->promote) ||
 		need_mlock_drain(cpu) ||
 		has_bh_in_lru(cpu, NULL);
 }
@@ -1013,6 +1063,18 @@ void release_pages(release_pages_arg arg
 			__folio_clear_lru_flags(folio);
 		}
 
+		/*
+		 * In rare cases, when truncation or holepunching raced with
+		 * munlock after VM_LOCKED was cleared, Mlocked may still be
+		 * found set here.  This does not indicate a problem, unless
+		 * "unevictable_pgs_cleared" appears worryingly large.
+		 */
+		if (unlikely(folio_test_mlocked(folio))) {
+			__folio_clear_mlocked(folio);
+			zone_stat_sub_folio(folio, NR_MLOCK);
+			count_vm_event(UNEVICTABLE_PGCLEARED);
+		}
+
 		list_add(&folio->lru, &pages_to_free);
 	}
 	if (lruvec)
Index: kernel-rpi-6_6/mm/khugepaged.c
===================================================================
--- kernel-rpi-6_6.orig/mm/khugepaged.c
+++ kernel-rpi-6_6/mm/khugepaged.c
@@ -887,6 +887,20 @@ static int hpage_collapse_find_target_no
 }
 #endif
 
+static bool hpage_collapse_alloc_page(struct page **hpage, gfp_t gfp, int node,
+				      nodemask_t *nmask)
+{
+	*hpage = __alloc_pages(gfp, HPAGE_PMD_ORDER, node, nmask);
+	if (unlikely(!*hpage)) {
+		count_vm_event(THP_COLLAPSE_ALLOC_FAILED);
+		return false;
+	}
+
+	folio_prep_large_rmappable((struct folio *)*hpage);
+	count_vm_event(THP_COLLAPSE_ALLOC);
+	return true;
+}
+
 /*
  * If mmap_lock temporarily dropped, revalidate vma
  * before taking mmap_lock.
@@ -1041,7 +1055,7 @@ out:
 	return result;
 }
 
-static int alloc_charge_folio(struct folio **foliop, struct mm_struct *mm,
+static int alloc_charge_hpage(struct page **hpage, struct mm_struct *mm,
 			      struct collapse_control *cc)
 {
 	gfp_t gfp = (cc->is_khugepaged ? alloc_hugepage_khugepaged_gfpmask() :
@@ -1049,23 +1063,17 @@ static int alloc_charge_folio(struct fol
 	int node = hpage_collapse_find_target_node(cc);
 	struct folio *folio;
 
-	folio = __folio_alloc(gfp, HPAGE_PMD_ORDER, node, &cc->alloc_nmask);
-	if (!folio) {
-		*foliop = NULL;
-		count_vm_event(THP_COLLAPSE_ALLOC_FAILED);
+	if (!hpage_collapse_alloc_page(hpage, gfp, node, &cc->alloc_nmask))
 		return SCAN_ALLOC_HUGE_PAGE_FAIL;
-	}
 
-	count_vm_event(THP_COLLAPSE_ALLOC);
+	folio = page_folio(*hpage);
 	if (unlikely(mem_cgroup_charge(folio, mm, gfp))) {
 		folio_put(folio);
-		*foliop = NULL;
+		*hpage = NULL;
 		return SCAN_CGROUP_CHARGE_FAIL;
 	}
+	count_memcg_page_event(*hpage, THP_COLLAPSE_ALLOC);
 
-	count_memcg_folio_events(folio, THP_COLLAPSE_ALLOC, 1);
-
-	*foliop = folio;
 	return SCAN_SUCCEED;
 }
 
@@ -1077,7 +1085,6 @@ static int collapse_huge_page(struct mm_
 	pmd_t *pmd, _pmd;
 	pte_t *pte;
 	pgtable_t pgtable;
-	struct folio *folio;
 	struct page *hpage;
 	spinlock_t *pmd_ptl, *pte_ptl;
 	int result = SCAN_FAIL;
@@ -1094,8 +1101,7 @@ static int collapse_huge_page(struct mm_
 	 */
 	mmap_read_unlock(mm);
 
-	result = alloc_charge_folio(&folio, mm, cc);
-	hpage = &folio->page;
+	result = alloc_charge_hpage(&hpage, mm, cc);
 	if (result != SCAN_SUCCEED)
 		goto out_nolock;
 
@@ -1199,11 +1205,12 @@ static int collapse_huge_page(struct mm_
 		goto out_up_write;
 
 	/*
-	 * The smp_wmb() inside __folio_mark_uptodate() ensures the
-	 * copy_huge_page writes become visible before the set_pmd_at()
-	 * write.
+	 * spin_lock() below is not the equivalent of smp_wmb(), but
+	 * the smp_wmb() inside __SetPageUptodate() can be reused to
+	 * avoid the copy_huge_page writes to become visible after
+	 * the set_pmd_at() write.
 	 */
-	__folio_mark_uptodate(folio);
+	__SetPageUptodate(hpage);
 	pgtable = pmd_pgtable(_pmd);
 
 	_pmd = mk_huge_pmd(hpage, vma->vm_page_prot);
@@ -1211,8 +1218,8 @@ static int collapse_huge_page(struct mm_
 
 	spin_lock(pmd_ptl);
 	BUG_ON(!pmd_none(*pmd));
-	folio_add_new_anon_rmap(folio, vma, address);
-	folio_add_lru_vma(folio, vma);
+	page_add_new_anon_rmap(hpage, vma, address);
+	lru_cache_add_inactive_or_unevictable(hpage, vma);
 	pgtable_trans_huge_deposit(mm, pmd, pgtable);
 	set_pmd_at(mm, address, pmd, _pmd);
 	update_mmu_cache_pmd(vma, address, pmd);
@@ -1783,27 +1790,29 @@ static int collapse_file(struct mm_struc
 			 struct collapse_control *cc)
 {
 	struct address_space *mapping = file->f_mapping;
+	struct page *hpage;
 	struct page *page;
-	struct page *tmp, *dst;
-	struct folio *folio, *new_folio;
+	struct page *tmp;
+	struct folio *folio;
 	pgoff_t index = 0, end = start + HPAGE_PMD_NR;
 	LIST_HEAD(pagelist);
 	XA_STATE_ORDER(xas, &mapping->i_pages, start, HPAGE_PMD_ORDER);
 	int nr_none = 0, result = SCAN_SUCCEED;
 	bool is_shmem = shmem_file(file);
+	int nr = 0;
 
 	VM_BUG_ON(!IS_ENABLED(CONFIG_READ_ONLY_THP_FOR_FS) && !is_shmem);
 	VM_BUG_ON(start & (HPAGE_PMD_NR - 1));
 
-	result = alloc_charge_folio(&new_folio, mm, cc);
+	result = alloc_charge_hpage(&hpage, mm, cc);
 	if (result != SCAN_SUCCEED)
 		goto out;
 
-	__folio_set_locked(new_folio);
+	__SetPageLocked(hpage);
 	if (is_shmem)
-		__folio_set_swapbacked(new_folio);
-	new_folio->index = start;
-	new_folio->mapping = mapping;
+		__SetPageSwapBacked(hpage);
+	hpage->index = start;
+	hpage->mapping = mapping;
 
 	/*
 	 * Ensure we have slots for all the pages in the range.  This is
@@ -2036,24 +2045,20 @@ xa_unlocked:
 	 * The old pages are locked, so they won't change anymore.
 	 */
 	index = start;
-	dst = folio_page(new_folio, 0);
 	list_for_each_entry(page, &pagelist, lru) {
 		while (index < page->index) {
-			clear_highpage(dst);
+			clear_highpage(hpage + (index % HPAGE_PMD_NR));
 			index++;
-			dst++;
 		}
-		if (copy_mc_highpage(dst, page) > 0) {
+		if (copy_mc_highpage(hpage + (page->index % HPAGE_PMD_NR), page) > 0) {
 			result = SCAN_COPY_MC;
 			goto rollback;
 		}
 		index++;
-		dst++;
 	}
 	while (index < end) {
-		clear_highpage(dst);
+		clear_highpage(hpage + (index % HPAGE_PMD_NR));
 		index++;
-		dst++;
 	}
 
 	if (nr_none) {
@@ -2081,17 +2086,16 @@ xa_unlocked:
 		}
 
 		/*
-		 * If userspace observed a missing page in a VMA with
-		 * a MODE_MISSING userfaultfd, then it might expect a
-		 * UFFD_EVENT_PAGEFAULT for that page. If so, we need to
-		 * roll back to avoid suppressing such an event. Since
-		 * wp/minor userfaultfds don't give userspace any
-		 * guarantees that the kernel doesn't fill a missing
-		 * page with a zero page, so they don't matter here.
+		 * If userspace observed a missing page in a VMA with a MODE_MISSING
+		 * userfaultfd, then it might expect a UFFD_EVENT_PAGEFAULT for that
+		 * page. If so, we need to roll back to avoid suppressing such an
+		 * event. Since wp/minor userfaultfds don't give userspace any
+		 * guarantees that the kernel doesn't fill a missing page with a zero
+		 * page, so they don't matter here.
 		 *
-		 * Any userfaultfds registered after this point will
-		 * not be able to observe any missing pages due to the
-		 * previously inserted retry entries.
+		 * Any userfaultfds registered after this point will not be able to
+		 * observe any missing pages due to the previously inserted retry
+		 * entries.
 		 */
 		vma_interval_tree_foreach(vma, &mapping->i_mmap, start, end) {
 			if (userfaultfd_missing(vma)) {
@@ -2116,32 +2120,33 @@ immap_locked:
 		xas_lock_irq(&xas);
 	}
 
+	nr = thp_nr_pages(hpage);
 	if (is_shmem)
-		__lruvec_stat_mod_folio(new_folio, NR_SHMEM_THPS, HPAGE_PMD_NR);
+		__mod_lruvec_page_state(hpage, NR_SHMEM_THPS, nr);
 	else
-		__lruvec_stat_mod_folio(new_folio, NR_FILE_THPS, HPAGE_PMD_NR);
+		__mod_lruvec_page_state(hpage, NR_FILE_THPS, nr);
 
 	if (nr_none) {
-		__lruvec_stat_mod_folio(new_folio, NR_FILE_PAGES, nr_none);
+		__mod_lruvec_page_state(hpage, NR_FILE_PAGES, nr_none);
 		/* nr_none is always 0 for non-shmem. */
-		__lruvec_stat_mod_folio(new_folio, NR_SHMEM, nr_none);
+		__mod_lruvec_page_state(hpage, NR_SHMEM, nr_none);
 	}
 
 	/*
-	 * Mark new_folio as uptodate before inserting it into the
-	 * page cache so that it isn't mistaken for an fallocated but
-	 * unwritten page.
+	 * Mark hpage as uptodate before inserting it into the page cache so
+	 * that it isn't mistaken for an fallocated but unwritten page.
 	 */
-	folio_mark_uptodate(new_folio);
-	folio_ref_add(new_folio, HPAGE_PMD_NR - 1);
+	folio = page_folio(hpage);
+	folio_mark_uptodate(folio);
+	folio_ref_add(folio, HPAGE_PMD_NR - 1);
 
 	if (is_shmem)
-		folio_mark_dirty(new_folio);
-	folio_add_lru(new_folio);
+		folio_mark_dirty(folio);
+	folio_add_lru(folio);
 
 	/* Join all the small entries into a single multi-index entry. */
 	xas_set_order(&xas, start, HPAGE_PMD_ORDER);
-	xas_store(&xas, new_folio);
+	xas_store(&xas, hpage);
 	WARN_ON_ONCE(xas_error(&xas));
 	xas_unlock_irq(&xas);
 
@@ -2152,7 +2157,7 @@ immap_locked:
 	retract_page_tables(mapping, start);
 	if (cc && !cc->is_khugepaged)
 		result = SCAN_PTE_MAPPED_HUGEPAGE;
-	folio_unlock(new_folio);
+	unlock_page(hpage);
 
 	/*
 	 * The collapse has succeeded, so free the old pages.
@@ -2197,13 +2202,13 @@ rollback:
 		smp_mb();
 	}
 
-	new_folio->mapping = NULL;
+	hpage->mapping = NULL;
 
-	folio_unlock(new_folio);
-	folio_put(new_folio);
+	unlock_page(hpage);
+	put_page(hpage);
 out:
 	VM_BUG_ON(!list_empty(&pagelist));
-	trace_mm_khugepaged_collapse_file(mm, new_folio, index, addr, is_shmem, file, HPAGE_PMD_NR, result);
+	trace_mm_khugepaged_collapse_file(mm, hpage, index, is_shmem, addr, file, nr, result);
 	return result;
 }
 
Index: kernel-rpi-6_6/include/linux/memcontrol.h
===================================================================
--- kernel-rpi-6_6.orig/include/linux/memcontrol.h
+++ kernel-rpi-6_6/include/linux/memcontrol.h
@@ -1080,6 +1080,15 @@ static inline void count_memcg_events(st
 	local_irq_restore(flags);
 }
 
+static inline void count_memcg_page_event(struct page *page,
+					  enum vm_event_item idx)
+{
+	struct mem_cgroup *memcg = page_memcg(page);
+
+	if (memcg)
+		count_memcg_events(memcg, idx, 1);
+}
+
 static inline void count_memcg_folio_events(struct folio *folio,
 		enum vm_event_item idx, unsigned long nr)
 {
@@ -1556,6 +1565,11 @@ static inline void __count_memcg_events(
 {
 }
 
+static inline void count_memcg_page_event(struct page *page,
+					  int idx)
+{
+}
+
 static inline void count_memcg_folio_events(struct folio *folio,
 		enum vm_event_item idx, unsigned long nr)
 {
Index: kernel-rpi-6_6/include/trace/events/huge_memory.h
===================================================================
--- kernel-rpi-6_6.orig/include/trace/events/huge_memory.h
+++ kernel-rpi-6_6/include/trace/events/huge_memory.h
@@ -207,41 +207,41 @@ TRACE_EVENT(mm_khugepaged_scan_file,
 );
 
 TRACE_EVENT(mm_khugepaged_collapse_file,
-	TP_PROTO(struct mm_struct *mm, struct folio *new_folio, pgoff_t index,
-			unsigned long addr, bool is_shmem, struct file *file,
-			int nr, int result),
-	TP_ARGS(mm, new_folio, index, addr, is_shmem, file, nr, result),
-	TP_STRUCT__entry(
-		__field(struct mm_struct *, mm)
-		__field(unsigned long, hpfn)
-		__field(pgoff_t, index)
-		__field(unsigned long, addr)
-		__field(bool, is_shmem)
-		__string(filename, file->f_path.dentry->d_iname)
-		__field(int, nr)
-		__field(int, result)
-	),
+  TP_PROTO(struct mm_struct *mm, struct page *hpage, pgoff_t index,
+      bool is_shmem, unsigned long addr, struct file *file,
+      int nr, int result),
+  TP_ARGS(mm, hpage, index, addr, is_shmem, file, nr, result),
+  TP_STRUCT__entry(
+    __field(struct mm_struct *, mm)
+    __field(unsigned long, hpfn)
+    __field(pgoff_t, index)
+    __field(unsigned long, addr)
+    __field(bool, is_shmem)
+    __string(filename, file->f_path.dentry->d_iname)
+    __field(int, nr)
+    __field(int, result)
+  ),
 
-	TP_fast_assign(
-		__entry->mm = mm;
-		__entry->hpfn = new_folio ? folio_pfn(new_folio) : -1;
-		__entry->index = index;
-		__entry->addr = addr;
-		__entry->is_shmem = is_shmem;
-		__assign_str(filename, file->f_path.dentry->d_iname);
-		__entry->nr = nr;
-		__entry->result = result;
-	),
+  TP_fast_assign(
+    __entry->mm = mm;
+    __entry->hpfn = hpage ? page_to_pfn(hpage) : -1;
+    __entry->index = index;
+    __entry->addr = addr;
+    __entry->is_shmem = is_shmem;
+    __assign_str(filename, file->f_path.dentry->d_iname);
+    __entry->nr = nr;
+    __entry->result = result;
+  ),
 
-	TP_printk("mm=%p, hpage_pfn=0x%lx, index=%ld, addr=%lx, is_shmem=%d, filename=%s, nr=%d, result=%s",
-		__entry->mm,
-		__entry->hpfn,
-		__entry->index,
-		__entry->addr,
-		__entry->is_shmem,
-		__get_str(filename),
-		__entry->nr,
-		__print_symbolic(__entry->result, SCAN_STATUS))
+  TP_printk("mm=%p, hpage_pfn=0x%lx, index=%ld, addr=%ld, is_shmem=%d, filename=%s, nr=%d, result=%s",
+    __entry->mm,
+    __entry->hpfn,
+    __entry->index,
+    __entry->addr,
+    __entry->is_shmem,
+    __get_str(filename),
+    __entry->nr,
+    __print_symbolic(__entry->result, SCAN_STATUS))
 );
 
 #endif /* __HUGE_MEMORY_H */
Index: kernel-rpi-6_6/include/linux/page-flags.h
===================================================================
--- kernel-rpi-6_6.orig/include/linux/page-flags.h
+++ kernel-rpi-6_6/include/linux/page-flags.h
@@ -1079,7 +1079,7 @@ static __always_inline void __ClearPageA
 	 1UL << PG_private	| 1UL << PG_private_2	|	\
 	 1UL << PG_writeback	| 1UL << PG_reserved	|	\
 	 1UL << PG_slab		| 1UL << PG_active 	|	\
-	 1UL << PG_unevictable	| __PG_MLOCKED | LRU_GEN_MASK)
+	 1UL << PG_unevictable	| __PG_MLOCKED )
 
 /*
  * Flags checked when a page is prepped for return by the page allocator.
