Index: rpi4-kernel/drivers/Kconfig
===================================================================
--- rpi4-kernel.orig/drivers/Kconfig
+++ rpi4-kernel/drivers/Kconfig
@@ -236,4 +236,7 @@ source "drivers/interconnect/Kconfig"
 source "drivers/counter/Kconfig"
 
 source "drivers/most/Kconfig"
+
+source "drivers/pkglist/Kconfig"
+
 endmenu
Index: rpi4-kernel/drivers/Makefile
===================================================================
--- rpi4-kernel.orig/drivers/Makefile
+++ rpi4-kernel/drivers/Makefile
@@ -187,3 +187,5 @@ obj-$(CONFIG_GNSS)		+= gnss/
 obj-$(CONFIG_INTERCONNECT)	+= interconnect/
 obj-$(CONFIG_COUNTER)		+= counter/
 obj-$(CONFIG_MOST)		+= most/
+
+obj-$(CONFIG_PKGLIST)   += pkglist/
Index: rpi4-kernel/drivers/char/mem.c
===================================================================
--- rpi4-kernel.orig/drivers/char/mem.c
+++ rpi4-kernel/drivers/char/mem.c
@@ -30,6 +30,7 @@
 #include <linux/uio.h>
 #include <linux/uaccess.h>
 #include <linux/security.h>
+#include <linux/low-mem-notify.h>
 
 #ifdef CONFIG_IA64
 # include <linux/efi.h>
@@ -707,6 +708,9 @@ static const struct memdev {
 #ifdef CONFIG_PRINTK
 	[11] = { "kmsg", 0644, &kmsg_fops, 0 },
 #endif
+#ifdef CONFIG_LOW_MEM_NOTIFY
+  [12] = { "chromeos-low-mem", 0666, &low_mem_notify_fops, 0 },
+#endif
 };
 
 static int memory_open(struct inode *inode, struct file *filp)
Index: rpi4-kernel/drivers/pkglist/Kconfig
===================================================================
--- /dev/null
+++ rpi4-kernel/drivers/pkglist/Kconfig
@@ -0,0 +1,30 @@
+config PKGLIST
+	tristate "Package list for emulated 'SD card' file system for Android"
+	depends on CONFIGFS_FS || !CONFIGFS_FS
+	help
+	  Pkglist presents an interface for Android's emulated sdcard layer.
+	  It relates the names of packages to their package ids, so that they can be
+	  given access to their app specific folders.
+
+	  Additionally, pkglist allows configuring the gid assigned to the lower file
+	  outside of package specific directories for the purpose of tracking storage
+	  with quotas.
+
+choice
+	prompt "Configuration options"
+	depends on PKGLIST
+	help
+	  Configuration options. This controls how you provide the emulated
+	  SD card layer with configuration information from userspace.
+
+config PKGLIST_USE_CONFIGFS
+	bool "Use Configfs based pkglist"
+	depends on CONFIGFS_FS
+	help
+	  Use configfs based pkglist driver for configuration information.
+
+config PKGLIST_NO_CONFIG
+	bool "None"
+	help
+	  This does not allow configuration of sdcardfs.
+endchoice
Index: rpi4-kernel/drivers/pkglist/Makefile
===================================================================
--- /dev/null
+++ rpi4-kernel/drivers/pkglist/Makefile
@@ -0,0 +1,3 @@
+obj-$(CONFIG_PKGLIST) += pkg.o
+pkg-$(CONFIG_PKGLIST_USE_CONFIGFS) += pkglist.o
+pkg-$(CONFIG_PKGLIST_NO_CONFIG) += pkglist_none.o
Index: rpi4-kernel/drivers/pkglist/pkglist.c
===================================================================
--- /dev/null
+++ rpi4-kernel/drivers/pkglist/pkglist.c
@@ -0,0 +1,966 @@
+/*
+ * Copyright (C) 2017 Google Inc., Author: Daniel Rosenberg <drosen@google.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/module.h>
+#include <linux/hashtable.h>
+#include <linux/atomic.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/configfs.h>
+#include <linux/dcache.h>
+#include <linux/ctype.h>
+#include <linux/cred.h>
+
+#include <linux/pkglist.h>
+
+/*
+ * This presents a configfs interface for Android's emulated sdcard layer.
+ * It relates the names of packages to their package ids, so that they can be
+ * given access to their app specific folders.
+ *
+ * To add a package, create a directory at the base level with the name of that
+ * package. Within these folders, write to appid to set its id.
+ * If an Android user should not know of an app's installation, write their
+ * Android user id to excluded_userids. Write to clear_userid to remove users
+ * from that list.
+ *
+ * remove_userid offers a way to remove all instances of a user from all exclude
+ * lists.
+ *
+ * Additionally, pkglist allows configuring the gid assigned to the lower file
+ * outside of package specific directories for the purpose of tracking storage
+ * with quotas.
+ *
+ * To track files with a particular extension, create a folder inside extensions
+ * for each class of thing you wish to track. Inside that directory, write the
+ * gid you want to associate to the group to ext_gid, and make a directory for
+ * extension you want to include. All are assumed to be case insensitive.
+ *
+ * ex: mkdir /config/[config_location]/extension/audio/
+ *     echo 1055 > /config/[config_location]/extension/audio/ext_gid
+ *     mkdir /config/[config_location]/extension/audio/
+ *
+ */
+
+static char *pkglist_config_location = "sdcardfs";
+module_param(pkglist_config_location, charp, 0);
+MODULE_PARM_DESC(pkglist_config_location, "Location of pkglist in configfs");
+
+static struct kmem_cache *hashtable_entry_cachep;
+
+static DEFINE_HASHTABLE(package_to_appid, 8);
+static DEFINE_HASHTABLE(package_to_userid, 8);
+static DEFINE_HASHTABLE(ext_to_groupid, 8);
+static DEFINE_MUTEX(pkg_list_lock);
+static LIST_HEAD(pkglist_listeners);
+
+struct extensions_value {
+	struct config_group group;
+	kgid_t gid;
+};
+
+struct extension_details {
+	struct config_item item;
+	struct hlist_node hlist;
+	struct qstr name;
+	struct extensions_value *value;
+};
+
+struct hashtable_entry {
+	struct hlist_node hlist;
+	struct hlist_node dlist; /* for deletion cleanup */
+	struct qstr key;
+	atomic_t value;
+};
+
+static unsigned int full_name_case_hash(const unsigned char *name,
+					unsigned int len)
+{
+	unsigned long hash = init_name_hash(0);
+
+	while (len--)
+		hash = partial_name_hash(tolower(*name++), hash);
+	return end_name_hash(hash);
+}
+
+static inline void qstr_init(struct qstr *q, const char *name)
+{
+	q->name = name;
+	q->len = strlen(q->name);
+	q->hash = full_name_case_hash(q->name, q->len);
+}
+
+static inline int qstr_copy(const struct qstr *src, struct qstr *dest)
+{
+	dest->name = kstrdup(src->name, GFP_KERNEL);
+	dest->hash_len = src->hash_len;
+	return !!dest->name;
+}
+
+static kuid_t __get_appid(const struct qstr *key)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = key->hash;
+	uid_t ret_id;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(package_to_appid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			ret_id = atomic_read(&hash_cur->value);
+			rcu_read_unlock();
+			return make_kuid(&init_user_ns, ret_id);
+		}
+	}
+	rcu_read_unlock();
+	return INVALID_UID;
+}
+
+kuid_t pkglist_get_appid(const char *key)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	return __get_appid(&q);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_appid);
+
+static kgid_t __get_ext_gid(const struct qstr *key)
+{
+	struct extension_details *hash_cur;
+	unsigned int hash = key->hash;
+	kgid_t ret_id;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(ext_to_groupid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->name)) {
+			ret_id = hash_cur->value->gid;
+			rcu_read_unlock();
+			return ret_id;
+		}
+	}
+	rcu_read_unlock();
+	return INVALID_GID;
+}
+
+kgid_t pkglist_get_ext_gid(const char *key)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	return __get_ext_gid(&q);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_ext_gid);
+
+static bool __is_excluded(const struct qstr *app_name, uint32_t user)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = app_name->hash;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (atomic_read(&hash_cur->value) == user &&
+				qstr_case_eq(app_name, &hash_cur->key)) {
+			rcu_read_unlock();
+			return true;
+		}
+	}
+	rcu_read_unlock();
+	return false;
+}
+
+bool pkglist_user_is_excluded(const char *key, uint32_t user)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	return __is_excluded(&q, user);
+}
+EXPORT_SYMBOL_GPL(pkglist_user_is_excluded);
+
+kuid_t pkglist_get_allowed_appid(const char *key, uint32_t user)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	if (!__is_excluded(&q, user))
+		return __get_appid(&q);
+	else
+		return INVALID_UID;
+}
+EXPORT_SYMBOL_GPL(pkglist_get_allowed_appid);
+
+static struct hashtable_entry *alloc_hashtable_entry(const struct qstr *key,
+		uid_t value)
+{
+	struct hashtable_entry *ret = kmem_cache_alloc(hashtable_entry_cachep,
+			GFP_KERNEL);
+	if (!ret)
+		return NULL;
+	INIT_HLIST_NODE(&ret->dlist);
+	INIT_HLIST_NODE(&ret->hlist);
+
+	if (!qstr_copy(key, &ret->key)) {
+		kmem_cache_free(hashtable_entry_cachep, ret);
+		return NULL;
+	}
+
+	atomic_set(&ret->value, value);
+	return ret;
+}
+
+static int insert_packagelist_appid_entry_locked(const struct qstr *key,
+						kuid_t value)
+{
+	struct hashtable_entry *hash_cur;
+	struct hashtable_entry *new_entry;
+	unsigned int hash = key->hash;
+
+	hash_for_each_possible_rcu(package_to_appid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			atomic_set(&hash_cur->value, value.val);
+			return 0;
+		}
+	}
+	new_entry = alloc_hashtable_entry(key, value.val);
+	if (!new_entry)
+		return -ENOMEM;
+	hash_add_rcu(package_to_appid, &new_entry->hlist, hash);
+	return 0;
+}
+
+static int insert_ext_gid_entry_locked(struct extension_details *ed)
+{
+	struct extension_details *hash_cur;
+	unsigned int hash = ed->name.hash;
+
+	/* An extension can only belong to one gid */
+	hash_for_each_possible_rcu(ext_to_groupid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(&ed->name, &hash_cur->name))
+			return -EINVAL;
+	}
+
+	hash_add_rcu(ext_to_groupid, &ed->hlist, hash);
+	return 0;
+}
+
+static int insert_userid_exclude_entry_locked(const struct qstr *key,
+						unsigned int value)
+{
+	struct hashtable_entry *hash_cur;
+	struct hashtable_entry *new_entry;
+	unsigned int hash = key->hash;
+
+	/* Only insert if not already present */
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (atomic_read(&hash_cur->value) == value &&
+				qstr_case_eq(key, &hash_cur->key))
+			return 0;
+	}
+	new_entry = alloc_hashtable_entry(key, value);
+	if (!new_entry)
+		return -ENOMEM;
+	hash_add_rcu(package_to_userid, &new_entry->hlist, hash);
+	return 0;
+}
+
+static int insert_packagelist_entry(const struct qstr *key, kuid_t value)
+{
+	struct pkg_list *pkg;
+	int err;
+
+	mutex_lock(&pkg_list_lock);
+	err = insert_packagelist_appid_entry_locked(key, value);
+	if (!err) {
+		list_for_each_entry(pkg, &pkglist_listeners, list) {
+			pkg->update(BY_NAME, key, 0);
+		}
+	}
+	mutex_unlock(&pkg_list_lock);
+
+	return err;
+}
+
+static int insert_ext_gid_entry(struct extension_details *ed)
+{
+	int err;
+
+	mutex_lock(&pkg_list_lock);
+	err = insert_ext_gid_entry_locked(ed);
+	mutex_unlock(&pkg_list_lock);
+
+	return err;
+}
+
+static int insert_userid_exclude_entry(const struct qstr *key, uint32_t value)
+{
+	int err;
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	err = insert_userid_exclude_entry_locked(key, value);
+	if (!err) {
+		list_for_each_entry(pkg, &pkglist_listeners, list) {
+			pkg->update(BY_NAME|BY_USERID, key, value);
+		}
+	}
+	mutex_unlock(&pkg_list_lock);
+
+	return err;
+}
+
+static void free_hashtable_entry(struct hashtable_entry *entry)
+{
+	kfree(entry->key.name);
+	kmem_cache_free(hashtable_entry_cachep, entry);
+}
+
+static void remove_packagelist_entry_locked(const struct qstr *key)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = key->hash;
+	struct hlist_node *h_t;
+	HLIST_HEAD(free_list);
+
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			hash_del_rcu(&hash_cur->hlist);
+			hlist_add_head(&hash_cur->dlist, &free_list);
+		}
+	}
+	hash_for_each_possible_rcu(package_to_appid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			hash_del_rcu(&hash_cur->hlist);
+			hlist_add_head(&hash_cur->dlist, &free_list);
+			break;
+		}
+	}
+	synchronize_rcu();
+	hlist_for_each_entry_safe(hash_cur, h_t, &free_list, dlist)
+		free_hashtable_entry(hash_cur);
+}
+
+static void remove_packagelist_entry(const struct qstr *key)
+{
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	remove_packagelist_entry_locked(key);
+	list_for_each_entry(pkg, &pkglist_listeners, list) {
+		pkg->update(BY_NAME, key, 0);
+	}
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void remove_ext_gid_entry_locked(struct extension_details *ed)
+{
+	struct extension_details *hash_cur;
+	struct qstr *key = &ed->name;
+	unsigned int hash = key->hash;
+
+	hash_for_each_possible_rcu(ext_to_groupid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->name)
+				&& hash_cur->value == ed->value) {
+			hash_del_rcu(&hash_cur->hlist);
+			synchronize_rcu();
+			break;
+		}
+	}
+}
+
+static void remove_ext_gid_entry(struct extension_details *ed)
+{
+	mutex_lock(&pkg_list_lock);
+	remove_ext_gid_entry_locked(ed);
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void remove_userid_all_entry_locked(uint32_t userid)
+{
+	struct hashtable_entry *hash_cur;
+	struct hlist_node *h_t;
+	HLIST_HEAD(free_list);
+	int i;
+
+	hash_for_each_rcu(package_to_userid, i, hash_cur, hlist) {
+		if (atomic_read(&hash_cur->value) == userid) {
+			hash_del_rcu(&hash_cur->hlist);
+			hlist_add_head(&hash_cur->dlist, &free_list);
+		}
+	}
+	synchronize_rcu();
+	hlist_for_each_entry_safe(hash_cur, h_t, &free_list, dlist) {
+		free_hashtable_entry(hash_cur);
+	}
+}
+
+static void remove_userid_all_entry(uint32_t userid)
+{
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	remove_userid_all_entry_locked(userid);
+
+	list_for_each_entry(pkg, &pkglist_listeners, list) {
+		pkg->update(BY_USERID, NULL, userid);
+	}
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void remove_userid_exclude_entry_locked(const struct qstr *key,
+						uint32_t userid)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = key->hash;
+
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key) &&
+				atomic_read(&hash_cur->value) == userid) {
+			hash_del_rcu(&hash_cur->hlist);
+			synchronize_rcu();
+			free_hashtable_entry(hash_cur);
+			break;
+		}
+	}
+}
+
+static void remove_userid_exclude_entry(const struct qstr *key, uint32_t userid)
+{
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	remove_userid_exclude_entry_locked(key, userid);
+	list_for_each_entry(pkg, &pkglist_listeners, list) {
+		pkg->update(BY_NAME|BY_USERID, key, userid);
+	}
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void packagelist_destroy(void)
+{
+	struct hashtable_entry *hash_cur;
+	struct hlist_node *h_t;
+	HLIST_HEAD(free_list);
+	int i;
+
+	mutex_lock(&pkg_list_lock);
+	hash_for_each_rcu(package_to_appid, i, hash_cur, hlist) {
+		hash_del_rcu(&hash_cur->hlist);
+		hlist_add_head(&hash_cur->dlist, &free_list);
+	}
+	hash_for_each_rcu(package_to_userid, i, hash_cur, hlist) {
+		hash_del_rcu(&hash_cur->hlist);
+		hlist_add_head(&hash_cur->dlist, &free_list);
+	}
+	synchronize_rcu();
+	hlist_for_each_entry_safe(hash_cur, h_t, &free_list, dlist)
+		free_hashtable_entry(hash_cur);
+	mutex_unlock(&pkg_list_lock);
+	pr_info("pkglist: destroyed pkglist\n");
+}
+
+#define PACKAGE_DETAILS_ATTR(_pfx, _name)			\
+static struct configfs_attribute _pfx##attr_##_name = {	\
+	.ca_name	= __stringify(_name),		\
+	.ca_mode	= S_IRUGO | S_IWUGO,		\
+	.ca_owner	= THIS_MODULE,			\
+	.show		= _pfx##_name##_show,		\
+	.store		= _pfx##_name##_store,		\
+}
+
+#define PACKAGE_DETAILS_ATTR_RO(_pfx, _name)			\
+static struct configfs_attribute _pfx##attr_##_name = {	\
+	.ca_name	= __stringify(_name),		\
+	.ca_mode	= S_IRUGO,			\
+	.ca_owner	= THIS_MODULE,			\
+	.show		= _pfx##_name##_show,		\
+}
+
+#define PACKAGE_DETAILS_ATTR_WO(_pfx, _name)			\
+static struct configfs_attribute _pfx##attr_##_name = {	\
+	.ca_name	= __stringify(_name),		\
+	.ca_mode	= S_IWUGO,			\
+	.ca_owner	= THIS_MODULE,			\
+	.store		= _pfx##_name##_store,		\
+}
+
+
+struct package_details {
+	struct config_item item;
+	struct qstr name;
+};
+
+static inline struct package_details *to_package_details(
+						struct config_item *item)
+{
+	return item ? container_of(item, struct package_details, item) : NULL;
+}
+
+#define PACKAGE_DETAILS_ATTRIBUTE(name) (&package_details_attr_##name)
+
+static ssize_t package_details_appid_show(struct config_item *item, char *page)
+{
+	return scnprintf(page, PAGE_SIZE, "%u\n", from_kuid(current_user_ns(),
+				__get_appid(&to_package_details(item)->name)));
+}
+
+static ssize_t package_details_appid_store(struct config_item *item,
+					   const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+	kuid_t uid;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+
+	uid = make_kuid(current_user_ns(), tmp);
+
+	ret = insert_packagelist_entry(&to_package_details(item)->name, uid);
+
+	if (ret)
+		return ret;
+
+	return count;
+}
+
+static ssize_t package_details_excluded_userids_show(struct config_item *item,
+						     char *page)
+{
+	struct package_details *package_details = to_package_details(item);
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = package_details->name.hash;
+	int count = 0;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(&package_details->name, &hash_cur->key))
+			count += scnprintf(page + count, PAGE_SIZE - count,
+					   "%d ", atomic_read(&hash_cur->value));
+	}
+	rcu_read_unlock();
+	if (count)
+		count--;
+	count += scnprintf(page + count, PAGE_SIZE - count, "\n");
+	return count;
+}
+
+static ssize_t package_details_excluded_userids_store(struct config_item *item,
+						      const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+
+	ret = insert_userid_exclude_entry(&to_package_details(item)->name, tmp);
+
+	if (ret)
+		return ret;
+
+	return count;
+}
+
+static ssize_t package_details_clear_userid_store(struct config_item *item,
+						  const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+	remove_userid_exclude_entry(&to_package_details(item)->name, tmp);
+	return count;
+}
+
+static void package_details_release(struct config_item *item)
+{
+	struct package_details *package_details = to_package_details(item);
+
+	pr_debug("pkglist: removing %s\n", package_details->name.name);
+	remove_packagelist_entry(&package_details->name);
+	kfree(package_details->name.name);
+	kfree(package_details);
+}
+
+PACKAGE_DETAILS_ATTR(package_details_, appid);
+PACKAGE_DETAILS_ATTR(package_details_, excluded_userids);
+PACKAGE_DETAILS_ATTR_WO(package_details_, clear_userid);
+
+static struct configfs_attribute *package_details_attrs[] = {
+	PACKAGE_DETAILS_ATTRIBUTE(appid),
+	PACKAGE_DETAILS_ATTRIBUTE(excluded_userids),
+	PACKAGE_DETAILS_ATTRIBUTE(clear_userid),
+	NULL,
+};
+
+static struct configfs_item_operations package_details_item_ops = {
+	.release = package_details_release,
+};
+
+static struct config_item_type package_appid_type = {
+	.ct_item_ops	= &package_details_item_ops,
+	.ct_attrs	= package_details_attrs,
+	.ct_owner	= THIS_MODULE,
+};
+
+static inline struct extensions_value *to_extensions_value(
+					struct config_item *item)
+{
+	return item ? container_of(to_config_group(item),
+				struct extensions_value, group)
+			: NULL;
+}
+
+static inline struct extension_details *to_extension_details(
+					struct config_item *item)
+{
+	return item ? container_of(item, struct extension_details, item)
+			: NULL;
+}
+
+#define EXTENSIONS_VALUE_ATTRIBUTE(name) (&extensions_value_attr_##name)
+
+static void extension_details_release(struct config_item *item)
+{
+	struct extension_details *ed = to_extension_details(item);
+
+	pr_debug("pkglist: No longer mapping %s files to gid %d\n",
+				ed->name.name,
+				from_kgid(current_user_ns(), ed->value->gid));
+	remove_ext_gid_entry(ed);
+	kfree(ed->name.name);
+	kfree(ed);
+}
+
+static struct configfs_item_operations extension_details_item_ops = {
+	.release = extension_details_release,
+};
+
+static ssize_t extensions_value_ext_gid_show(
+			struct config_item *item, char *page)
+{
+	return scnprintf(page, PAGE_SIZE, "%u\n",
+				from_kgid(current_user_ns(), to_extensions_value(item)->gid));
+}
+
+static ssize_t extensions_value_ext_gid_store(
+				struct config_item *item,
+				const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+
+	to_extensions_value(item)->gid = make_kgid(current_user_ns(), tmp);
+
+	return count;
+}
+
+PACKAGE_DETAILS_ATTR(extensions_value_, ext_gid);
+
+static struct configfs_attribute *extensions_value_attrs[] = {
+	EXTENSIONS_VALUE_ATTRIBUTE(ext_gid),
+	NULL,
+};
+
+static struct config_item_type extension_details_type = {
+	.ct_item_ops = &extension_details_item_ops,
+	.ct_owner = THIS_MODULE,
+};
+
+static struct config_item *extension_details_make_item(
+				struct config_group *group, const char *name)
+{
+	struct extensions_value *extensions_value =
+			to_extensions_value(&group->cg_item);
+	struct extension_details *extension_details =
+			kzalloc(sizeof(struct extension_details), GFP_KERNEL);
+	const char *tmp;
+	int ret;
+
+	if (!extension_details)
+		return ERR_PTR(-ENOMEM);
+
+	tmp = kstrdup(name, GFP_KERNEL);
+	if (!tmp) {
+		kfree(extension_details);
+		return ERR_PTR(-ENOMEM);
+	}
+	qstr_init(&extension_details->name, tmp);
+	extension_details->value = extensions_value;
+	ret = insert_ext_gid_entry(extension_details);
+
+	if (ret) {
+		kfree(extension_details->name.name);
+		kfree(extension_details);
+		return ERR_PTR(ret);
+	}
+	config_item_init_type_name(&extension_details->item, name,
+					&extension_details_type);
+
+	return &extension_details->item;
+}
+
+static struct configfs_group_operations extensions_value_group_ops = {
+	.make_item = extension_details_make_item,
+};
+
+static struct config_item_type extensions_name_type = {
+	.ct_attrs	= extensions_value_attrs,
+	.ct_group_ops	= &extensions_value_group_ops,
+	.ct_owner	= THIS_MODULE,
+};
+
+static struct config_group *extensions_make_group(struct config_group *group,
+							const char *name)
+{
+	struct extensions_value *extensions_value;
+	unsigned int tmp;
+	int ret;
+
+	extensions_value = kzalloc(sizeof(struct extensions_value), GFP_KERNEL);
+	if (!extensions_value)
+		return ERR_PTR(-ENOMEM);
+	/* For legacy reasons, if the name is a number, assume it's the gid*/
+	ret = kstrtouint(name, 10, &tmp);
+	if (!ret)
+		extensions_value->gid = make_kgid(current_user_ns(), tmp);
+
+	config_group_init_type_name(&extensions_value->group, name,
+						&extensions_name_type);
+	return &extensions_value->group;
+}
+
+static void extensions_drop_group(struct config_group *group,
+					struct config_item *item)
+{
+	struct extensions_value *value = to_extensions_value(item);
+
+	pr_debug("pkglist: No longer mapping any files to gid %d\n",
+			from_kgid(current_user_ns(), value->gid));
+	kfree(value);
+}
+
+static struct configfs_group_operations extensions_group_ops = {
+	.make_group	= extensions_make_group,
+	.drop_item	= extensions_drop_group,
+};
+
+static struct config_item_type extensions_type = {
+	.ct_group_ops	= &extensions_group_ops,
+	.ct_owner	= THIS_MODULE,
+};
+
+static struct config_group extension_group = {
+	.cg_item = {
+		.ci_namebuf = "extensions",
+		.ci_type = &extensions_type,
+	},
+};
+
+struct packages {
+	struct configfs_subsystem subsystem;
+};
+
+static inline struct packages *to_packages(struct config_item *item)
+{
+	return item ? container_of(
+			to_configfs_subsystem(to_config_group(item)),
+					struct packages, subsystem) : NULL;
+}
+
+static struct config_item *packages_make_item(struct config_group *group,
+							const char *name)
+{
+	struct package_details *package_details;
+	const char *tmp;
+
+	package_details = kzalloc(sizeof(struct package_details), GFP_KERNEL);
+	if (!package_details)
+		return ERR_PTR(-ENOMEM);
+	tmp = kstrdup(name, GFP_KERNEL);
+	if (!tmp) {
+		kfree(package_details);
+		return ERR_PTR(-ENOMEM);
+	}
+	qstr_init(&package_details->name, tmp);
+	config_item_init_type_name(&package_details->item, name,
+						&package_appid_type);
+
+	return &package_details->item;
+}
+
+static ssize_t packages_list_show(struct config_item *item, char *page)
+{
+	struct hashtable_entry *hash_cur_app;
+	struct hashtable_entry *hash_cur_user;
+	int i;
+	int count = 0, written = 0;
+	const char errormsg[] = "<truncated>\n";
+	unsigned int hash;
+
+	rcu_read_lock();
+	hash_for_each_rcu(package_to_appid, i, hash_cur_app, hlist) {
+		written = scnprintf(page + count,
+				    PAGE_SIZE - sizeof(errormsg) - count,
+				    "%s %d\n",
+				    hash_cur_app->key.name,
+				    atomic_read(&hash_cur_app->value));
+		hash = hash_cur_app->key.hash;
+		hash_for_each_possible_rcu(package_to_userid, hash_cur_user, hlist, hash) {
+			if (qstr_case_eq(&hash_cur_app->key, &hash_cur_user->key)) {
+				written += scnprintf(page + count + written - 1,
+					PAGE_SIZE - sizeof(errormsg) - count - written + 1,
+					" %d\n", atomic_read(&hash_cur_user->value)) - 1;
+			}
+		}
+		if (count + written == PAGE_SIZE - sizeof(errormsg) - 1) {
+			count += scnprintf(page + count, PAGE_SIZE - count, errormsg);
+			break;
+		}
+		count += written;
+	}
+	rcu_read_unlock();
+
+	return count;
+}
+
+static ssize_t packages_remove_userid_store(struct config_item *item,
+					    const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+	remove_userid_all_entry(tmp);
+	return count;
+}
+
+static struct configfs_attribute packages_attr_packages_gid_list = {
+    .ca_name	= "packages_gid.list",
+    .ca_mode	= S_IRUGO,
+    .ca_owner	= THIS_MODULE,
+    .show	= packages_list_show,
+};
+PACKAGE_DETAILS_ATTR_WO(packages_, remove_userid);
+
+static struct configfs_attribute *packages_attrs[] = {
+	&packages_attr_packages_gid_list,
+	&packages_attr_remove_userid,
+	NULL,
+};
+
+/*
+ * Note that, since no extra work is required on ->drop_item(),
+ * no ->drop_item() is provided.
+ */
+static struct configfs_group_operations packages_group_ops = {
+	.make_item	= packages_make_item,
+};
+
+static struct config_item_type packages_type = {
+	.ct_group_ops	= &packages_group_ops,
+	.ct_attrs	= packages_attrs,
+	.ct_owner	= THIS_MODULE,
+};
+
+static struct config_group *sd_default_groups[] = {
+	&extension_group,
+	NULL,
+};
+
+static struct packages pkglist_packages = {
+	.subsystem = {
+		.su_group = {
+			.cg_item = {
+				.ci_type = &packages_type,
+			},
+		},
+	},
+};
+
+static int configfs_pkglist_init(void)
+{
+	int ret, i;
+	struct configfs_subsystem *subsys = &pkglist_packages.subsystem;
+	config_item_set_name(&pkglist_packages.subsystem.su_group.cg_item,
+						pkglist_config_location);
+	config_group_init(&subsys->su_group);
+
+	for (i = 0; sd_default_groups[i]; i++) {
+		config_group_init(sd_default_groups[i]);
+		configfs_add_default_group(sd_default_groups[i], &subsys->su_group);
+	}
+	mutex_init(&subsys->su_mutex);
+	ret = configfs_register_subsystem(subsys);
+	if (ret) {
+		pr_err("Error %d while registering subsystem %s\n", ret,
+				subsys->su_group.cg_item.ci_namebuf);
+	}
+	return ret;
+}
+
+static void configfs_pkglist_exit(void)
+{
+	configfs_unregister_subsystem(&pkglist_packages.subsystem);
+}
+
+void pkglist_register_update_listener(struct pkg_list *pkg)
+{
+	if (!pkg->update)
+		return;
+	mutex_lock(&pkg_list_lock);
+	list_add(&pkg->list, &pkglist_listeners);
+	mutex_unlock(&pkg_list_lock);
+}
+EXPORT_SYMBOL_GPL(pkglist_register_update_listener);
+
+void pkglist_unregister_update_listener(struct pkg_list *pkg)
+{
+	mutex_lock(&pkg_list_lock);
+	list_del(&pkg->list);
+	mutex_unlock(&pkg_list_lock);
+}
+EXPORT_SYMBOL_GPL(pkglist_unregister_update_listener);
+
+static int __init pkglist_init(void)
+{
+	hashtable_entry_cachep =
+		kmem_cache_create("packagelist_hashtable_entry",
+				sizeof(struct hashtable_entry), 0, 0, NULL);
+	if (!hashtable_entry_cachep) {
+		pr_err("pkglist: failed creating pkgl_hashtable entry slab cache\n");
+		return -ENOMEM;
+	}
+
+	return configfs_pkglist_init();
+}
+module_init(pkglist_init);
+
+static void __exit pkglist_exit(void)
+{
+	configfs_pkglist_exit();
+	packagelist_destroy();
+	kmem_cache_destroy(hashtable_entry_cachep);
+}
+
+module_exit(pkglist_exit);
+
+MODULE_AUTHOR("Daniel Rosenberg, Google");
+MODULE_DESCRIPTION("Configfs Pkglist implementation");
+MODULE_LICENSE("GPL v2");
Index: rpi4-kernel/drivers/pkglist/pkglist_none.c
===================================================================
--- /dev/null
+++ rpi4-kernel/drivers/pkglist/pkglist_none.c
@@ -0,0 +1,57 @@
+/*
+ * Copyright (C) 2017 Google Inc., Author: Daniel Rosenberg <drosen@google.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/ctype.h>
+#include <linux/dcache.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/pkglist.h>
+
+kuid_t pkglist_get_appid(const char *key)
+{
+	return make_kuid(&init_user_ns, 0);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_appid);
+
+kgid_t pkglist_get_ext_gid(const char *key)
+{
+	return make_kgid(&init_user_ns, 0);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_ext_gid);
+
+bool pkglist_user_is_excluded(const char *key, uint32_t user)
+{
+	return false;
+}
+EXPORT_SYMBOL_GPL(pkglist_user_is_excluded);
+
+kuid_t pkglist_get_allowed_appid(const char *key, uint32_t user)
+{
+	return make_kuid(&init_user_ns, 0);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_allowed_appid);
+
+void pkglist_register_update_listener(struct pkg_list *pkg) { }
+EXPORT_SYMBOL_GPL(pkglist_register_update_listener);
+
+void pkglist_unregister_update_listener(struct pkg_list *pkg) { }
+EXPORT_SYMBOL_GPL(pkglist_unregister_update_listener);
+
+static int __init pkglist_init(void)
+{
+	return 0;
+}
+module_init(pkglist_init);
+
+static void pkglist_exit(void) { }
+
+module_exit(pkglist_exit);
+
+MODULE_AUTHOR("Daniel Rosenberg, Google");
+MODULE_DESCRIPTION("Empty Pkglist implementation");
+MODULE_LICENSE("GPL v2");
Index: rpi4-kernel/fs/Kconfig
===================================================================
--- rpi4-kernel.orig/fs/Kconfig
+++ rpi4-kernel/fs/Kconfig
@@ -290,6 +290,7 @@ source "fs/orangefs/Kconfig"
 source "fs/adfs/Kconfig"
 source "fs/affs/Kconfig"
 source "fs/ecryptfs/Kconfig"
+source "fs/esdfs/Kconfig"
 source "fs/hfs/Kconfig"
 source "fs/hfsplus/Kconfig"
 source "fs/befs/Kconfig"
Index: rpi4-kernel/fs/Makefile
===================================================================
--- rpi4-kernel.orig/fs/Makefile
+++ rpi4-kernel/fs/Makefile
@@ -86,6 +86,7 @@ obj-$(CONFIG_ISO9660_FS)	+= isofs/
 obj-$(CONFIG_HFSPLUS_FS)	+= hfsplus/ # Before hfs to find wrapped HFS+
 obj-$(CONFIG_HFS_FS)		+= hfs/
 obj-$(CONFIG_ECRYPT_FS)		+= ecryptfs/
+obj-$(CONFIG_ESD_FS)    += esdfs/
 obj-$(CONFIG_VXFS_FS)		+= freevxfs/
 obj-$(CONFIG_NFS_FS)		+= nfs/
 obj-$(CONFIG_EXPORTFS)		+= exportfs/
Index: rpi4-kernel/fs/esdfs/Kconfig
===================================================================
--- /dev/null
+++ rpi4-kernel/fs/esdfs/Kconfig
@@ -0,0 +1,7 @@
+config ESD_FS
+	tristate "Emulated 'SD card' file system for Android (EXPERIMENTAL)"
+	depends on PKGLIST
+	depends on USER_NS
+	help
+	  Esdfs is a wrapfs-based file system, designed to implement the
+	  Android "sdcard" FUSE-backed file system from within the kernel.
Index: rpi4-kernel/fs/esdfs/Makefile
===================================================================
--- /dev/null
+++ rpi4-kernel/fs/esdfs/Makefile
@@ -0,0 +1,7 @@
+ESDFS_VERSION="0.2"
+
+EXTRA_CFLAGS += -DESDFS_VERSION=\"$(ESDFS_VERSION)\"
+
+obj-$(CONFIG_ESD_FS) += esdfs.o
+
+esdfs-y := dentry.o file.o inode.o main.o super.o lookup.o mmap.o derive.o
Index: rpi4-kernel/fs/esdfs/dentry.c
===================================================================
--- /dev/null
+++ rpi4-kernel/fs/esdfs/dentry.c
@@ -0,0 +1,158 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/ctype.h>
+#include "esdfs.h"
+
+/*
+ * returns: -ERRNO if error (returned to user)
+ *          0: tell VFS to invalidate dentry
+ *          1: dentry is valid
+ */
+static int esdfs_d_revalidate(struct dentry *dentry, unsigned int flags)
+{
+	struct path lower_path;
+	struct path lower_parent_path;
+	struct dentry *parent_dentry = NULL;
+	struct dentry *lower_dentry = NULL;
+	struct dentry *lower_parent_dentry = NULL;
+	int err = 1;
+
+	if (flags & LOOKUP_RCU)
+		return -ECHILD;
+
+	/* short-circuit if it's root */
+	spin_lock(&dentry->d_lock);
+	if (IS_ROOT(dentry)) {
+		spin_unlock(&dentry->d_lock);
+		return 1;
+	}
+	spin_unlock(&dentry->d_lock);
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	esdfs_get_lower_parent(dentry, lower_dentry, &lower_parent_dentry);
+
+	parent_dentry = dget_parent(dentry);
+	esdfs_get_lower_path(parent_dentry, &lower_parent_path);
+
+	if (lower_parent_path.dentry != lower_parent_dentry)
+		goto drop;
+
+	if (lower_dentry->d_flags & DCACHE_OP_REVALIDATE) {
+		err = lower_dentry->d_op->d_revalidate(lower_dentry, flags);
+		if (err == 0)
+			goto drop;
+	}
+
+	/* can't do strcmp if lower is hashed */
+	spin_lock(&lower_dentry->d_lock);
+	if (d_unhashed(lower_dentry)) {
+		spin_unlock(&lower_dentry->d_lock);
+		goto drop;
+	}
+
+	spin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);
+
+	if (!qstr_case_eq(&lower_dentry->d_name, &dentry->d_name)) {
+		err = 0;
+		__d_drop(dentry);	/* already holding spin lock */
+	}
+
+	spin_unlock(&dentry->d_lock);
+	spin_unlock(&lower_dentry->d_lock);
+
+	esdfs_revalidate_perms(dentry);
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(dentry->d_sb)) &&
+	    esdfs_derived_revalidate(dentry, parent_dentry))
+		goto drop;
+
+	goto out;
+
+drop:
+	d_drop(dentry);
+	err = 0;
+out:
+	esdfs_put_lower_path(parent_dentry, &lower_parent_path);
+	dput(parent_dentry);
+	esdfs_put_lower_parent(dentry, &lower_parent_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	return err;
+}
+
+/* directly from fs/fat/namei_vfat.c */
+static unsigned int __vfat_striptail_len(unsigned int len, const char *name)
+{
+	while (len && name[len - 1] == '.')
+		len--;
+	return len;
+}
+
+static unsigned int vfat_striptail_len(const struct qstr *qstr)
+{
+	return __vfat_striptail_len(qstr->len, qstr->name);
+}
+
+
+/* based on vfat_hashi() in fs/fat/namei_vfat.c (no code pages) */
+static int esdfs_d_hash(const struct dentry *dentry, struct qstr *qstr)
+{
+	const unsigned char *name;
+	unsigned int len;
+	unsigned long hash;
+
+	name = qstr->name;
+	len = vfat_striptail_len(qstr);
+
+	hash = init_name_hash(dentry);
+	while (len--)
+		hash = partial_name_hash(tolower(*name++), hash);
+	qstr->hash = end_name_hash(hash);
+
+	return 0;
+}
+
+/* based on vfat_cmpi() in fs/fat/namei_vfat.c (no code pages) */
+static int esdfs_d_compare(const struct dentry *dentry, unsigned int len,
+			   const char *str, const struct qstr *name)
+{
+	unsigned int alen, blen;
+
+	/* A filename cannot end in '.' or we treat it like it has none */
+	alen = vfat_striptail_len(name);
+	blen = __vfat_striptail_len(len, str);
+	if (alen == blen) {
+		if (str_n_case_eq(name->name, str, alen))
+			return 0;
+	}
+	return 1;
+}
+
+static void esdfs_d_release(struct dentry *dentry)
+{
+	if (!dentry || !dentry->d_fsdata)
+		return;
+
+	/* release and reset the lower paths */
+	esdfs_put_reset_lower_paths(dentry);
+	esdfs_release_lower_parent(dentry);
+	esdfs_free_dentry_private_data(dentry);
+}
+
+const struct dentry_operations esdfs_dops = {
+	.d_revalidate	= esdfs_d_revalidate,
+	.d_delete	= always_delete_dentry,
+	.d_hash		= esdfs_d_hash,
+	.d_compare	= esdfs_d_compare,
+	.d_release	= esdfs_d_release,
+};
Index: rpi4-kernel/fs/esdfs/derive.c
===================================================================
--- /dev/null
+++ rpi4-kernel/fs/esdfs/derive.c
@@ -0,0 +1,608 @@
+/*
+ * Copyright (c) 2013-2014 Motorola Mobility LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/proc_fs.h>
+#include <linux/hashtable.h>
+#include <linux/syscalls.h>
+#include <linux/fcntl.h>
+#include <linux/ctype.h>
+#include <linux/vmalloc.h>
+#include <linux/security.h>
+#include <linux/uaccess.h>
+#include "esdfs.h"
+
+static struct qstr names_secure[] = {
+	QSTR_LITERAL("autorun.inf"),
+	QSTR_LITERAL(".android_secure"),
+	QSTR_LITERAL("android_secure"),
+	QSTR_LITERAL("")
+};
+
+/* special path name searches */
+static inline bool match_name(struct qstr *name, struct qstr names[])
+{
+	int i = 0;
+
+	BUG_ON(!name);
+	for (i = 0; *names[i].name; i++)
+		if (qstr_case_eq(name, &names[i]))
+			return true;
+
+	return false;
+}
+
+unsigned esdfs_package_list_version;
+
+static void fixup_perms_by_flag(int flags, const struct qstr *key,
+					uint32_t userid)
+{
+	esdfs_package_list_version++;
+}
+
+static struct pkg_list esdfs_pkg_list = {
+		.update = fixup_perms_by_flag,
+};
+
+int esdfs_init_package_list(void)
+{
+	pkglist_register_update_listener(&esdfs_pkg_list);
+	return 0;
+}
+
+void esdfs_destroy_package_list(void)
+{
+	pkglist_unregister_update_listener(&esdfs_pkg_list);
+}
+
+/*
+ * Derive an entry's premissions tree position based on its parent.
+ */
+void esdfs_derive_perms(struct dentry *dentry)
+{
+	struct esdfs_inode_info *inode_i = ESDFS_I(dentry->d_inode);
+	bool is_root;
+	int __maybe_unused ret;
+	kuid_t appid;
+	struct qstr q_Download = QSTR_LITERAL("Download");
+	struct qstr q_Android = QSTR_LITERAL("Android");
+	struct qstr q_data = QSTR_LITERAL("data");
+	struct qstr q_obb = QSTR_LITERAL("obb");
+	struct qstr q_media = QSTR_LITERAL("media");
+	struct qstr q_cache = QSTR_LITERAL("cache");
+	struct qstr q_user = QSTR_LITERAL("user");
+	struct esdfs_inode_info *parent_i = ESDFS_I(dentry->d_parent->d_inode);
+
+	spin_lock(&dentry->d_lock);
+	is_root = IS_ROOT(dentry);
+	spin_unlock(&dentry->d_lock);
+	if (is_root)
+		return;
+
+	/* Inherit from the parent to start */
+	inode_i->tree = parent_i->tree;
+	inode_i->userid = parent_i->userid;
+	inode_i->appid = parent_i->appid;
+	inode_i->under_obb = parent_i->under_obb;
+
+	/*
+	 * ESDFS_TREE_MEDIA* are intentionally dead ends.
+	 */
+	switch (inode_i->tree) {
+	case ESDFS_TREE_ROOT_LEGACY:
+		inode_i->tree = ESDFS_TREE_ROOT;
+		ret = kstrtou32(dentry->d_name.name, 0, &inode_i->userid);
+		if (qstr_case_eq(&dentry->d_name, &q_obb))
+			inode_i->tree = ESDFS_TREE_ANDROID_OBB;
+		break;
+
+	case ESDFS_TREE_ROOT:
+		inode_i->tree = ESDFS_TREE_MEDIA;
+		if (qstr_case_eq(&dentry->d_name, &q_Download))
+			inode_i->tree = ESDFS_TREE_DOWNLOAD;
+		else if (qstr_case_eq(&dentry->d_name, &q_Android))
+			inode_i->tree = ESDFS_TREE_ANDROID;
+		break;
+
+	case ESDFS_TREE_ANDROID:
+		if (qstr_case_eq(&dentry->d_name, &q_data)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_DATA;
+		} else if (qstr_case_eq(&dentry->d_name, &q_obb)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_OBB;
+			inode_i->under_obb = true;
+		} else if (qstr_case_eq(&dentry->d_name, &q_media)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_MEDIA;
+		} else if (ESDFS_RESTRICT_PERMS(ESDFS_SB(dentry->d_sb)) &&
+			 qstr_case_eq(&dentry->d_name, &q_user)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_USER;
+		}
+		break;
+
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_OBB:
+	case ESDFS_TREE_ANDROID_MEDIA:
+		appid = pkglist_get_allowed_appid(dentry->d_name.name,
+						inode_i->userid);
+		if (uid_valid(appid))
+			inode_i->appid = esdfs_from_kuid(
+					ESDFS_SB(dentry->d_sb), appid);
+		else
+			inode_i->appid = 0;
+		inode_i->tree = ESDFS_TREE_ANDROID_APP;
+		break;
+	case ESDFS_TREE_ANDROID_APP:
+		if (qstr_case_eq(&dentry->d_name, &q_cache))
+			inode_i->tree = ESDFS_TREE_ANDROID_APP_CACHE;
+		break;
+	case ESDFS_TREE_ANDROID_USER:
+		/* Another user, so start over */
+		inode_i->tree = ESDFS_TREE_ROOT;
+		ret = kstrtou32(dentry->d_name.name, 0, &inode_i->userid);
+		break;
+	}
+}
+
+/* Apply tree position-specific permissions */
+void esdfs_set_derived_perms(struct inode *inode)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(inode->i_sb);
+	struct esdfs_inode_info *inode_i = ESDFS_I(inode);
+	gid_t gid = sbi->upper_perms.gid;
+
+	esdfs_i_uid_write(inode, sbi->upper_perms.uid);
+	inode->i_mode &= S_IFMT;
+	if (ESDFS_RESTRICT_PERMS(sbi))
+		esdfs_i_gid_write(inode, gid);
+	else {
+		if (gid == AID_SDCARD_RW && !test_opt(sbi, DEFAULT_NORMAL))
+			esdfs_i_gid_write(inode, AID_SDCARD_RW);
+		else
+			esdfs_i_gid_write(inode, derive_uid(inode_i, gid));
+		inode->i_mode |= sbi->upper_perms.dmask;
+	}
+
+	switch (inode_i->tree) {
+	case ESDFS_TREE_ROOT_LEGACY:
+		if (ESDFS_RESTRICT_PERMS(sbi))
+			inode->i_mode |= sbi->upper_perms.dmask;
+		else if (test_opt(sbi, DERIVE_MULTI)) {
+			inode->i_mode &= S_IFMT;
+			inode->i_mode |= 0711;
+		}
+		break;
+
+	case ESDFS_TREE_NONE:
+	case ESDFS_TREE_ROOT:
+		if (ESDFS_RESTRICT_PERMS(sbi)) {
+			esdfs_i_gid_write(inode, AID_SDCARD_R);
+			inode->i_mode |= sbi->upper_perms.dmask;
+		} else if (test_opt(sbi, DERIVE_PUBLIC) &&
+			   test_opt(ESDFS_SB(inode->i_sb), DERIVE_CONFINE)) {
+			inode->i_mode &= S_IFMT;
+			inode->i_mode |= 0771;
+		}
+		break;
+
+	case ESDFS_TREE_MEDIA:
+		if (ESDFS_RESTRICT_PERMS(sbi)) {
+			esdfs_i_gid_write(inode, AID_SDCARD_R);
+			inode->i_mode |= 0770;
+		}
+		break;
+
+	case ESDFS_TREE_DOWNLOAD:
+	case ESDFS_TREE_ANDROID:
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_OBB:
+	case ESDFS_TREE_ANDROID_MEDIA:
+		if (ESDFS_RESTRICT_PERMS(sbi))
+			inode->i_mode |= 0771;
+		break;
+
+	case ESDFS_TREE_ANDROID_APP:
+	case ESDFS_TREE_ANDROID_APP_CACHE:
+		if (inode_i->appid)
+			esdfs_i_uid_write(inode, derive_uid(inode_i,
+							inode_i->appid));
+		if (ESDFS_RESTRICT_PERMS(sbi))
+			inode->i_mode |= 0770;
+		break;
+
+	case ESDFS_TREE_ANDROID_USER:
+		if (ESDFS_RESTRICT_PERMS(sbi)) {
+			esdfs_i_gid_write(inode, AID_SDCARD_ALL);
+			inode->i_mode |= 0770;
+		}
+		inode->i_mode |= 0770;
+		break;
+	}
+
+	/* strip execute bits from any non-directories */
+	if (!S_ISDIR(inode->i_mode))
+		inode->i_mode &= ~S_IXUGO;
+}
+
+/*
+ * Before rerouting a lookup to follow a pseudo hard link, make sure that
+ * a stub exists at the source.  Without it, readdir won't see an entry there
+ * resulting in a strange user experience.
+ */
+static int lookup_link_source(struct dentry *dentry, struct dentry *parent)
+{
+	struct path lower_parent_path, lower_path;
+	int err;
+
+	esdfs_get_lower_path(parent, &lower_parent_path);
+
+	/* Check if the stub user profile folder is there. */
+	err = esdfs_lookup_nocase(&lower_parent_path, &dentry->d_name,
+					&lower_path);
+	/* Remember it to handle renames and removal. */
+	if (!err)
+		esdfs_set_lower_stub_path(dentry, &lower_path);
+
+	esdfs_put_lower_path(parent, &lower_parent_path);
+
+	return err;
+}
+
+int esdfs_is_dl_lookup(struct dentry *dentry, struct dentry *parent)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(parent->d_sb);
+	struct esdfs_inode_info *parent_i = ESDFS_I(parent->d_inode);
+	/*
+	 * Return 1 if this is the Download directory:
+	 * The test for download checks:
+	 * 1. The parent is the mount root.
+	 * 2. The directory is named 'Download'.
+	 * 3. The stub for the directory exists.
+	 */
+	if (test_opt(sbi, SPECIAL_DOWNLOAD) &&
+			parent_i->tree == ESDFS_TREE_ROOT &&
+			ESDFS_DENTRY_NEEDS_DL_LINK(dentry) &&
+			lookup_link_source(dentry, parent) == 0) {
+		return 1;
+	}
+
+	return 0;
+}
+
+int esdfs_derived_lookup(struct dentry *dentry, struct dentry **parent)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB((*parent)->d_sb);
+	struct esdfs_inode_info *parent_i = ESDFS_I((*parent)->d_inode);
+	struct qstr q_Android = QSTR_LITERAL("Android");
+
+	/* Deny access to security-sensitive entries. */
+	if (ESDFS_I((*parent)->d_inode)->tree == ESDFS_TREE_ROOT &&
+	    match_name(&dentry->d_name, names_secure)) {
+		pr_debug("esdfs: denying access to: %s", dentry->d_name.name);
+		return -EACCES;
+	}
+
+	/* Pin the unified mode obb link parent as it flies by. */
+	if (!sbi->obb_parent &&
+	    test_opt(sbi, DERIVE_UNIFIED) &&
+	    parent_i->tree == ESDFS_TREE_ROOT &&
+	    parent_i->userid == 0 &&
+	    qstr_case_eq(&dentry->d_name, &q_Android))
+		sbi->obb_parent = dget(dentry);		/* keep it pinned */
+
+	/*
+	 * Handle obb directory "grafting" as a pseudo hard link by overriding
+	 * its parent to point to the target obb directory's parent.  The rest
+	 * of the lookup process will take care of setting up the bottom half
+	 * to point to the real obb directory.
+	 */
+	if (parent_i->tree == ESDFS_TREE_ANDROID &&
+	    ESDFS_DENTRY_NEEDS_LINK(dentry) &&
+	    lookup_link_source(dentry, *parent) == 0) {
+		BUG_ON(!sbi->obb_parent);
+		if (ESDFS_INODE_CAN_LINK((*parent)->d_inode))
+			*parent = dget(sbi->obb_parent);
+	}
+
+	return 0;
+}
+
+int esdfs_derived_revalidate(struct dentry *dentry, struct dentry *parent)
+{
+	/*
+	 * If obb is not linked yet, it means the dentry is pointing to the
+	 * stub.  Invalidate the dentry to force another lookup.
+	 */
+	if (ESDFS_I(parent->d_inode)->tree == ESDFS_TREE_ANDROID &&
+	    ESDFS_INODE_CAN_LINK(dentry->d_inode) &&
+	    ESDFS_DENTRY_NEEDS_LINK(dentry) &&
+	    !ESDFS_DENTRY_IS_LINKED(dentry))
+		return -ESTALE;
+	if (ESDFS_I(parent->d_inode)->tree == ESDFS_TREE_ROOT &&
+	    ESDFS_DENTRY_NEEDS_DL_LINK(dentry) &&
+	    !ESDFS_DENTRY_IS_LINKED(dentry))
+		return -ESTALE;
+	return 0;
+}
+
+/*
+ * Implement the extra checking that is done based on the caller's package
+ * list-based access rights.
+ */
+int esdfs_check_derived_permission(struct inode *inode, int mask)
+{
+	const struct cred *cred;
+	uid_t uid, appid;
+
+	/*
+	 * If we don't need to restrict access based on app GIDs and confine
+	 * writes to outside of the Android/... tree, we can skip all of this.
+	 */
+	if (!ESDFS_RESTRICT_PERMS(ESDFS_SB(inode->i_sb)) &&
+	    !test_opt(ESDFS_SB(inode->i_sb), DERIVE_CONFINE))
+			return 0;
+
+	cred = current_cred();
+	uid = from_kuid(&init_user_ns, cred->uid);
+	appid = uid % PKG_APPID_PER_USER;
+
+	/* Reads, owners, and root are always granted access */
+	if (!(mask & (MAY_WRITE | ESDFS_MAY_CREATE)) ||
+	    uid == 0 || uid_eq(cred->uid, inode->i_uid))
+		return 0;
+
+	/*
+	 * Grant access to sdcard_rw holders, unless we are in unified mode
+	 * and we are trying to write to the protected /Android tree or to
+	 * create files in the root (aka, "confined" access).
+	 */
+	if ((!test_opt(ESDFS_SB(inode->i_sb), DERIVE_UNIFIED) ||
+	     (ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_DOWNLOAD &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_DATA &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_OBB &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_MEDIA &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_APP &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_APP_CACHE &&
+	      (ESDFS_I(inode)->tree != ESDFS_TREE_ROOT ||
+	       !(mask & ESDFS_MAY_CREATE)))))
+		return 0;
+
+	pr_debug("esdfs: %s: denying access to appid: %u\n", __func__, appid);
+	return -EACCES;
+}
+
+static gid_t get_type(struct esdfs_sb_info *sbi, const char *name)
+{
+	const char *ext = strrchr(name, '.');
+	kgid_t id;
+
+	if (ext && ext[0]) {
+		ext = &ext[1];
+		id = pkglist_get_ext_gid(ext);
+		return gid_valid(id)?esdfs_from_kgid(sbi, id):AID_MEDIA_RW;
+	}
+	return AID_MEDIA_RW;
+}
+
+static kuid_t esdfs_get_derived_lower_uid(struct esdfs_sb_info *sbi,
+				struct esdfs_inode_info *info)
+{
+	uid_t uid = sbi->lower_perms.uid;
+	int perm;
+
+	perm = info->tree;
+	if (info->under_obb)
+		perm = ESDFS_TREE_ANDROID_OBB;
+
+	switch (perm) {
+	case ESDFS_TREE_DOWNLOAD:
+		if (test_opt(sbi, SPECIAL_DOWNLOAD))
+			return make_kuid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_uid);
+		fallthrough;
+	case ESDFS_TREE_ROOT:
+	case ESDFS_TREE_MEDIA:
+	case ESDFS_TREE_ANDROID:
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_MEDIA:
+	case ESDFS_TREE_ANDROID_APP:
+	case ESDFS_TREE_ANDROID_APP_CACHE:
+		uid = derive_uid(info, uid);
+		break;
+	case ESDFS_TREE_ANDROID_OBB:
+		uid = AID_MEDIA_OBB;
+		break;
+	case ESDFS_TREE_ROOT_LEGACY:
+	default:
+		break;
+	}
+	return esdfs_make_kuid(sbi, uid);
+}
+
+static kgid_t esdfs_get_derived_lower_gid(struct esdfs_sb_info *sbi,
+				struct esdfs_inode_info *info, const char *name)
+{
+	gid_t gid = sbi->lower_perms.gid;
+	uid_t upper_uid;
+	int perm;
+
+	upper_uid = esdfs_i_uid_read(&info->vfs_inode);
+	perm = info->tree;
+	if (info->under_obb)
+		perm = ESDFS_TREE_ANDROID_OBB;
+
+	switch (perm) {
+	case ESDFS_TREE_DOWNLOAD:
+		if (test_opt(sbi, SPECIAL_DOWNLOAD))
+			return make_kgid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_gid);
+		fallthrough;
+	case ESDFS_TREE_ROOT:
+	case ESDFS_TREE_MEDIA:
+	case ESDFS_TREE_ANDROID:
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_MEDIA:
+		if (S_ISDIR(info->vfs_inode.i_mode))
+			gid = derive_uid(info, AID_MEDIA_RW);
+		else
+			gid = derive_uid(info, get_type(sbi, name));
+		break;
+	case ESDFS_TREE_ANDROID_OBB:
+		gid = AID_MEDIA_OBB;
+		break;
+	case ESDFS_TREE_ANDROID_APP:
+		if (uid_is_app(upper_uid))
+			gid = multiuser_get_ext_gid(upper_uid);
+		else
+			gid = derive_uid(info, AID_MEDIA_RW);
+		break;
+	case ESDFS_TREE_ANDROID_APP_CACHE:
+		if (uid_is_app(upper_uid))
+			gid = multiuser_get_ext_cache_gid(upper_uid);
+		else
+			gid = derive_uid(info, AID_MEDIA_RW);
+		break;
+	case ESDFS_TREE_ROOT_LEGACY:
+	default:
+		break;
+	}
+	return esdfs_make_kgid(sbi, gid);
+}
+
+void esdfs_derive_lower_ownership(struct dentry *dentry, const char *name)
+{
+	struct path path;
+	struct inode *inode;
+	struct inode *delegated_inode = NULL;
+	int error;
+	struct esdfs_sb_info *sbi = ESDFS_SB(dentry->d_sb);
+	struct esdfs_inode_info *info = ESDFS_I(dentry->d_inode);
+	kuid_t kuid;
+	kgid_t kgid;
+	struct iattr newattrs;
+
+	if (!test_opt(sbi, GID_DERIVATION))
+		return;
+
+	esdfs_get_lower_path(dentry, &path);
+	inode = path.dentry->d_inode;
+	kuid = esdfs_get_derived_lower_uid(sbi, info);
+	kgid = esdfs_get_derived_lower_gid(sbi, info, name);
+	if (!gid_eq(path.dentry->d_inode->i_gid, kgid)
+		|| !uid_eq(path.dentry->d_inode->i_uid, kuid)) {
+retry_deleg:
+		newattrs.ia_valid = ATTR_GID | ATTR_UID | ATTR_FORCE;
+		newattrs.ia_uid = kuid;
+		newattrs.ia_gid = kgid;
+		if (!S_ISDIR(inode->i_mode))
+			newattrs.ia_valid |= ATTR_KILL_SUID | ATTR_KILL_SGID
+						| ATTR_KILL_PRIV;
+		inode_lock(inode);
+		error = security_path_chown(&path, newattrs.ia_uid,
+						newattrs.ia_gid);
+		if (!error)
+			error = notify_change(&init_user_ns, path.dentry,
+						&newattrs, &delegated_inode);
+		inode_unlock(inode);
+		if (delegated_inode) {
+			error = break_deleg_wait(&delegated_inode);
+			if (!error)
+				goto retry_deleg;
+		}
+		if (error)
+			pr_debug("esdfs: Failed to touch up lower fs gid/uid for %s\n", name);
+	}
+	esdfs_put_lower_path(dentry, &path);
+}
+
+/*
+ * The sdcard service has a hack that creates .nomedia files along certain
+ * paths to stop MediaScanner.  Create those here.
+ */
+int esdfs_derive_mkdir_contents(struct dentry *dir_dentry)
+{
+	struct esdfs_inode_info *inode_i;
+	struct qstr nomedia;
+	struct dentry *lower_dentry;
+	struct path lower_dir_path, lower_path;
+	struct dentry *lower_parent_dentry = NULL;
+	umode_t mode;
+	int err = 0;
+	const struct cred *creds;
+	int mask = 0;
+
+	if (!dir_dentry->d_inode)
+		return 0;
+
+	inode_i = ESDFS_I(dir_dentry->d_inode);
+
+	/*
+	 * Only create .nomedia in Android/data and Android/obb, but never in
+	 * pseudo link stubs.
+	 */
+	if ((inode_i->tree != ESDFS_TREE_ANDROID_DATA &&
+	     inode_i->tree != ESDFS_TREE_ANDROID_OBB) ||
+	    (ESDFS_INODE_CAN_LINK(dir_dentry->d_inode) &&
+	     ESDFS_DENTRY_NEEDS_LINK(dir_dentry) &&
+	     !ESDFS_DENTRY_IS_LINKED(dir_dentry)))
+		return 0;
+
+	esdfs_get_lower_path(dir_dentry, &lower_dir_path);
+
+	nomedia.name = ".nomedia";
+	nomedia.len = strlen(nomedia.name);
+	nomedia.hash = full_name_hash(lower_dir_path.dentry, nomedia.name,
+				      nomedia.len);
+
+	/* check if lower has its own hash */
+	if (lower_dir_path.dentry->d_flags & DCACHE_OP_HASH)
+		lower_dir_path.dentry->d_op->d_hash(lower_dir_path.dentry,
+							&nomedia);
+
+	creds = esdfs_override_creds(ESDFS_SB(dir_dentry->d_sb),
+					inode_i, &mask);
+	/* See if the lower file is there already. */
+	err = vfs_path_lookup(lower_dir_path.dentry, lower_dir_path.mnt,
+			      nomedia.name, 0, &lower_path);
+	if (!err)
+		path_put(&lower_path);
+	/* If it's there or there was an error, we're done */
+	if (!err || err != -ENOENT)
+		goto out;
+
+	/* The lower file is not there.  See if the dentry is in the cache. */
+	lower_dentry = d_lookup(lower_dir_path.dentry, &nomedia);
+	if (!lower_dentry) {
+		/* It's not there, so create a negative lower dentry. */
+		lower_dentry = d_alloc(lower_dir_path.dentry, &nomedia);
+		if (!lower_dentry) {
+			err = -ENOMEM;
+			goto out;
+		}
+		d_add(lower_dentry, NULL);
+	}
+
+	/* Now create the lower file. */
+	mode = S_IFREG;
+	lower_parent_dentry = lock_parent(lower_dentry);
+	esdfs_set_lower_mode(ESDFS_SB(dir_dentry->d_sb), inode_i, &mode);
+	err = vfs_create(&init_user_ns, lower_dir_path.dentry->d_inode,
+			 lower_dentry, mode, true);
+	unlock_dir(lower_parent_dentry);
+	dput(lower_dentry);
+
+out:
+	esdfs_put_lower_path(dir_dentry, &lower_dir_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
Index: rpi4-kernel/fs/esdfs/esdfs.h
===================================================================
--- /dev/null
+++ rpi4-kernel/fs/esdfs/esdfs.h
@@ -0,0 +1,627 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef _ESDFS_H_
+#define _ESDFS_H_
+
+#include <linux/dcache.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/iversion.h>
+#include <linux/aio.h>
+#include <linux/mm.h>
+#include <linux/mount.h>
+#include <uapi/linux/mount.h>
+#include <linux/namei.h>
+#include <linux/seq_file.h>
+#include <linux/statfs.h>
+#include <linux/fs_stack.h>
+#include <linux/magic.h>
+#include <linux/uaccess.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/fs_struct.h>
+#include <linux/uidgid.h>
+#include <linux/user_namespace.h>
+#include <linux/pkglist.h>
+
+#include "../internal.h"
+
+/* the file system name */
+#define ESDFS_NAME "esdfs"
+
+/* ioctl command */
+#define ESDFS_IOCTL_MAGIC	'e'
+#define ESDFS_IOC_DIS_ACCESS	_IO(ESDFS_IOCTL_MAGIC, 1)
+
+/* esdfs root inode number */
+#define ESDFS_ROOT_INO     1
+
+/* useful for tracking code reachability */
+#define UDBG printk(KERN_DEFAULT "DBG:%s:%s:%d\n", __FILE__, __func__, __LINE__)
+
+/* mount options */
+#define ESDFS_MOUNT_DERIVE_LEGACY	0x00000001
+#define ESDFS_MOUNT_DERIVE_UNIFIED	0x00000002
+#define ESDFS_MOUNT_DERIVE_MULTI	0x00000004
+#define ESDFS_MOUNT_DERIVE_PUBLIC	0x00000008
+#define ESDFS_MOUNT_DERIVE_CONFINE	0x00000010
+#define ESDFS_MOUNT_ACCESS_DISABLE	0x00000020
+#define ESDFS_MOUNT_GID_DERIVATION	0x00000040
+#define ESDFS_MOUNT_DEFAULT_NORMAL	0x00000080
+#define ESDFS_MOUNT_SPECIAL_DOWNLOAD	0x00000100
+
+#define clear_opt(sbi, option)	(sbi->options &= ~ESDFS_MOUNT_##option)
+#define set_opt(sbi, option)	(sbi->options |= ESDFS_MOUNT_##option)
+#define test_opt(sbi, option)	(sbi->options & ESDFS_MOUNT_##option)
+
+#define ESDFS_DERIVE_PERMS(sbi)	(test_opt(sbi, DERIVE_UNIFIED) || \
+				 test_opt(sbi, DERIVE_LEGACY))
+#define ESDFS_RESTRICT_PERMS(sbi) (ESDFS_DERIVE_PERMS(sbi) && \
+				   !test_opt(sbi, DERIVE_PUBLIC) && \
+				   !test_opt(sbi, DERIVE_MULTI))
+
+/* from android_filesystem_config.h */
+#define AID_ROOT             0
+#define AID_SDCARD_RW     1015
+#define AID_MEDIA_RW      1023
+#define AID_SDCARD_R      1028
+#define AID_SDCARD_PICS   1033
+#define AID_SDCARD_AV     1034
+#define AID_SDCARD_ALL    1035
+#define AID_MEDIA_OBB     1059
+
+/* used in extra persmission check during file creation */
+#define ESDFS_MAY_CREATE	0x00001000
+
+/* derived permissions model based on tree location */
+enum {
+	ESDFS_TREE_NONE = 0,		/* permissions not derived */
+	ESDFS_TREE_ROOT_LEGACY,		/* root for legacy emulated storage */
+	ESDFS_TREE_ROOT,		/* root for a user */
+	ESDFS_TREE_MEDIA,		/* per-user basic permissions */
+	ESDFS_TREE_DOWNLOAD,		/* .../Download */
+	ESDFS_TREE_ANDROID,		/* .../Android */
+	ESDFS_TREE_ANDROID_DATA,	/* .../Android/data */
+	ESDFS_TREE_ANDROID_OBB,		/* .../Android/obb */
+	ESDFS_TREE_ANDROID_MEDIA,	/* .../Android/media */
+	ESDFS_TREE_ANDROID_APP,		/* .../Android/data|obb|media/... */
+	ESDFS_TREE_ANDROID_APP_CACHE,	/* .../Android/data|obb|media/.../cache */
+	ESDFS_TREE_ANDROID_USER,	/* .../Android/user */
+};
+
+/* for permissions table lookups */
+enum {
+	ESDFS_PERMS_LOWER_DEFAULT = 0,
+	ESDFS_PERMS_UPPER_LEGACY,
+	ESDFS_PERMS_UPPER_DERIVED,
+	ESDFS_PERMS_LOWER_DOWNLOAD,
+	ESDFS_PERMS_TABLE_SIZE
+
+};
+
+#define PKG_NAME_MAX		128
+#define PKG_APPID_PER_USER	100000
+#define AID_APP_START		10000 /* first app user */
+#define AID_APP_END		19999 /* last app user */
+#define AID_CACHE_GID_START	20000 /* start of gids for apps to mark cached data */
+#define AID_EXT_GID_START	30000 /* start of gids for apps to mark external data */
+#define AID_EXT_CACHE_GID_START	40000 /* start of gids for apps to mark external cached data */
+#define AID_EXT_CACHE_GID_END	49999 /* end of gids for apps to mark external cached data */
+#define AID_SHARED_GID_START	50000 /* start of gids for apps in each user to share */
+#define PKG_APPID_MIN		1000
+#define PKG_APPID_MAX		(PKG_APPID_PER_USER - 1)
+
+/* operations vectors defined in specific files */
+extern const struct file_operations esdfs_main_fops;
+extern const struct file_operations esdfs_dir_fops;
+extern const struct inode_operations esdfs_main_iops;
+extern const struct inode_operations esdfs_dir_iops;
+extern const struct inode_operations esdfs_symlink_iops;
+extern const struct super_operations esdfs_sops;
+extern const struct dentry_operations esdfs_dops;
+extern const struct address_space_operations esdfs_aops, esdfs_dummy_aops;
+extern const struct vm_operations_struct esdfs_vm_ops;
+
+extern void esdfs_msg(struct super_block *, const char *, const char *, ...);
+extern int esdfs_init_inode_cache(void);
+extern void esdfs_destroy_inode_cache(void);
+extern int esdfs_init_dentry_cache(void);
+extern void esdfs_destroy_dentry_cache(void);
+extern int esdfs_new_dentry_private_data(struct dentry *dentry);
+extern void esdfs_free_dentry_private_data(struct dentry *dentry);
+extern struct dentry *esdfs_lookup(struct inode *dir, struct dentry *dentry,
+				   unsigned int flags);
+extern struct inode *esdfs_iget(struct super_block *sb,
+				struct inode *lower_inode,
+				uint32_t id);
+extern int esdfs_interpose(struct dentry *dentry, struct super_block *sb,
+			   struct path *lower_path, uint32_t id);
+extern int esdfs_init_package_list(void);
+extern void esdfs_destroy_package_list(void);
+extern void esdfs_derive_perms(struct dentry *dentry);
+extern void esdfs_set_derived_perms(struct inode *inode);
+extern int esdfs_is_dl_lookup(struct dentry *dentry, struct dentry *parent);
+extern int esdfs_derived_lookup(struct dentry *dentry, struct dentry **parent);
+extern int esdfs_derived_revalidate(struct dentry *dentry,
+				    struct dentry *parent);
+extern int esdfs_check_derived_permission(struct inode *inode, int mask);
+extern int esdfs_derive_mkdir_contents(struct dentry *dentry);
+extern int esdfs_lookup_nocase(struct path *lower_parent_path,
+		const struct qstr *name, struct path *lower_path);
+
+/* file private data */
+struct esdfs_file_info {
+	struct file *lower_file;
+	const struct vm_operations_struct *lower_vm_ops;
+};
+
+struct esdfs_perms {
+	uid_t raw_uid;
+	uid_t raw_gid;
+	uid_t uid;
+	gid_t gid;
+	unsigned short fmask;
+	unsigned short dmask;
+};
+
+/* esdfs inode data in memory */
+struct esdfs_inode_info {
+	struct inode *lower_inode;
+	struct inode vfs_inode;
+	unsigned version;	/* package list version this was derived from */
+	int tree;		/* storage tree location */
+	uint32_t userid;	/* Android User ID (not Linux UID) */
+	uid_t appid;		/* Linux UID for this app/user combo */
+	bool under_obb;
+};
+
+/* esdfs dentry data in memory */
+struct esdfs_dentry_info {
+	spinlock_t lock;	/* protects lower_path and lower_stub_path */
+	struct path lower_path;
+	struct path lower_stub_path;
+	struct dentry *real_parent;
+};
+
+/* esdfs super-block data in memory */
+struct esdfs_sb_info {
+	struct super_block *lower_sb;
+	struct super_block *s_sb;
+	struct user_namespace *base_ns;
+	struct list_head s_list;
+	struct esdfs_perms lower_perms;
+	struct esdfs_perms upper_perms;	   /* root in derived mode */
+	struct dentry *obb_parent;	   /* pinned dentry for obb link parent */
+	struct path dl_path;		   /* path of lower downloads folder */
+	struct qstr dl_name;		   /* name of lower downloads folder */
+	const char *dl_loc;		   /* location of dl folder */
+	struct esdfs_perms lower_dl_perms; /* permissions for lower downloads folder */
+	struct user_namespace *dl_ns;	   /* lower downloads namespace */
+	int ns_fd;
+	unsigned int options;
+};
+
+extern struct esdfs_perms esdfs_perms_table[ESDFS_PERMS_TABLE_SIZE];
+extern unsigned esdfs_package_list_version;
+
+void esdfs_add_super(struct esdfs_sb_info *, struct super_block *);
+void esdfs_truncate_share(struct super_block *, struct inode *, loff_t newsize);
+
+void esdfs_derive_lower_ownership(struct dentry *dentry, const char *name);
+
+static inline bool is_obb(struct qstr *name)
+{
+	struct qstr q_obb = QSTR_LITERAL("obb");
+	return qstr_case_eq(name, &q_obb);
+}
+
+static inline bool is_dl(struct qstr *name)
+{
+	struct qstr q_dl = QSTR_LITERAL("Download");
+
+	return qstr_case_eq(name, &q_dl);
+}
+
+#define ESDFS_INODE_IS_STALE(i) ((i)->version != esdfs_package_list_version)
+#define ESDFS_INODE_CAN_LINK(i) (test_opt(ESDFS_SB((i)->i_sb), \
+					  DERIVE_LEGACY) || \
+				 (test_opt(ESDFS_SB((i)->i_sb), \
+					   DERIVE_UNIFIED) && \
+				  ESDFS_I(i)->userid > 0))
+#define ESDFS_DENTRY_NEEDS_LINK(d) (is_obb(&(d)->d_name))
+#define ESDFS_DENTRY_NEEDS_DL_LINK(d) (is_dl(&(d)->d_name))
+#define ESDFS_DENTRY_IS_LINKED(d) (ESDFS_D(d)->real_parent)
+#define ESDFS_DENTRY_HAS_STUB(d) (ESDFS_D(d)->lower_stub_path.dentry)
+
+/*
+ * inode to private data
+ *
+ * Since we use containers and the struct inode is _inside_ the
+ * esdfs_inode_info structure, ESDFS_I will always (given a non-NULL
+ * inode pointer), return a valid non-NULL pointer.
+ */
+static inline struct esdfs_inode_info *ESDFS_I(const struct inode *inode)
+{
+	return container_of(inode, struct esdfs_inode_info, vfs_inode);
+}
+
+/* dentry to private data */
+#define ESDFS_D(dent) ((struct esdfs_dentry_info *)(dent)->d_fsdata)
+
+/* superblock to private data */
+#define ESDFS_SB(super) ((struct esdfs_sb_info *)(super)->s_fs_info)
+
+/* file to private Data */
+#define ESDFS_F(file) ((struct esdfs_file_info *)((file)->private_data))
+
+/* file to lower file */
+static inline struct file *esdfs_lower_file(const struct file *f)
+{
+	return ESDFS_F(f)->lower_file;
+}
+
+static inline void esdfs_set_lower_file(struct file *f, struct file *val)
+{
+	ESDFS_F(f)->lower_file = val;
+}
+
+/* inode to lower inode. */
+static inline struct inode *esdfs_lower_inode(const struct inode *i)
+{
+	return ESDFS_I(i)->lower_inode;
+}
+
+static inline void esdfs_set_lower_inode(struct inode *i, struct inode *val)
+{
+	ESDFS_I(i)->lower_inode = val;
+}
+
+/* superblock to lower superblock */
+static inline struct super_block *esdfs_lower_super(
+	const struct super_block *sb)
+{
+	return ESDFS_SB(sb)->lower_sb;
+}
+
+static inline void esdfs_set_lower_super(struct super_block *sb,
+					  struct super_block *val)
+{
+	ESDFS_SB(sb)->lower_sb = val;
+}
+
+/* path based (dentry/mnt) macros */
+static inline void pathcpy(struct path *dst, const struct path *src)
+{
+	dst->dentry = src->dentry;
+	dst->mnt = src->mnt;
+}
+/* Returns struct path.  Caller must path_put it. */
+static inline void esdfs_get_lower_path(const struct dentry *dent,
+					 struct path *lower_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(lower_path, &ESDFS_D(dent)->lower_path);
+	path_get(lower_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_get_lower_stub_path(const struct dentry *dent,
+					     struct path *lower_stub_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(lower_stub_path, &ESDFS_D(dent)->lower_stub_path);
+	path_get(lower_stub_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_put_lower_path(const struct dentry *dent,
+					 struct path *lower_path)
+{
+	path_put(lower_path);
+}
+static inline void esdfs_set_lower_path(const struct dentry *dent,
+					 struct path *lower_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(&ESDFS_D(dent)->lower_path, lower_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_set_lower_stub_path(const struct dentry *dent,
+					     struct path *lower_stub_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(&ESDFS_D(dent)->lower_stub_path, lower_stub_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_put_reset_lower_paths(const struct dentry *dent)
+{
+	struct path lower_path;
+	struct path lower_stub_path = {
+		.mnt = NULL,
+		.dentry = NULL,
+	};
+
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(&lower_path, &ESDFS_D(dent)->lower_path);
+	ESDFS_D(dent)->lower_path.dentry = NULL;
+	ESDFS_D(dent)->lower_path.mnt = NULL;
+	if (ESDFS_DENTRY_HAS_STUB(dent)) {
+		pathcpy(&lower_stub_path, &ESDFS_D(dent)->lower_stub_path);
+		ESDFS_D(dent)->lower_stub_path.dentry = NULL;
+		ESDFS_D(dent)->lower_stub_path.mnt = NULL;
+	}
+	spin_unlock(&ESDFS_D(dent)->lock);
+
+	path_put(&lower_path);
+	if (lower_stub_path.dentry)
+		path_put(&lower_stub_path);
+}
+static inline void esdfs_get_lower_parent(const struct dentry *dent,
+					  struct dentry *lower_dentry,
+					  struct dentry **lower_parent)
+{
+	*lower_parent = NULL;
+	spin_lock(&ESDFS_D(dent)->lock);
+	if (ESDFS_DENTRY_IS_LINKED(dent)) {
+		*lower_parent = ESDFS_D(dent)->real_parent;
+		dget(*lower_parent);
+	}
+	spin_unlock(&ESDFS_D(dent)->lock);
+	if (!*lower_parent)
+		*lower_parent = dget_parent(lower_dentry);
+}
+static inline void esdfs_put_lower_parent(const struct dentry *dent,
+					  struct dentry **lower_parent)
+{
+	dput(*lower_parent);
+}
+static inline void esdfs_set_lower_parent(const struct dentry *dent,
+					  struct dentry *parent)
+{
+	struct dentry *old_parent = NULL;
+
+	spin_lock(&ESDFS_D(dent)->lock);
+	if (ESDFS_DENTRY_IS_LINKED(dent))
+		old_parent = ESDFS_D(dent)->real_parent;
+	ESDFS_D(dent)->real_parent = parent;
+	dget(parent);	/* pin the lower parent */
+	spin_unlock(&ESDFS_D(dent)->lock);
+	if (old_parent)
+		dput(old_parent);
+}
+static inline void esdfs_release_lower_parent(const struct dentry *dent)
+{
+	struct dentry *real_parent = NULL;
+
+	spin_lock(&ESDFS_D(dent)->lock);
+	if (ESDFS_DENTRY_IS_LINKED(dent)) {
+		real_parent = ESDFS_D(dent)->real_parent;
+		ESDFS_D(dent)->real_parent = NULL;
+	}
+	spin_unlock(&ESDFS_D(dent)->lock);
+	if (real_parent)
+		dput(real_parent);
+}
+
+/* locking helpers */
+static inline struct dentry *lock_parent(struct dentry *dentry)
+{
+	struct dentry *dir = dget_parent(dentry);
+
+	inode_lock_nested(dir->d_inode, I_MUTEX_PARENT);
+	return dir;
+}
+
+static inline void unlock_dir(struct dentry *dir)
+{
+	inode_unlock(dir->d_inode);
+	dput(dir);
+}
+
+static inline void esdfs_set_lower_mode(struct esdfs_sb_info *sbi,
+		struct esdfs_inode_info *inode_i, umode_t *mode)
+{
+	struct esdfs_perms *perms = &sbi->lower_perms;
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD) &&
+			inode_i->tree == ESDFS_TREE_DOWNLOAD)
+		perms = &sbi->lower_dl_perms;
+
+	if (S_ISDIR(*mode))
+		*mode = (*mode & S_IFMT) | perms->dmask;
+	else
+		*mode = (*mode & S_IFMT) | perms->fmask;
+}
+
+static inline void esdfs_set_perms(struct inode *inode)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(inode->i_sb);
+
+	if (ESDFS_DERIVE_PERMS(sbi)) {
+		esdfs_set_derived_perms(inode);
+		return;
+	}
+	i_uid_write(inode, sbi->upper_perms.uid);
+	i_gid_write(inode, sbi->upper_perms.gid);
+	if (S_ISDIR(inode->i_mode))
+		inode->i_mode = (inode->i_mode & S_IFMT) |
+				sbi->upper_perms.dmask;
+	else
+		inode->i_mode = (inode->i_mode & S_IFMT) |
+				sbi->upper_perms.fmask;
+}
+
+static inline void esdfs_revalidate_perms(struct dentry *dentry)
+{
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(dentry->d_sb)) &&
+	    dentry->d_inode &&
+	    ESDFS_INODE_IS_STALE(ESDFS_I(dentry->d_inode))) {
+		esdfs_derive_perms(dentry);
+		esdfs_set_perms(dentry->d_inode);
+	}
+}
+
+static inline uid_t derive_uid(struct esdfs_inode_info *inode_i, uid_t uid)
+{
+	return inode_i->userid * PKG_APPID_PER_USER +
+	       (uid % PKG_APPID_PER_USER);
+}
+
+static inline bool uid_is_app(uid_t uid)
+{
+	uid_t appid = uid % PKG_APPID_PER_USER;
+
+	return appid >= AID_APP_START && appid <= AID_APP_END;
+}
+
+static inline gid_t multiuser_get_ext_cache_gid(uid_t uid)
+{
+	return uid - AID_APP_START + AID_EXT_CACHE_GID_START;
+}
+
+static inline gid_t multiuser_get_ext_gid(uid_t uid)
+{
+	return uid - AID_APP_START + AID_EXT_GID_START;
+}
+
+/* file attribute helpers */
+static inline void esdfs_copy_lower_attr(struct inode *dest,
+					 const struct inode *src)
+{
+	dest->i_mode = src->i_mode & S_IFMT;
+	dest->i_rdev = src->i_rdev;
+	dest->i_atime = src->i_atime;
+	dest->i_mtime = src->i_mtime;
+	dest->i_ctime = src->i_ctime;
+	dest->i_blkbits = src->i_blkbits;
+	dest->i_flags = src->i_flags;
+	set_nlink(dest, src->i_nlink);
+}
+
+static inline void esdfs_copy_attr(struct inode *dest, const struct inode *src)
+{
+	esdfs_copy_lower_attr(dest, src);
+	esdfs_set_perms(dest);
+}
+
+static inline uid_t esdfs_from_local_uid(struct esdfs_sb_info *sbi, uid_t uid)
+{
+	return from_kuid(sbi->base_ns, make_kuid(current_user_ns(), uid));
+}
+
+static inline gid_t esdfs_from_local_gid(struct esdfs_sb_info *sbi, gid_t gid)
+{
+	return from_kgid(sbi->base_ns, make_kgid(current_user_ns(), gid));
+}
+
+static inline uid_t esdfs_from_kuid(struct esdfs_sb_info *sbi, kuid_t uid)
+{
+	return from_kuid(sbi->base_ns, uid);
+}
+
+static inline gid_t esdfs_from_kgid(struct esdfs_sb_info *sbi, kgid_t gid)
+{
+	return from_kgid(sbi->base_ns, gid);
+}
+
+static inline kuid_t esdfs_make_kuid(struct esdfs_sb_info *sbi, uid_t uid)
+{
+	return make_kuid(sbi->base_ns, uid);
+}
+
+static inline kgid_t esdfs_make_kgid(struct esdfs_sb_info *sbi, gid_t gid)
+{
+	return make_kgid(sbi->base_ns, gid);
+}
+
+/* Helper functions to read and write to inode uid/gids without
+ * having to worry about translating into/out of esdfs's preferred
+ * base user namespace.
+ */
+static inline uid_t esdfs_i_uid_read(const struct inode *inode)
+{
+	return esdfs_from_kuid(ESDFS_SB(inode->i_sb), inode->i_uid);
+}
+
+static inline gid_t esdfs_i_gid_read(const struct inode *inode)
+{
+	return esdfs_from_kgid(ESDFS_SB(inode->i_sb), inode->i_gid);
+}
+
+static inline void esdfs_i_uid_write(struct inode *inode, uid_t uid)
+{
+	inode->i_uid = esdfs_make_kuid(ESDFS_SB(inode->i_sb), uid);
+}
+
+static inline void esdfs_i_gid_write(struct inode *inode, gid_t gid)
+{
+	inode->i_gid = esdfs_make_kgid(ESDFS_SB(inode->i_sb), gid);
+}
+
+/*
+ * Based on nfs4_save_creds() and nfs4_reset_creds() in nfsd/nfs4recover.c.
+ * Returns NULL if prepare_creds() could not allocate heap, otherwise
+ */
+static inline const struct cred *esdfs_override_creds(
+		struct esdfs_sb_info *sbi,
+		struct esdfs_inode_info *info, int *mask)
+{
+	struct cred *creds = prepare_creds();
+	uid_t uid;
+	gid_t gid = sbi->lower_perms.gid;
+
+	if (!creds)
+		return NULL;
+
+	/* clear the umask so that the lower mode works for create cases */
+	if (mask) {
+		*mask = 0;
+		*mask = xchg(&current->fs->umask, *mask & S_IRWXUGO);
+	}
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD) &&
+			info->tree == ESDFS_TREE_DOWNLOAD) {
+		creds->fsuid = make_kuid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_uid);
+		creds->fsgid = make_kgid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_gid);
+	} else {
+		if (test_opt(sbi, GID_DERIVATION)) {
+			if (info->under_obb)
+				uid = AID_MEDIA_OBB;
+			else
+				uid = derive_uid(info, sbi->lower_perms.uid);
+		} else {
+			uid = sbi->lower_perms.uid;
+		}
+		creds->fsuid = esdfs_make_kuid(sbi, uid);
+		creds->fsgid = esdfs_make_kgid(sbi, gid);
+	}
+
+	/* this installs the new creds into current, which we must destroy */
+	return override_creds(creds);
+}
+
+static inline void esdfs_revert_creds(const struct cred *creds, int *mask)
+{
+	const struct cred *current_creds = current->cred;
+
+	/* restore the old umask */
+	if (mask)
+		*mask = xchg(&current->fs->umask, *mask & S_IRWXUGO);
+
+	/* restore the old creds into current */
+	revert_creds(creds);
+	put_cred(current_creds);	/* destroy the old temporary creds */
+}
+
+#endif	/* not _ESDFS_H_ */
Index: rpi4-kernel/fs/esdfs/file.c
===================================================================
--- /dev/null
+++ rpi4-kernel/fs/esdfs/file.c
@@ -0,0 +1,471 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014, 2016 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+static ssize_t esdfs_read(struct file *file, char __user *buf,
+			   size_t count, loff_t *ppos)
+{
+	int err;
+	struct file *lower_file;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	err = vfs_read(lower_file, buf, count, ppos);
+	/* update our inode atime upon a successful lower read */
+	if (err >= 0)
+		fsstack_copy_attr_atime(dentry->d_inode,
+					file_inode(lower_file));
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static ssize_t esdfs_write(struct file *file, const char __user *buf,
+			    size_t count, loff_t *ppos)
+{
+	int err;
+
+	struct file *lower_file;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	err = vfs_write(lower_file, buf, count, ppos);
+	/* update our inode times+sizes upon a successful lower write */
+	if (err >= 0) {
+		fsstack_copy_inode_size(dentry->d_inode,
+					file_inode(lower_file));
+		esdfs_copy_attr(dentry->d_inode,
+				file_inode(lower_file));
+	}
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_readdir(struct file *file, struct dir_context *ctx)
+{
+	int err;
+	struct file *lower_file = NULL;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	err = iterate_dir(lower_file, ctx);
+	file->f_pos = lower_file->f_pos;
+	if (err >= 0)		/* copy the atime */
+		fsstack_copy_attr_atime(dentry->d_inode,
+					file_inode(lower_file));
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static long esdfs_unlocked_ioctl(struct file *file, unsigned int cmd,
+				  unsigned long arg)
+{
+	long err = -ENOTTY;
+	struct file *lower_file;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (cmd == ESDFS_IOC_DIS_ACCESS) {
+		if (!capable(CAP_SYS_ADMIN)) {
+			err = -EPERM;
+			goto out;
+		}
+		set_opt(sbi, ACCESS_DISABLE);
+		err = 0;
+		goto out;
+	}
+
+	lower_file = esdfs_lower_file(file);
+
+	/* XXX: use vfs_ioctl if/when VFS exports it */
+	if (!lower_file || !lower_file->f_op)
+		goto out;
+	if (lower_file->f_op->unlocked_ioctl)
+		err = lower_file->f_op->unlocked_ioctl(lower_file, cmd, arg);
+
+	/* some ioctls can change inode attributes (EXT2_IOC_SETFLAGS) */
+	if (!err)
+		esdfs_copy_attr(file->f_path.dentry->d_inode,
+				file_inode(lower_file));
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+#ifdef CONFIG_COMPAT
+static long esdfs_compat_ioctl(struct file *file, unsigned int cmd,
+				unsigned long arg)
+{
+	long err = -ENOTTY;
+	struct file *lower_file;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+
+	/* XXX: use vfs_ioctl if/when VFS exports it */
+	if (!lower_file || !lower_file->f_op)
+		goto out;
+	if (lower_file->f_op->compat_ioctl)
+		err = lower_file->f_op->compat_ioctl(lower_file, cmd, arg);
+
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+#endif
+
+static int esdfs_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	int err = 0;
+	bool willwrite;
+	struct file *lower_file;
+	const struct vm_operations_struct *saved_vm_ops = NULL;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	/* this might be deferred to mmap's writepage */
+	willwrite = ((vma->vm_flags | VM_SHARED | VM_WRITE) == vma->vm_flags);
+
+	/*
+	 * File systems which do not implement ->writepage may use
+	 * generic_file_readonly_mmap as their ->mmap op.  If you call
+	 * generic_file_readonly_mmap with VM_WRITE, you'd get an -EINVAL.
+	 * But we cannot call the lower ->mmap op, so we can't tell that
+	 * writeable mappings won't work.  Therefore, our only choice is to
+	 * check if the lower file system supports the ->writepage, and if
+	 * not, return EINVAL (the same error that
+	 * generic_file_readonly_mmap returns in that case).
+	 */
+	lower_file = esdfs_lower_file(file);
+	if (willwrite && !lower_file->f_mapping->a_ops->writepage) {
+		err = -EINVAL;
+		esdfs_msg(file->f_mapping->host->i_sb, KERN_INFO,
+			"lower file system does not support writeable mmap\n");
+		goto out;
+	}
+
+	/*
+	 * find and save lower vm_ops.
+	 *
+	 * XXX: the VFS should have a cleaner way of finding the lower vm_ops
+	 */
+	if (!ESDFS_F(file)->lower_vm_ops) {
+		err = lower_file->f_op->mmap(lower_file, vma);
+		if (err) {
+			esdfs_msg(file->f_mapping->host->i_sb, KERN_ERR,
+				"lower mmap failed %d\n", err);
+			goto out;
+		}
+		saved_vm_ops = vma->vm_ops; /* save: came from lower ->mmap */
+	}
+
+	/*
+	 * Next 3 lines are all I need from generic_file_mmap.  I definitely
+	 * don't want its test for ->readpage which returns -ENOEXEC.
+	 */
+	file_accessed(file);
+	vma->vm_ops = &esdfs_vm_ops;
+
+	file->f_mapping->a_ops = &esdfs_aops; /* set our aops */
+	if (!ESDFS_F(file)->lower_vm_ops) /* save for our ->fault */
+		ESDFS_F(file)->lower_vm_ops = saved_vm_ops;
+
+	vma->vm_private_data = file;
+	get_file(lower_file);
+	vma->vm_file = lower_file;
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_open(struct inode *inode, struct file *file)
+{
+	int err = 0;
+	struct file *lower_file = NULL;
+	struct path lower_path;
+	struct esdfs_sb_info *sbi = ESDFS_SB(inode->i_sb);
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(inode->i_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(sbi, ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	/* don't open unhashed/deleted files */
+	if (d_unhashed(file->f_path.dentry)) {
+		err = -ENOENT;
+		goto out_err;
+	}
+
+	file->private_data =
+		kzalloc(sizeof(struct esdfs_file_info), GFP_KERNEL);
+	if (!ESDFS_F(file)) {
+		err = -ENOMEM;
+		goto out_err;
+	}
+
+	/* open lower object and link esdfs's file struct to lower's */
+	esdfs_get_lower_path(file->f_path.dentry, &lower_path);
+	lower_file = dentry_open(&lower_path, file->f_flags, current_cred());
+	path_put(&lower_path);
+	if (IS_ERR(lower_file)) {
+		err = PTR_ERR(lower_file);
+		lower_file = esdfs_lower_file(file);
+		if (lower_file) {
+			esdfs_set_lower_file(file, NULL);
+			fput(lower_file); /* fput calls dput for lower_dentry */
+		}
+	} else {
+		esdfs_set_lower_file(file, lower_file);
+	}
+
+	if (err)
+		kfree(ESDFS_F(file));
+	else
+		esdfs_copy_attr(inode, esdfs_lower_inode(inode));
+out_err:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_flush(struct file *file, fl_owner_t id)
+{
+	int err = 0;
+	struct file *lower_file = NULL;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	if (lower_file && lower_file->f_op && lower_file->f_op->flush) {
+		filemap_write_and_wait(file->f_mapping);
+		err = lower_file->f_op->flush(lower_file, id);
+	}
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/* release all lower object references & free the file info structure */
+static int esdfs_file_release(struct inode *inode, struct file *file)
+{
+	struct file *lower_file;
+
+	lower_file = esdfs_lower_file(file);
+	if (lower_file) {
+		esdfs_set_lower_file(file, NULL);
+		fput(lower_file);
+	}
+
+	kfree(ESDFS_F(file));
+	return 0;
+}
+
+static int esdfs_fsync(struct file *file, loff_t start, loff_t end,
+			int datasync)
+{
+	int err;
+	struct file *lower_file;
+	struct path lower_path;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	err = __generic_file_fsync(file, start, end, datasync);
+	if (err)
+		goto out;
+	lower_file = esdfs_lower_file(file);
+	esdfs_get_lower_path(dentry, &lower_path);
+	err = vfs_fsync_range(lower_file, start, end, datasync);
+	esdfs_put_lower_path(dentry, &lower_path);
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_fasync(int fd, struct file *file, int flag)
+{
+	int err = 0;
+	struct file *lower_file = NULL;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	if (lower_file->f_op && lower_file->f_op->fasync)
+		err = lower_file->f_op->fasync(fd, lower_file, flag);
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/*
+ * Wrapfs cannot use generic_file_llseek as ->llseek, because it would
+ * only set the offset of the upper file.  So we have to implement our
+ * own method to set both the upper and lower file offsets
+ * consistently.
+ */
+static loff_t esdfs_file_llseek(struct file *file, loff_t offset, int whence)
+{
+	int err;
+	struct file *lower_file;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+				ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	err = generic_file_llseek(file, offset, whence);
+	if (err < 0)
+		goto out;
+
+	lower_file = esdfs_lower_file(file);
+	err = generic_file_llseek(lower_file, offset, whence);
+
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/*
+ * Wrapfs read_iter, redirect modified iocb to lower read_iter
+ */
+static ssize_t
+esdfs_read_iter(struct kiocb *iocb, struct iov_iter *iter)
+{
+	int err;
+	struct file *file = iocb->ki_filp, *lower_file;
+
+	lower_file = esdfs_lower_file(file);
+	if (!lower_file->f_op->read_iter) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	get_file(lower_file); /* prevent lower_file from being released */
+	iocb->ki_filp = lower_file;
+	err = lower_file->f_op->read_iter(iocb, iter);
+	iocb->ki_filp = file;
+	fput(lower_file);
+	/* update upper inode atime as needed */
+	if (err >= 0 || err == -EIOCBQUEUED)
+		fsstack_copy_attr_atime(file->f_path.dentry->d_inode,
+					file_inode(lower_file));
+out:
+	return err;
+}
+
+/*
+ * Wrapfs write_iter, redirect modified iocb to lower write_iter
+ */
+static ssize_t
+esdfs_write_iter(struct kiocb *iocb, struct iov_iter *iter)
+{
+	int err;
+	struct file *file = iocb->ki_filp, *lower_file;
+
+	lower_file = esdfs_lower_file(file);
+	if (!lower_file->f_op->write_iter) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	get_file(lower_file); /* prevent lower_file from being released */
+	iocb->ki_filp = lower_file;
+	err = lower_file->f_op->write_iter(iocb, iter);
+	iocb->ki_filp = file;
+	fput(lower_file);
+	/* update upper inode times/sizes as needed */
+	if (err >= 0 || err == -EIOCBQUEUED) {
+		fsstack_copy_inode_size(file->f_path.dentry->d_inode,
+					file_inode(lower_file));
+		fsstack_copy_attr_times(file->f_path.dentry->d_inode,
+					file_inode(lower_file));
+	}
+out:
+	return err;
+}
+
+const struct file_operations esdfs_main_fops = {
+	.llseek		= generic_file_llseek,
+	.read		= esdfs_read,
+	.write		= esdfs_write,
+	.unlocked_ioctl	= esdfs_unlocked_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl	= esdfs_compat_ioctl,
+#endif
+	.mmap		= esdfs_mmap,
+	.open		= esdfs_open,
+	.flush		= esdfs_flush,
+	.release	= esdfs_file_release,
+	.fsync		= esdfs_fsync,
+	.fasync		= esdfs_fasync,
+	.read_iter	= esdfs_read_iter,
+	.write_iter	= esdfs_write_iter,
+	.splice_read    = generic_file_splice_read,
+	.splice_write   = iter_file_splice_write,
+};
+
+/* trimmed directory options */
+const struct file_operations esdfs_dir_fops = {
+	.llseek		= esdfs_file_llseek,
+	.read		= generic_read_dir,
+	.iterate	= esdfs_readdir,
+	.unlocked_ioctl	= esdfs_unlocked_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl	= esdfs_compat_ioctl,
+#endif
+	.open		= esdfs_open,
+	.release	= esdfs_file_release,
+	.flush		= esdfs_flush,
+	.fsync		= esdfs_fsync,
+	.fasync		= esdfs_fasync,
+};
Index: rpi4-kernel/fs/esdfs/inode.c
===================================================================
--- /dev/null
+++ rpi4-kernel/fs/esdfs/inode.c
@@ -0,0 +1,550 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+#include <linux/fsnotify.h>
+
+static int esdfs_create(struct user_namespace *mnt_userns, struct inode *dir,
+			struct dentry *dentry, umode_t mode, bool want_excl)
+{
+	int err;
+	struct dentry *lower_dentry;
+	struct dentry *lower_parent_dentry = NULL;
+	struct path lower_path;
+	struct inode *lower_inode;
+	int mask;
+	const struct cred *creds;
+
+	/*
+	 * Need to recheck derived permissions unified mode to prevent certain
+	 * applications from creating files at the root.
+	 */
+	if (test_opt(ESDFS_SB(dir->i_sb), DERIVE_UNIFIED) &&
+	    esdfs_check_derived_permission(dir, ESDFS_MAY_CREATE) != 0)
+		return -EACCES;
+
+	if (test_opt(ESDFS_SB(dir->i_sb), ACCESS_DISABLE))
+		return -ENOENT;
+
+	creds = esdfs_override_creds(ESDFS_SB(dir->i_sb), ESDFS_I(dir), &mask);
+	if (!creds)
+		return -ENOMEM;
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	lower_parent_dentry = lock_parent(lower_dentry);
+
+	esdfs_set_lower_mode(ESDFS_SB(dir->i_sb), ESDFS_I(dir), &mode);
+
+	lower_inode = esdfs_lower_inode(dir);
+	err = vfs_create(mnt_userns, lower_inode, lower_dentry, mode,
+			 want_excl);
+	if (err)
+		goto out;
+
+	err = esdfs_interpose(dentry, dir->i_sb, &lower_path,
+				ESDFS_I(dir)->userid);
+	if (err)
+		goto out;
+	fsstack_copy_attr_times(dir, esdfs_lower_inode(dir));
+	fsstack_copy_inode_size(dir, lower_parent_dentry->d_inode);
+	esdfs_derive_lower_ownership(dentry, dentry->d_name.name);
+
+out:
+	unlock_dir(lower_parent_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
+
+static int esdfs_unlink(struct inode *dir, struct dentry *dentry)
+{
+	int err;
+	struct dentry *lower_dentry;
+	struct inode *lower_dir_inode;
+	struct dentry *lower_dir_dentry;
+	struct path lower_path;
+	const struct cred *creds;
+
+	creds = esdfs_override_creds(ESDFS_SB(dir->i_sb), ESDFS_I(dir), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(dir->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	dget(lower_dentry);
+
+	lower_dir_dentry = lock_parent(lower_dentry);
+
+	/* d_parent might be changed in vfs_rename */
+	if (lower_dir_dentry != lower_dentry->d_parent) {
+		err = -ENOENT;
+		goto out;
+	}
+
+	/* lower_dir_inode might be changed as well
+	 * get the new inode with new lower dir dentry
+	 */
+	lower_dir_inode = lower_dir_dentry->d_inode;
+
+	err = vfs_unlink(&init_user_ns, lower_dir_inode, lower_dentry, NULL);
+
+	/*
+	 * Note: unlinking on top of NFS can cause silly-renamed files.
+	 * Trying to delete such files results in EBUSY from NFS
+	 * below.  Silly-renamed files will get deleted by NFS later on, so
+	 * we just need to detect them here and treat such EBUSY errors as
+	 * if the upper file was successfully deleted.
+	 */
+	if (err == -EBUSY && lower_dentry->d_flags & DCACHE_NFSFS_RENAMED)
+		err = 0;
+	if (err)
+		goto out;
+	fsstack_copy_attr_times(dir, lower_dir_inode);
+	fsstack_copy_inode_size(dir, lower_dir_inode);
+	set_nlink(dentry->d_inode,
+		  esdfs_lower_inode(dentry->d_inode)->i_nlink);
+	dentry->d_inode->i_ctime = dir->i_ctime;
+	d_drop(dentry); /* this is needed, else LTP fails (VFS won't do it) */
+out:
+	unlock_dir(lower_dir_dentry);
+	dput(lower_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_mkdir(struct user_namespace *mnt_userns, struct inode *dir,
+		       struct dentry *dentry, umode_t mode)
+{
+	int err;
+	struct dentry *lower_dentry;
+	struct dentry *lower_parent_dentry = NULL;
+	struct path lower_path;
+	int mask;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dir->i_sb),
+					ESDFS_I(dir), &mask);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(dir->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	lower_parent_dentry = lock_parent(lower_dentry);
+
+	mode |= S_IFDIR;
+	esdfs_set_lower_mode(ESDFS_SB(dir->i_sb), ESDFS_I(dir), &mode);
+	err = vfs_mkdir(mnt_userns, lower_parent_dentry->d_inode, lower_dentry,
+			mode);
+	if (err)
+		goto unlock_lower_parent;
+
+	err = esdfs_interpose(dentry, dir->i_sb, &lower_path,
+				ESDFS_I(dir)->userid);
+	if (err)
+		goto unlock_lower_parent;
+
+	fsstack_copy_attr_times(dir, esdfs_lower_inode(dir));
+	fsstack_copy_inode_size(dir, lower_parent_dentry->d_inode);
+	/* update number of links on parent directory */
+	set_nlink(dir, esdfs_lower_inode(dir)->i_nlink);
+	esdfs_derive_lower_ownership(dentry, dentry->d_name.name);
+
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(dir->i_sb))) {
+		unlock_dir(lower_parent_dentry);
+		err = esdfs_derive_mkdir_contents(dentry);
+		goto out;
+	}
+
+unlock_lower_parent:
+	unlock_dir(lower_parent_dentry);
+out:
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
+
+static int esdfs_rmdir(struct inode *dir, struct dentry *dentry)
+{
+	struct dentry *lower_dentry;
+	struct dentry *lower_dir_dentry;
+	int err;
+	struct path lower_path;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dir->i_sb),
+					ESDFS_I(dir), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	/* Never remove a pseudo link target.  Only the source. */
+	if (ESDFS_DENTRY_HAS_STUB(dentry))
+		esdfs_get_lower_stub_path(dentry, &lower_path);
+	else
+		esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+
+	lower_dir_dentry = lock_parent(lower_dentry);
+
+	/* d_parent might be changed in vfs_rename */
+	if (lower_dir_dentry != lower_dentry->d_parent) {
+		err = -ENOENT;
+		goto out;
+	}
+
+	err = vfs_rmdir(&init_user_ns, lower_dir_dentry->d_inode, lower_dentry);
+	if (err)
+		goto out;
+
+	d_drop(dentry);	/* drop our dentry on success (why not VFS's job?) */
+	if (dentry->d_inode)
+		clear_nlink(dentry->d_inode);
+	fsstack_copy_attr_times(dir, lower_dir_dentry->d_inode);
+	fsstack_copy_inode_size(dir, lower_dir_dentry->d_inode);
+	set_nlink(dir, lower_dir_dentry->d_inode->i_nlink);
+
+out:
+	unlock_dir(lower_dir_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/*
+ * The locking rules in esdfs_rename are complex.  We could use a simpler
+ * superblock-level name-space lock for renames and copy-ups.
+ */
+static int esdfs_rename(struct user_namespace *mnt_userns,
+			struct inode *old_dir, struct dentry *old_dentry,
+			struct inode *new_dir, struct dentry *new_dentry,
+			unsigned int flags)
+{
+	int err = 0;
+	struct esdfs_sb_info *sbi = ESDFS_SB(old_dir->i_sb);
+	struct dentry *lower_old_dentry = NULL;
+	struct dentry *lower_new_dentry = NULL;
+	struct dentry *lower_old_dir_dentry = NULL;
+	struct dentry *lower_new_dir_dentry = NULL;
+	struct dentry *trap = NULL;
+	struct path lower_old_path, lower_new_path;
+	int mask;
+	const struct cred *creds;
+	struct renamedata rd;
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD)) {
+		if ((ESDFS_I(old_dir)->tree == ESDFS_TREE_DOWNLOAD
+			|| ESDFS_I(new_dir)->tree == ESDFS_TREE_DOWNLOAD)
+			&& ESDFS_I(old_dir)->tree != ESDFS_I(new_dir)->tree)
+			return -EXDEV;
+	}
+
+	if (test_opt(sbi, GID_DERIVATION)) {
+		if (ESDFS_I(old_dir)->userid != ESDFS_I(new_dir)->userid
+			|| ((ESDFS_I(old_dir)->under_obb
+			|| ESDFS_I(new_dir)->under_obb)
+			&& ESDFS_I(old_dir)->under_obb
+				!= ESDFS_I(new_dir)->under_obb))
+			return -EXDEV;
+	}
+	creds = esdfs_override_creds(sbi, ESDFS_I(new_dir), &mask);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(old_dir->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	/* Never rename to or from a pseudo hard link target. */
+	if (ESDFS_DENTRY_HAS_STUB(old_dentry))
+		esdfs_get_lower_stub_path(old_dentry, &lower_old_path);
+	else
+		esdfs_get_lower_path(old_dentry, &lower_old_path);
+	if (ESDFS_DENTRY_HAS_STUB(new_dentry))
+		esdfs_get_lower_stub_path(new_dentry, &lower_new_path);
+	else
+		esdfs_get_lower_path(new_dentry, &lower_new_path);
+	lower_old_dentry = lower_old_path.dentry;
+	lower_new_dentry = lower_new_path.dentry;
+	esdfs_get_lower_parent(old_dentry, lower_old_dentry,
+			       &lower_old_dir_dentry);
+	esdfs_get_lower_parent(new_dentry, lower_new_dentry,
+			       &lower_new_dir_dentry);
+
+	trap = lock_rename(lower_old_dir_dentry, lower_new_dir_dentry);
+	/* source should not be ancestor of target */
+	if (trap == lower_old_dentry) {
+		err = -EINVAL;
+		goto out;
+	}
+	/* target should not be ancestor of source */
+	if (trap == lower_new_dentry) {
+		err = -ENOTEMPTY;
+		goto out;
+	}
+
+	rd.old_mnt_userns = mnt_userns;
+	rd.old_dir = lower_old_dir_dentry->d_inode;
+	rd.old_dentry = lower_old_dentry;
+	rd.new_mnt_userns = mnt_userns;
+	rd.new_dir = lower_new_dir_dentry->d_inode;
+	rd.new_dentry = lower_new_dentry;
+	rd.flags = flags;
+
+	err = vfs_rename(&rd);
+	if (err)
+		goto out;
+
+	esdfs_copy_attr(new_dir, lower_new_dir_dentry->d_inode);
+	fsstack_copy_inode_size(new_dir, lower_new_dir_dentry->d_inode);
+	if (new_dir != old_dir) {
+		esdfs_copy_attr(old_dir,
+				      lower_old_dir_dentry->d_inode);
+		fsstack_copy_inode_size(old_dir,
+					lower_old_dir_dentry->d_inode);
+	}
+
+	/* Drop any old links */
+	if (ESDFS_DENTRY_HAS_STUB(old_dentry))
+		d_drop(old_dentry);
+	if (ESDFS_DENTRY_HAS_STUB(new_dentry))
+		d_drop(new_dentry);
+	esdfs_derive_lower_ownership(old_dentry, new_dentry->d_name.name);
+out:
+	unlock_rename(lower_old_dir_dentry, lower_new_dir_dentry);
+	esdfs_put_lower_parent(old_dentry, &lower_old_dir_dentry);
+	esdfs_put_lower_parent(new_dentry, &lower_new_dir_dentry);
+	esdfs_put_lower_path(old_dentry, &lower_old_path);
+	esdfs_put_lower_path(new_dentry, &lower_new_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
+
+static int esdfs_permission(struct user_namespace *mnt_userns,
+			    struct inode *inode, int mask)
+{
+	struct inode *lower_inode;
+	int err;
+
+	/* First, check the upper permissions */
+	err = generic_permission(mnt_userns, inode, mask);
+
+	/* Basic checking of the lower inode (can't override creds here) */
+	lower_inode = esdfs_lower_inode(inode);
+	if (S_ISSOCK(lower_inode->i_mode) ||
+	    S_ISLNK(lower_inode->i_mode) ||
+	    S_ISBLK(lower_inode->i_mode) ||
+	    S_ISCHR(lower_inode->i_mode) ||
+	    S_ISFIFO(lower_inode->i_mode))
+		err = -EACCES;
+
+	/* Finally, check the derived permissions */
+	if (!err && ESDFS_DERIVE_PERMS(ESDFS_SB(inode->i_sb)))
+		err = esdfs_check_derived_permission(inode, mask);
+
+	return err;
+}
+
+static int esdfs_setattr(struct user_namespace *mnt_userns,
+			 struct dentry *dentry, struct iattr *ia)
+{
+	int err;
+	loff_t oldsize;
+	loff_t newsize;
+	struct dentry *lower_dentry;
+	struct inode *inode;
+	struct inode *lower_inode;
+	struct path lower_path;
+	struct iattr lower_ia;
+	const struct cred *creds;
+
+	/* We don't allow chmod or chown, so skip those */
+	ia->ia_valid &= ~(ATTR_UID | ATTR_GID | ATTR_MODE);
+	if (!ia->ia_valid)
+		return 0;
+	/* Allow touch updating timestamps. A previous permission check ensures
+	 * we have write access. Changes to mode, owner, and group are ignored
+	 */
+	ia->ia_valid |= ATTR_FORCE;
+
+	inode = dentry->d_inode;
+
+	if (test_opt(ESDFS_SB(inode->i_sb), ACCESS_DISABLE))
+		return -ENOENT;
+
+	/*
+	 * Check if user has permission to change inode.  We don't check if
+	 * this user can change the lower inode: that should happen when
+	 * calling notify_change on the lower inode.
+	 */
+	err = setattr_prepare(mnt_userns, dentry, ia);
+	if (err)
+		return err;
+
+	creds = esdfs_override_creds(ESDFS_SB(dentry->d_inode->i_sb),
+				ESDFS_I(inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	lower_inode = esdfs_lower_inode(inode);
+
+	/* prepare our own lower struct iattr (with the lower file) */
+	memcpy(&lower_ia, ia, sizeof(lower_ia));
+	if (ia->ia_valid & ATTR_FILE)
+		lower_ia.ia_file = esdfs_lower_file(ia->ia_file);
+
+	/*
+	 * If shrinking, first truncate upper level to cancel writing dirty
+	 * pages beyond the new eof; and also if its' maxbytes is more
+	 * limiting (fail with -EFBIG before making any change to the lower
+	 * level).  There is no need to vmtruncate the upper level
+	 * afterwards in the other cases: we fsstack_copy_inode_size from
+	 * the lower level.
+	 */
+	if (ia->ia_valid & ATTR_SIZE) {
+		err = inode_newsize_ok(inode, ia->ia_size);
+		if (err)
+			goto out;
+		/*
+		 * i_size_write needs locking around it
+		 * otherwise i_size_read() may spin forever
+		 * (see include/linux/fs.h).
+		 * similar to function fsstack_copy_inode_size
+		 */
+		oldsize = i_size_read(inode);
+		newsize = ia->ia_size;
+
+#if BITS_PER_LONG == 32 && defined(CONFIG_SMP)
+		spin_lock(&inode->i_lock);
+#endif
+		i_size_write(inode, newsize);
+#if BITS_PER_LONG == 32 && defined(CONFIG_SMP)
+		spin_unlock(&inode->i_lock);
+#endif
+		if (newsize > oldsize)
+			pagecache_isize_extended(inode, oldsize, newsize);
+		truncate_pagecache(inode, newsize);
+		esdfs_truncate_share(inode->i_sb, lower_dentry->d_inode,
+					ia->ia_size);
+	}
+
+	/*
+	 * mode change is for clearing setuid/setgid bits. Allow lower fs
+	 * to interpret this in its own way.
+	 */
+	if (lower_ia.ia_valid & (ATTR_KILL_SUID | ATTR_KILL_SGID))
+		lower_ia.ia_valid &= ~ATTR_MODE;
+
+	/* notify the (possibly copied-up) lower inode */
+	/*
+	 * Note: we use lower_dentry->d_inode, because lower_inode may be
+	 * unlinked (no inode->i_sb and i_ino==0.  This happens if someone
+	 * tries to open(), unlink(), then ftruncate() a file.
+	 */
+	inode_lock(lower_dentry->d_inode);
+	err = notify_change(mnt_userns, lower_dentry,
+			    &lower_ia, /* note: lower_ia */
+			    NULL);
+	inode_unlock(lower_dentry->d_inode);
+	if (err)
+		goto out;
+
+	/* get attributes from the lower inode */
+	esdfs_copy_attr(inode, lower_inode);
+	/*
+	 * Not running fsstack_copy_inode_size(inode, lower_inode), because
+	 * VFS should update our inode size, and notify_change on
+	 * lower_inode should update its size.
+	 */
+
+out:
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_getattr(struct user_namespace *mnt_userns,
+			 const struct path *path, struct kstat *stat,
+			 u32 request_mask, unsigned int flags)
+{
+	int err;
+	struct dentry *dentry = path->dentry;
+	struct path lower_path;
+	struct kstat lower_stat;
+	struct inode *lower_inode;
+	struct inode *inode = dentry->d_inode;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(inode->i_sb),
+						ESDFS_I(inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(inode->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	esdfs_get_lower_path(dentry, &lower_path);
+
+	/* We need the lower getattr to calculate stat->blocks for us. */
+	err = vfs_getattr(&lower_path, &lower_stat, request_mask, flags);
+	if (err)
+		goto out;
+
+	lower_inode = esdfs_lower_inode(inode);
+	esdfs_copy_attr(inode, lower_inode);
+	fsstack_copy_inode_size(inode, lower_inode);
+	generic_fillattr(mnt_userns, inode, stat);
+
+	stat->blocks = lower_stat.blocks;
+
+out:
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+const struct inode_operations esdfs_symlink_iops = {
+	.permission     = esdfs_permission,
+	.setattr	= esdfs_setattr,
+	.getattr	= esdfs_getattr,
+};
+
+const struct inode_operations esdfs_dir_iops = {
+	.create		= esdfs_create,
+	.lookup		= esdfs_lookup,
+	.unlink		= esdfs_unlink,
+	.mkdir		= esdfs_mkdir,
+	.rmdir		= esdfs_rmdir,
+	.rename		= esdfs_rename,
+	.permission     = esdfs_permission,
+	.setattr	= esdfs_setattr,
+	.getattr	= esdfs_getattr,
+};
+
+const struct inode_operations esdfs_main_iops = {
+	.permission     = esdfs_permission,
+	.setattr	= esdfs_setattr,
+	.getattr	= esdfs_getattr,
+};
Index: rpi4-kernel/fs/esdfs/lookup.c
===================================================================
--- /dev/null
+++ rpi4-kernel/fs/esdfs/lookup.c
@@ -0,0 +1,474 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+struct esdfs_name_data {
+	struct dir_context ctx;
+	const struct qstr *to_find;
+	char match_name[NAME_MAX+1];
+	bool found;
+};
+
+static int esdfs_name_match(struct dir_context *ctx, const char *name, int namelen,
+		loff_t offset, u64 ino, unsigned int d_type)
+{
+	struct esdfs_name_data *buf = container_of(ctx, struct esdfs_name_data, ctx);
+	struct qstr candidate = QSTR_INIT(name, namelen);
+
+	if (qstr_case_eq(buf->to_find, &candidate)) {
+		memcpy(buf->match_name, name, namelen);
+		buf->match_name[namelen] = 0;
+		buf->found = true;
+		return 1;
+	}
+	return 0;
+}
+
+int esdfs_lookup_nocase(struct path *parent,
+		const struct qstr *name,
+		struct path *path) {
+	int err = 0;
+	/* Use vfs_path_lookup to check if the dentry exists or not */
+	err = vfs_path_lookup(parent->dentry, parent->mnt, name->name, 0, path);
+	/* check for other cases */
+	if (err == -ENOENT) {
+		struct file *file;
+		const struct cred *cred = current_cred();
+
+		struct esdfs_name_data buffer = {
+			.ctx.actor = esdfs_name_match,
+			.to_find = name,
+			.found = false,
+		};
+
+		file = dentry_open(parent, O_RDONLY | O_DIRECTORY, cred);
+		if (IS_ERR(file))
+			return PTR_ERR(file);
+		err = iterate_dir(file, &buffer.ctx);
+		fput(file);
+		if (err)
+			return err;
+
+		if (buffer.found)
+			err = vfs_path_lookup(parent->dentry, parent->mnt,
+						buffer.match_name, 0, path);
+		else
+			err = -ENOENT;
+	}
+	return err;
+}
+
+struct esdfs_ci_getdents_callback {
+	struct dir_context ctx;
+	const char *name;
+	char match_name[NAME_MAX+1];
+	int found; /*-1: not found, 0: found*/
+	int count;
+};
+
+/* The dentry cache is just so we have properly sized dentries */
+static struct kmem_cache *esdfs_dentry_cachep;
+
+int esdfs_init_dentry_cache(void)
+{
+	esdfs_dentry_cachep =
+		kmem_cache_create("esdfs_dentry",
+				  sizeof(struct esdfs_dentry_info),
+				  0, SLAB_RECLAIM_ACCOUNT, NULL);
+
+	return esdfs_dentry_cachep ? 0 : -ENOMEM;
+}
+
+void esdfs_destroy_dentry_cache(void)
+{
+	if (esdfs_dentry_cachep)
+		kmem_cache_destroy(esdfs_dentry_cachep);
+}
+
+void esdfs_free_dentry_private_data(struct dentry *dentry)
+{
+	kmem_cache_free(esdfs_dentry_cachep, dentry->d_fsdata);
+	dentry->d_fsdata = NULL;
+}
+
+/* allocate new dentry private data */
+int esdfs_new_dentry_private_data(struct dentry *dentry)
+{
+	struct esdfs_dentry_info *info = ESDFS_D(dentry);
+
+	/* use zalloc to init dentry_info.lower_path */
+	info = kmem_cache_zalloc(esdfs_dentry_cachep, GFP_ATOMIC);
+	if (!info)
+		return -ENOMEM;
+
+	spin_lock_init(&info->lock);
+	dentry->d_fsdata = info;
+
+	return 0;
+}
+
+struct inode_data {
+	struct inode *lower_inode;
+	uint32_t id;
+};
+
+/* Multiple obb files can point to the same lower file */
+static int esdfs_inode_test(struct inode *inode, void *candidate_data)
+{
+	struct inode *current_lower_inode = esdfs_lower_inode(inode);
+	uint32_t current_userid = ESDFS_I(inode)->userid;
+	struct inode_data *data = (struct inode_data *)candidate_data;
+
+	if (current_lower_inode == data->lower_inode
+			&& current_userid == data->id)
+		return 1; /* found a match */
+	else
+		return 0; /* no match */
+}
+
+static int esdfs_inode_set(struct inode *inode, void *lower_inode)
+{
+	/* we do actual inode initialization in esdfs_iget */
+	return 0;
+}
+
+struct inode *esdfs_iget(struct super_block *sb, struct inode *lower_inode,
+						uint32_t id)
+{
+	struct esdfs_inode_info *info;
+	struct inode_data data;
+	struct inode *inode; /* the new inode to return */
+
+	if (!igrab(lower_inode))
+		return ERR_PTR(-ESTALE);
+	data.id = id;
+	data.lower_inode = lower_inode;
+	inode = iget5_locked(sb, /* our superblock */
+			     /*
+			      * hashval: we use inode number, but we can
+			      * also use "(unsigned long)lower_inode"
+			      * instead.
+			      */
+			     lower_inode->i_ino, /* hashval */
+			     esdfs_inode_test,	/* inode comparison function */
+			     esdfs_inode_set, /* inode init function */
+			     &data); /* data passed to test+set fxns */
+	if (!inode) {
+		iput(lower_inode);
+		return ERR_PTR(-ENOMEM);
+	}
+	/* if found a cached inode, then just return it (after iput) */
+	if (!(inode->i_state & I_NEW)) {
+		iput(lower_inode);
+		return inode;
+	}
+
+	/* initialize new inode */
+	info = ESDFS_I(inode);
+	info->tree = ESDFS_TREE_NONE;
+	info->userid = 0;
+	info->appid = 0;
+	info->under_obb = false;
+
+	inode->i_ino = lower_inode->i_ino;
+	esdfs_set_lower_inode(inode, lower_inode);
+
+	inode_inc_iversion(inode);
+
+	/* use different set of inode ops for symlinks & directories */
+	if (S_ISDIR(lower_inode->i_mode))
+		inode->i_op = &esdfs_dir_iops;
+	else if (S_ISLNK(lower_inode->i_mode))
+		inode->i_op = &esdfs_symlink_iops;
+	else
+		inode->i_op = &esdfs_main_iops;
+
+	/* use different set of file ops for directories */
+	if (S_ISDIR(lower_inode->i_mode))
+		inode->i_fop = &esdfs_dir_fops;
+	else
+		inode->i_fop = &esdfs_main_fops;
+
+	inode->i_mapping->a_ops = &esdfs_aops;
+
+	inode->i_atime.tv_sec = 0;
+	inode->i_atime.tv_nsec = 0;
+	inode->i_mtime.tv_sec = 0;
+	inode->i_mtime.tv_nsec = 0;
+	inode->i_ctime.tv_sec = 0;
+	inode->i_ctime.tv_nsec = 0;
+
+	/* properly initialize special inodes */
+	if (S_ISBLK(lower_inode->i_mode) || S_ISCHR(lower_inode->i_mode) ||
+	    S_ISFIFO(lower_inode->i_mode) || S_ISSOCK(lower_inode->i_mode))
+		init_special_inode(inode, lower_inode->i_mode,
+				   lower_inode->i_rdev);
+
+	/* all well, copy inode attributes */
+	esdfs_copy_lower_attr(inode, lower_inode);
+	fsstack_copy_inode_size(inode, lower_inode);
+
+	unlock_new_inode(inode);
+	return inode;
+}
+
+/*
+ * Helper interpose routine, called directly by ->lookup to handle
+ * spliced dentries
+ */
+static struct dentry *__esdfs_interpose(struct dentry *dentry,
+					struct super_block *sb,
+					struct path *lower_path,
+					uint32_t id)
+{
+	struct inode *inode;
+	struct inode *lower_inode;
+	struct super_block *lower_sb;
+	struct dentry *ret_dentry;
+
+	lower_inode = lower_path->dentry->d_inode;
+	lower_sb = esdfs_lower_super(sb);
+
+	/* check that the lower file system didn't cross a mount point */
+	if (lower_inode->i_sb != lower_sb) {
+		ret_dentry = ERR_PTR(-EXDEV);
+		goto out;
+	}
+
+	/*
+	 * We allocate our new inode below by calling esdfs_iget,
+	 * which will initialize some of the new inode's fields
+	 */
+
+	/* inherit lower inode number for esdfs's inode */
+	inode = esdfs_iget(sb, lower_inode, id);
+	if (IS_ERR(inode)) {
+		ret_dentry = ERR_CAST(inode);
+		goto out;
+	}
+
+	ret_dentry = d_splice_alias(inode, dentry);
+	dentry = ret_dentry ?: dentry;
+	if (IS_ERR(dentry))
+		goto out;
+
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(sb)))
+		esdfs_derive_perms(dentry);
+	esdfs_set_perms(inode);
+out:
+	return ret_dentry;
+}
+
+/*
+ * Connect an esdfs inode dentry/inode with several lower ones.  This is
+ * the classic stackable file system "vnode interposition" action.
+ *
+ * @dentry: esdfs's dentry which interposes on lower one
+ * @sb: esdfs's super_block
+ * @lower_path: the lower path (caller does path_get/put)
+ */
+int esdfs_interpose(struct dentry *dentry, struct super_block *sb,
+		     struct path *lower_path, uint32_t id)
+{
+	struct dentry *ret_dentry;
+
+	ret_dentry = __esdfs_interpose(dentry, sb, lower_path, id);
+	return PTR_ERR(ret_dentry);
+}
+
+/*
+ * Main driver function for esdfs's lookup.
+ *
+ * Returns: NULL (ok), ERR_PTR if an error occurred.
+ * Fills in lower_parent_path with <dentry,mnt> on success.
+ */
+static struct dentry *__esdfs_lookup(struct dentry *dentry,
+				     unsigned int flags,
+				     struct path *lower_parent_path,
+				     uint32_t id, bool use_dl)
+{
+	int err = 0;
+	struct vfsmount *lower_dir_mnt;
+	struct dentry *lower_dir_dentry = NULL;
+	struct dentry *lower_dentry;
+	const char *name;
+	struct path lower_path;
+	struct qstr dname;
+	struct dentry *ret_dentry = NULL;
+
+	/* must initialize dentry operations */
+	d_set_d_op(dentry, &esdfs_dops);
+
+	if (IS_ROOT(dentry))
+		goto out;
+
+	if (use_dl)
+		name = ESDFS_SB(dentry->d_sb)->dl_name.name;
+	else
+		name = dentry->d_name.name;
+
+	dname.name = name;
+	dname.len = strlen(name);
+
+	/* now start the actual lookup procedure */
+	lower_dir_dentry = lower_parent_path->dentry;
+	lower_dir_mnt = lower_parent_path->mnt;
+
+	/* if the access is to the Download directory, redirect
+	 * to lower path.
+	 */
+	if (use_dl) {
+		pathcpy(&lower_path, &ESDFS_SB(dentry->d_sb)->dl_path);
+		path_get(&ESDFS_SB(dentry->d_sb)->dl_path);
+	} else {
+		err = esdfs_lookup_nocase(lower_parent_path, &dname,
+					  &lower_path);
+	}
+
+	/* no error: handle positive dentries */
+	if (!err) {
+		esdfs_set_lower_path(dentry, &lower_path);
+		ret_dentry =
+			__esdfs_interpose(dentry, dentry->d_sb,
+						&lower_path, id);
+		if (IS_ERR(ret_dentry)) {
+			err = PTR_ERR(ret_dentry);
+			/* path_put underlying underlying path on error */
+			esdfs_put_reset_lower_paths(dentry);
+		}
+		goto out;
+	}
+
+	/*
+	 * We don't consider ENOENT an error, and we want to return a
+	 * negative dentry.
+	 */
+	if (err && err != -ENOENT)
+		goto out;
+
+	/* instatiate a new negative dentry */
+	/* See if the low-level filesystem might want
+	 * to use its own hash */
+	lower_dentry = d_hash_and_lookup(lower_dir_dentry, &dname);
+	if (IS_ERR(lower_dentry))
+		return lower_dentry;
+
+	if (!lower_dentry) {
+		/* We called vfs_path_lookup earlier, and did not get a negative
+		 * dentry then. Don't confuse the lower filesystem by forcing
+		 * one on it now...
+		 */
+		err = -ENOENT;
+		goto out;
+	}
+
+	lower_path.dentry = lower_dentry;
+	lower_path.mnt = mntget(lower_dir_mnt);
+	esdfs_set_lower_path(dentry, &lower_path);
+
+	/*
+	 * If the intent is to create a file, then don't return an error, so
+	 * the VFS will continue the process of making this negative dentry
+	 * into a positive one.
+	 */
+	if (flags & (LOOKUP_CREATE|LOOKUP_RENAME_TARGET))
+		err = 0;
+
+out:
+	if (err)
+		return ERR_PTR(err);
+	return ret_dentry;
+}
+
+struct dentry *esdfs_lookup(struct inode *dir, struct dentry *dentry,
+			    unsigned int flags)
+{
+	int err;
+	struct dentry *ret, *real_parent, *parent;
+	struct path lower_parent_path, old_lower_parent_path;
+	const struct cred *creds;
+	struct esdfs_sb_info *sbi = ESDFS_SB(dir->i_sb);
+	int use_dl;
+
+	parent = real_parent = dget_parent(dentry);
+
+	/* allocate dentry private data.  We free it in ->d_release */
+	err = esdfs_new_dentry_private_data(dentry);
+	if (err) {
+		ret = ERR_PTR(err);
+		goto out;
+	}
+
+	if (ESDFS_DERIVE_PERMS(sbi)) {
+		err = esdfs_derived_lookup(dentry, &parent);
+		if (err) {
+			ret = ERR_PTR(err);
+			goto out;
+		}
+	}
+
+	esdfs_get_lower_path(parent, &lower_parent_path);
+
+	creds =	esdfs_override_creds(ESDFS_SB(dir->i_sb),
+			ESDFS_I(d_inode(parent)), NULL);
+	if (!creds) {
+		ret = ERR_PTR(-EINVAL);
+		goto out_put;
+	}
+
+	/* Check if the lookup corresponds to the Download directory */
+	use_dl = esdfs_is_dl_lookup(dentry, parent);
+
+	ret = __esdfs_lookup(dentry, flags, &lower_parent_path,
+					ESDFS_I(dir)->userid,
+					use_dl);
+	if (IS_ERR(ret))
+		goto out_cred;
+	if (ret)
+		dentry = ret;
+	if (dentry->d_inode) {
+		fsstack_copy_attr_times(dentry->d_inode,
+					esdfs_lower_inode(dentry->d_inode));
+		/*
+		 * Do not modify the ownership of the lower directory if it
+		 * is the Download directory
+		 */
+		if (!use_dl)
+			esdfs_derive_lower_ownership(dentry,
+						     dentry->d_name.name);
+	}
+	/* update parent directory's atime */
+	fsstack_copy_attr_atime(parent->d_inode,
+				esdfs_lower_inode(parent->d_inode));
+
+	/*
+	 * If this is a pseudo hard link, store the real parent and ensure
+	 * that the link target directory contains any derived contents.
+	 */
+	if (parent != real_parent) {
+		esdfs_get_lower_path(real_parent, &old_lower_parent_path);
+		esdfs_set_lower_parent(dentry, old_lower_parent_path.dentry);
+		esdfs_put_lower_path(real_parent, &old_lower_parent_path);
+		esdfs_derive_mkdir_contents(dentry);
+	}
+out_cred:
+	esdfs_revert_creds(creds, NULL);
+out_put:
+	esdfs_put_lower_path(parent, &lower_parent_path);
+out:
+	dput(parent);
+	if (parent != real_parent)
+		dput(real_parent);
+	return ret;
+}
Index: rpi4-kernel/fs/esdfs/main.c
===================================================================
--- /dev/null
+++ rpi4-kernel/fs/esdfs/main.c
@@ -0,0 +1,725 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+#include <linux/module.h>
+#include <linux/parser.h>
+#include <linux/security.h>
+#include <linux/proc_ns.h>
+
+/*
+ * Derived from first generation "ANDROID_EMU" glue in modifed F2FS driver.
+ */
+enum {
+	Opt_lower_perms,
+	Opt_upper_perms,
+	Opt_derive_none,
+	Opt_derive_legacy,
+	Opt_derive_unified,
+	Opt_derive_multi,
+	Opt_derive_public,
+	Opt_confine,
+	Opt_noconfine,
+	Opt_gid_derivation,
+	Opt_default_normal,
+	Opt_dl_loc,
+	Opt_dl_uid,
+	Opt_dl_gid,
+	Opt_ns_fd,
+
+	/* From sdcardfs */
+	Opt_fsuid,
+	Opt_fsgid,
+	Opt_gid,
+	Opt_debug,
+	Opt_mask,
+	Opt_multiuser,
+	Opt_userid,
+
+	Opt_err,
+};
+
+static match_table_t esdfs_tokens = {
+	{Opt_lower_perms, "lower=%s"},
+	{Opt_upper_perms, "upper=%s"},
+	{Opt_derive_none, "derive=none"},
+	{Opt_derive_legacy, "derive=legacy"},
+	{Opt_derive_unified, "derive=unified"},
+	{Opt_derive_multi, "derive=multi"},
+	{Opt_derive_public, "derive=public"},
+	{Opt_confine, "confine"},
+	{Opt_noconfine, "noconfine"},
+	{Opt_gid_derivation, "derive_gid"},
+	{Opt_default_normal, "default_normal"},
+	{Opt_dl_loc, "dl_loc=%s"},
+	{Opt_dl_uid, "dl_uid=%u"},
+	{Opt_dl_gid, "dl_gid=%u"},
+	{Opt_ns_fd, "ns_fd=%d"},
+	/* compatibility with sdcardfs options */
+	{Opt_fsuid, "fsuid=%u"},
+	{Opt_fsgid, "fsgid=%u"},
+	{Opt_gid, "gid=%u"},
+	{Opt_mask, "mask=%u"},
+	{Opt_userid, "userid=%d"},
+	{Opt_multiuser, "multiuser"},
+	{Opt_gid_derivation, "derive_gid"},
+	{Opt_err, NULL},
+};
+
+struct esdfs_perms esdfs_perms_table[ESDFS_PERMS_TABLE_SIZE] = {
+	/* ESDFS_PERMS_LOWER_DEFAULT */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = AID_MEDIA_RW,
+	  .gid   = AID_MEDIA_RW,
+	  .fmask = 0664,
+	  .dmask = 0775 },
+	/* ESDFS_PERMS_UPPER_LEGACY */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = AID_ROOT,
+	  .gid   = AID_SDCARD_RW,
+	  .fmask = 0664,
+	  .dmask = 0775 },
+	/* ESDFS_PERMS_UPPER_DERIVED */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = AID_ROOT,
+	  .gid   = AID_SDCARD_R,
+	  .fmask = 0660,
+	  .dmask = 0771 },
+	/* ESDFS_PERMS_LOWER_DOWNLOAD */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = -1,
+	  .gid   = -1,
+	  .fmask = 0644,
+	  .dmask = 0711 },
+};
+
+static int parse_perms(struct esdfs_perms *perms, char *args)
+{
+	char *sep = args;
+	char *sepres;
+	int ret;
+
+	if (!sep)
+		return -EINVAL;
+
+	sepres = strsep(&sep, ":");
+	if (!sep)
+		return -EINVAL;
+	ret = kstrtou32(sepres, 0, &perms->uid);
+	if (ret)
+		return ret;
+
+	sepres = strsep(&sep, ":");
+	if (!sep)
+		return -EINVAL;
+	ret = kstrtou32(sepres, 0, &perms->gid);
+	if (ret)
+		return ret;
+
+	sepres = strsep(&sep, ":");
+	if (!sep)
+		return -EINVAL;
+	ret = kstrtou16(sepres, 8, &perms->fmask);
+	if (ret)
+		return ret;
+
+	sepres = strsep(&sep, ":");
+	ret = kstrtou16(sepres, 8, &perms->dmask);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static inline struct user_namespace *to_user_ns(struct ns_common *ns)
+{
+	return container_of(ns, struct user_namespace, ns);
+}
+
+static struct user_namespace *get_ns_from_fd(int fd)
+{
+	struct file *file;
+	struct ns_common *ns;
+	struct user_namespace *user_ns = ERR_PTR(-EINVAL);
+
+	file = proc_ns_fget(fd);
+	if (IS_ERR(file))
+		return ERR_CAST(file);
+
+	ns = get_proc_ns(file_inode(file));
+#ifdef CONFIG_USER_NS
+	if (ns->ops == &userns_operations)
+		user_ns = to_user_ns(ns);
+#endif
+	fput(file);
+	return user_ns;
+}
+
+static int parse_options(struct super_block *sb, char *options)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(sb);
+	substring_t args[MAX_OPT_ARGS];
+	char *p;
+	int option;
+
+	if (!options)
+		return 0;
+
+	while ((p = strsep(&options, ",")) != NULL) {
+		int token;
+
+		if (!*p)
+			continue;
+		/*
+		 * Initialize args struct so we know whether arg was
+		 * found; some options take optional arguments.
+		 */
+		args[0].to = args[0].from = NULL;
+		token = match_token(p, esdfs_tokens, args);
+
+		switch (token) {
+		case Opt_lower_perms:
+			if (args->from) {
+				int ret;
+				char *perms = match_strdup(args);
+
+				ret = parse_perms(&sbi->lower_perms, perms);
+				kfree(perms);
+
+				if (ret)
+					return -EINVAL;
+			} else
+				return -EINVAL;
+			break;
+		case Opt_upper_perms:
+			if (args->from) {
+				int ret;
+				char *perms = match_strdup(args);
+
+				ret = parse_perms(&sbi->upper_perms, perms);
+				kfree(perms);
+
+				if (ret)
+					return -EINVAL;
+			} else
+				return -EINVAL;
+			break;
+		case Opt_derive_none:
+			clear_opt(sbi, DERIVE_LEGACY);
+			clear_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_derive_legacy:
+			set_opt(sbi, DERIVE_LEGACY);
+			clear_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_derive_unified:
+			clear_opt(sbi, DERIVE_LEGACY);
+			set_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			set_opt(sbi, DERIVE_CONFINE);	/* confine by default */
+			break;
+		case Opt_derive_multi:
+		case Opt_multiuser:
+			set_opt(sbi, DERIVE_LEGACY);
+			clear_opt(sbi, DERIVE_UNIFIED);
+			set_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_derive_public:
+			clear_opt(sbi, DERIVE_LEGACY);
+			set_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			set_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_confine:
+			set_opt(sbi, DERIVE_CONFINE);
+			break;
+		case Opt_noconfine:
+			clear_opt(sbi, DERIVE_CONFINE);
+			break;
+		/* for compatibility with sdcardfs options */
+		case Opt_gid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->upper_perms.raw_gid = option;
+			break;
+		case Opt_userid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->upper_perms.raw_uid = option;
+			break;
+		case Opt_mask:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->upper_perms.dmask = 0775 & ~option;
+			sbi->upper_perms.fmask = 0775 & ~option;
+			break;
+		case Opt_fsuid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_perms.raw_uid = option;
+			break;
+		case Opt_fsgid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_perms.raw_gid = option;
+			break;
+		case Opt_gid_derivation:
+			set_opt(sbi, GID_DERIVATION);
+			break;
+		case Opt_default_normal:
+			set_opt(sbi, DEFAULT_NORMAL);
+			break;
+		case Opt_dl_loc:
+			set_opt(sbi, SPECIAL_DOWNLOAD);
+			sbi->dl_loc = match_strdup(args);
+			break;
+		case Opt_dl_uid:
+			set_opt(sbi, SPECIAL_DOWNLOAD);
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_dl_perms.raw_uid = option;
+			break;
+		case Opt_dl_gid:
+			set_opt(sbi, SPECIAL_DOWNLOAD);
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_dl_perms.raw_gid = option;
+			break;
+		case Opt_ns_fd:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->ns_fd = option;
+			break;
+		default:
+			esdfs_msg(sb, KERN_ERR,
+			  "unrecognized mount option \"%s\" or missing value\n",
+			  p);
+			return -EINVAL;
+		}
+	}
+	return 0;
+}
+
+static int interpret_perms(struct esdfs_sb_info *sbi, struct esdfs_perms *perms)
+{
+	if (perms->raw_uid == -1) {
+		perms->raw_uid = perms->uid;
+	} else {
+		perms->uid = esdfs_from_local_uid(sbi, perms->raw_uid);
+		if (perms->uid == -1)
+			return -EINVAL;
+	}
+
+	if (perms->raw_gid == -1) {
+		perms->raw_gid = perms->gid;
+	} else {
+		perms->gid = esdfs_from_local_gid(sbi, perms->raw_gid);
+		if (perms->gid == -1)
+			return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * There is no need to lock the esdfs_super_info's rwsem as there is no
+ * way anyone can have a reference to the superblock at this point in time.
+ */
+static int esdfs_read_super(struct super_block *sb, const char *dev_name,
+		void *raw_data, int silent)
+{
+	int err = 0;
+	struct super_block *lower_sb;
+	struct path lower_path;
+	struct esdfs_sb_info *sbi;
+	struct inode *inode;
+	struct dentry *lower_dl_dentry;
+	struct user_namespace *user_ns;
+	kuid_t dl_kuid = INVALID_UID;
+	kgid_t dl_kgid = INVALID_GID;
+
+	if (!dev_name) {
+		esdfs_msg(sb, KERN_ERR, "missing dev_name argument\n");
+		err = -EINVAL;
+		goto out;
+	}
+
+	/* parse lower path */
+	err = kern_path(dev_name, LOOKUP_FOLLOW | LOOKUP_DIRECTORY,
+			&lower_path);
+	if (err) {
+		esdfs_msg(sb, KERN_ERR,
+			"error accessing lower directory '%s'\n", dev_name);
+		goto out;
+	}
+
+	/* allocate superblock private data */
+	sb->s_fs_info = kzalloc(sizeof(struct esdfs_sb_info), GFP_KERNEL);
+	sbi = ESDFS_SB(sb);
+	if (!sbi) {
+		esdfs_msg(sb, KERN_CRIT, "read_super: out of memory\n");
+		err = -ENOMEM;
+		goto out_pput;
+	}
+	INIT_LIST_HEAD(&sbi->s_list);
+
+	/* set defaults and then parse the mount options */
+
+	sbi->ns_fd = -1;
+
+	/* make public default */
+	clear_opt(sbi, DERIVE_LEGACY);
+	set_opt(sbi, DERIVE_UNIFIED);
+	clear_opt(sbi, DERIVE_MULTI);
+	set_opt(sbi, DERIVE_PUBLIC);
+
+	memcpy(&sbi->lower_perms,
+	       &esdfs_perms_table[ESDFS_PERMS_LOWER_DEFAULT],
+	       sizeof(struct esdfs_perms));
+	if (ESDFS_DERIVE_PERMS(sbi))
+		memcpy(&sbi->upper_perms,
+		       &esdfs_perms_table[ESDFS_PERMS_UPPER_DERIVED],
+		       sizeof(struct esdfs_perms));
+	else
+		memcpy(&sbi->upper_perms,
+		       &esdfs_perms_table[ESDFS_PERMS_UPPER_LEGACY],
+		       sizeof(struct esdfs_perms));
+
+	memcpy(&sbi->lower_dl_perms,
+	       &esdfs_perms_table[ESDFS_PERMS_LOWER_DOWNLOAD],
+	       sizeof(struct esdfs_perms));
+
+	err = parse_options(sb, (char *)raw_data);
+	if (err)
+		goto out_free;
+
+	/* Initialize special namespace for lower Downloads directory */
+	sbi->dl_ns = get_user_ns(current_user_ns());
+
+	if (sbi->ns_fd == -1) {
+		sbi->base_ns = get_user_ns(current_user_ns());
+	} else {
+		user_ns = get_ns_from_fd(sbi->ns_fd);
+		if (IS_ERR(user_ns)) {
+			err = PTR_ERR(user_ns);
+			goto out_free;
+		}
+		sbi->base_ns = get_user_ns(user_ns);
+	}
+	/* interpret all parameters in given namespace */
+	err = interpret_perms(sbi, &sbi->lower_perms);
+	if (err) {
+		pr_err("esdfs: Invalid permissions for lower layer\n");
+		goto out_free;
+	}
+	err = interpret_perms(sbi, &sbi->upper_perms);
+	if (err) {
+		pr_err("esdfs: Invalid permissions for upper layer\n");
+		goto out_free;
+	}
+
+	/* Check if the downloads uid maps into a valid kuid from
+	 * the namespace of the mounting process
+	 */
+	if (sbi->lower_dl_perms.raw_uid != -1) {
+		dl_kuid = make_kuid(sbi->dl_ns,
+				    sbi->lower_dl_perms.raw_uid);
+		if (!uid_valid(dl_kuid)) {
+			pr_err("esdfs: Invalid permissions for dl_uid");
+			err = -EINVAL;
+			goto out_free;
+		}
+	}
+	if (sbi->lower_dl_perms.raw_gid != -1) {
+		dl_kgid = make_kgid(sbi->dl_ns,
+				    sbi->lower_dl_perms.raw_gid);
+		if (!gid_valid(dl_kgid)) {
+			pr_err("esdfs: Invalid permissions for dl_gid");
+			err = -EINVAL;
+			goto out_free;
+		}
+	}
+
+	/* set the lower superblock field of upper superblock */
+	lower_sb = lower_path.dentry->d_sb;
+	atomic_inc(&lower_sb->s_active);
+	esdfs_set_lower_super(sb, lower_sb);
+
+	sb->s_stack_depth = lower_sb->s_stack_depth + 1;
+	if (sb->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {
+		pr_err("esdfs: maximum fs stacking depth exceeded\n");
+		err = -EINVAL;
+		goto out_sput;
+	}
+
+	/* inherit maxbytes from lower file system */
+	sb->s_maxbytes = lower_sb->s_maxbytes;
+
+	/*
+	 * Our c/m/atime granularity is 1 ns because we may stack on file
+	 * systems whose granularity is as good.
+	 */
+	sb->s_time_gran = 1;
+
+	sb->s_op = &esdfs_sops;
+
+	/* get a new inode and allocate our root dentry */
+	inode = esdfs_iget(sb, lower_path.dentry->d_inode, 0);
+	if (IS_ERR(inode)) {
+		err = PTR_ERR(inode);
+		goto out_sput;
+	}
+	sb->s_root = d_make_root(inode);
+	if (!sb->s_root) {
+		err = -ENOMEM;
+		goto out_sput;
+	}
+	d_set_d_op(sb->s_root, &esdfs_dops);
+
+	/* link the upper and lower dentries */
+	sb->s_root->d_fsdata = NULL;
+	err = esdfs_new_dentry_private_data(sb->s_root);
+	if (err)
+		goto out_freeroot;
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD)) {
+		/* parse lower path */
+		err = kern_path(sbi->dl_loc, LOOKUP_FOLLOW | LOOKUP_DIRECTORY,
+				&sbi->dl_path);
+		if (err) {
+			esdfs_msg(sb, KERN_ERR,
+				"error accessing download directory '%s'\n",
+				sbi->dl_loc);
+			goto out_freeroot;
+		}
+
+		lower_dl_dentry = sbi->dl_path.dentry;
+
+		if (!S_ISDIR(lower_dl_dentry->d_inode->i_mode)) {
+			err = -EINVAL;
+			esdfs_msg(sb, KERN_ERR,
+				"dl_loc must be a directory '%s'\n",
+				sbi->dl_loc);
+			goto out_dlput;
+		}
+
+		if (lower_dl_dentry->d_sb != lower_sb) {
+			esdfs_msg(sb, KERN_ERR,
+				"dl_loc must be in the same filesystem '%s'\n",
+				sbi->dl_loc);
+			goto out_dlput;
+		}
+
+		if (!uid_valid(dl_kuid)) {
+			dl_kuid = esdfs_make_kuid(sbi, sbi->lower_perms.uid);
+			sbi->lower_dl_perms.raw_uid = from_kuid(sbi->dl_ns,
+								dl_kuid);
+		}
+		if (!gid_valid(dl_kgid)) {
+			dl_kgid = esdfs_make_kgid(sbi, sbi->lower_perms.gid);
+			sbi->lower_dl_perms.raw_gid = from_kgid(sbi->dl_ns,
+								dl_kgid);
+		}
+		spin_lock(&lower_dl_dentry->d_lock);
+		sbi->dl_name.name = kstrndup(lower_dl_dentry->d_name.name,
+				lower_dl_dentry->d_name.len, GFP_ATOMIC);
+		sbi->dl_name.len = lower_dl_dentry->d_name.len;
+		spin_unlock(&lower_dl_dentry->d_lock);
+	}
+	/* if get here: cannot have error */
+
+	/* set the lower dentries for s_root */
+	esdfs_set_lower_path(sb->s_root, &lower_path);
+
+	/*
+	 * No need to call interpose because we already have a positive
+	 * dentry, which was instantiated by d_make_root.  Just need to
+	 * d_rehash it.
+	 */
+	d_rehash(sb->s_root);
+	if (!silent)
+		esdfs_msg(sb, KERN_INFO, "mounted on top of %s type %s\n",
+			dev_name, lower_sb->s_type->name);
+
+	if (!ESDFS_DERIVE_PERMS(sbi))
+		goto out;
+
+	/* let user know that we ignore this option in older derived modes */
+	if (ESDFS_RESTRICT_PERMS(sbi) &&
+	    memcmp(&sbi->upper_perms,
+		   &esdfs_perms_table[ESDFS_PERMS_UPPER_DERIVED],
+		   sizeof(struct esdfs_perms)))
+		esdfs_msg(sb, KERN_WARNING,
+			"'upper' mount option ignored in this derived mode\n");
+
+	/*
+	 * In Android 3.0 all user conent in the emulated storage tree was
+	 * stored in /data/media.  Android 4.2 introduced multi-user support,
+	 * which required that the primary user's content be migrated from
+	 * /data/media to /data/media/0.  The framework then uses bind mounts
+	 * to create per-process namespaces to isolate each user's tree at
+	 * /data/media/N.  This approach of having each user in a common root
+	 * is now considered "legacy" by the sdcard service.
+	 */
+	if (test_opt(sbi, DERIVE_LEGACY)) {
+		ESDFS_I(inode)->tree = ESDFS_TREE_ROOT_LEGACY;
+		sbi->obb_parent = dget(sb->s_root);
+	/*
+	 * Android 4.4 reorganized this sturcture yet again, so that the
+	 * primary user's content was again at the root.  Secondary users'
+	 * content is found in Android/user/N.  Emulated internal storage still
+	 * seems to use the legacy tree, but secondary external storage uses
+	 * this method.
+	 */
+	} else if (test_opt(sbi, DERIVE_UNIFIED))
+		ESDFS_I(inode)->tree = ESDFS_TREE_ROOT;
+	/*
+	 * Later versions of Android organize user content using quantum
+	 * entanglement, which has a low probability of being supported by
+	 * this driver.
+	 */
+	else
+		esdfs_msg(sb, KERN_WARNING,
+				"unsupported derived permissions mode\n");
+
+	/* initialize root inode */
+	esdfs_derive_perms(sb->s_root);
+	esdfs_set_perms(inode);
+
+	esdfs_add_super(sbi, sb);
+
+	goto out;
+
+out_dlput:
+	path_put(&sbi->dl_path);
+	sbi->dl_path.dentry = NULL;
+	sbi->dl_path.mnt = NULL;
+out_freeroot:
+	dput(sb->s_root);
+	sb->s_root = NULL;
+out_sput:
+	/* drop refs we took earlier */
+	atomic_dec(&lower_sb->s_active);
+out_free:
+	if (sbi->dl_ns)
+		put_user_ns(sbi->dl_ns);
+	if (sbi->base_ns)
+		put_user_ns(sbi->base_ns);
+	kfree(sbi->dl_loc);
+	kfree(ESDFS_SB(sb));
+	sb->s_fs_info = NULL;
+out_pput:
+	path_put(&lower_path);
+
+out:
+	return err;
+}
+
+struct esdfs_mount_private {
+	const char *dev_name;
+	void *raw_data;
+};
+
+static int __esdfs_fill_super(struct super_block *sb, void *_priv, int silent)
+{
+	struct esdfs_mount_private *priv = _priv;
+
+	return esdfs_read_super(sb, priv->dev_name, priv->raw_data, silent);
+}
+
+static struct dentry *esdfs_mount(struct file_system_type *fs_type, int flags,
+				const char *dev_name, void *raw_data)
+{
+	struct esdfs_mount_private priv = {
+		.dev_name = dev_name,
+		.raw_data = raw_data,
+	};
+
+	return mount_nodev(fs_type, flags, &priv, __esdfs_fill_super);
+}
+
+static void esdfs_kill_sb(struct super_block *sb)
+{
+	if (sb->s_fs_info && ESDFS_SB(sb)->obb_parent)
+		dput(ESDFS_SB(sb)->obb_parent);
+	if (sb->s_fs_info && ESDFS_SB(sb)->dl_ns)
+		put_user_ns(ESDFS_SB(sb)->dl_ns);
+	if (sb->s_fs_info && ESDFS_SB(sb)->base_ns)
+		put_user_ns(ESDFS_SB(sb)->base_ns);
+	if (sb->s_fs_info) {
+		kfree(ESDFS_SB(sb)->dl_loc);
+		kfree(ESDFS_SB(sb)->dl_name.name);
+		path_put(&ESDFS_SB(sb)->dl_path);
+	}
+
+	kill_anon_super(sb);
+}
+
+static struct file_system_type esdfs_fs_type = {
+	.owner		= THIS_MODULE,
+	.name		= ESDFS_NAME,
+	.mount		= esdfs_mount,
+	.kill_sb	= esdfs_kill_sb,
+	.fs_flags	= 0,
+};
+MODULE_ALIAS_FS(ESDFS_NAME);
+
+static int __init init_esdfs_fs(void)
+{
+	int err;
+
+	pr_info("Registering esdfs " ESDFS_VERSION "\n");
+
+	esdfs_init_package_list();
+
+	err = esdfs_init_inode_cache();
+	if (err)
+		goto out;
+	err = esdfs_init_dentry_cache();
+	if (err)
+		goto out;
+	err = register_filesystem(&esdfs_fs_type);
+out:
+	if (err) {
+		esdfs_destroy_inode_cache();
+		esdfs_destroy_dentry_cache();
+		esdfs_destroy_package_list();
+	}
+	return err;
+}
+
+static void __exit exit_esdfs_fs(void)
+{
+	esdfs_destroy_inode_cache();
+	esdfs_destroy_dentry_cache();
+	esdfs_destroy_package_list();
+	unregister_filesystem(&esdfs_fs_type);
+	pr_info("Completed esdfs module unload\n");
+}
+
+MODULE_AUTHOR("Erez Zadok, Filesystems and Storage Lab, Stony Brook University"
+	      " (http://www.fsl.cs.sunysb.edu/)");
+MODULE_DESCRIPTION("esdfs " ESDFS_VERSION);
+MODULE_LICENSE("GPL");
+
+module_init(init_esdfs_fs);
+module_exit(exit_esdfs_fs);
Index: rpi4-kernel/fs/esdfs/mmap.c
===================================================================
--- /dev/null
+++ rpi4-kernel/fs/esdfs/mmap.c
@@ -0,0 +1,98 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014, 2016 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+static vm_fault_t esdfs_fault(struct vm_fault *vmf)
+{
+	vm_fault_t err;
+	struct file *file;
+	const struct vm_operations_struct *lower_vm_ops;
+	struct esdfs_sb_info *sbi;
+	const struct cred *creds;
+	const struct vm_area_struct *vma = vmf->vma;
+
+	file = (struct file *)vma->vm_private_data;
+	sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	creds = esdfs_override_creds(sbi, ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return VM_FAULT_OOM;
+
+	lower_vm_ops = ESDFS_F(file)->lower_vm_ops;
+	BUG_ON(!lower_vm_ops);
+	err = lower_vm_ops->fault(vmf);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static void esdfs_vm_open(struct vm_area_struct *vma)
+{
+	struct file *file = (struct file *)vma->vm_private_data;
+
+	get_file(file);
+}
+
+static void esdfs_vm_close(struct vm_area_struct *vma)
+{
+	struct file *file = (struct file *)vma->vm_private_data;
+
+	fput(file);
+}
+
+static vm_fault_t esdfs_page_mkwrite(struct vm_fault *vmf)
+{
+	vm_fault_t err = 0;
+	struct file *file;
+	const struct vm_operations_struct *lower_vm_ops;
+	struct esdfs_sb_info *sbi;
+	const struct cred *creds;
+	const struct vm_area_struct *vma = vmf->vma;
+
+	file = (struct file *)vma->vm_private_data;
+	sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	creds = esdfs_override_creds(sbi, ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return VM_FAULT_OOM;
+
+	lower_vm_ops = ESDFS_F(file)->lower_vm_ops;
+	BUG_ON(!lower_vm_ops);
+	if (!lower_vm_ops->page_mkwrite)
+		goto out;
+
+	err = lower_vm_ops->page_mkwrite(vmf);
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static ssize_t esdfs_direct_IO(struct kiocb *iocb,
+				struct iov_iter *iter)
+{
+	/*
+	 * This function should never be called directly.  We need it
+	 * to exist, to get past a check in open_check_o_direct(),
+	 * which is called from do_last().
+	 */
+	return -EINVAL;
+}
+
+const struct address_space_operations esdfs_aops = {
+	.direct_IO = esdfs_direct_IO,
+};
+
+const struct vm_operations_struct esdfs_vm_ops = {
+	.fault		= esdfs_fault,
+	.page_mkwrite	= esdfs_page_mkwrite,
+	.open		= esdfs_vm_open,
+	.close		= esdfs_vm_close,
+};
Index: rpi4-kernel/fs/esdfs/super.c
===================================================================
--- /dev/null
+++ rpi4-kernel/fs/esdfs/super.c
@@ -0,0 +1,290 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+/*
+ * The inode cache is used with alloc_inode for both our inode info and the
+ * vfs inode.
+ */
+static struct kmem_cache *esdfs_inode_cachep;
+static LIST_HEAD(esdfs_list);
+static DEFINE_SPINLOCK(esdfs_list_lock);
+
+void esdfs_msg(struct super_block *sb, const char *level, const char *fmt, ...)
+{
+	struct va_format vaf;
+	va_list args;
+
+	va_start(args, fmt);
+	vaf.fmt = fmt;
+	vaf.va = &args;
+	printk("%sESDFS-fs (%s): %pV", level, sb->s_id, &vaf);
+	va_end(args);
+}
+
+void esdfs_add_super(struct esdfs_sb_info *sbi, struct super_block *sb)
+{
+	sbi->s_sb = sb;
+
+	spin_lock(&esdfs_list_lock);
+	list_add_tail(&sbi->s_list, &esdfs_list);
+	spin_unlock(&esdfs_list_lock);
+}
+
+static void esdfs_remove_super(struct esdfs_sb_info *sbi)
+{
+	spin_lock(&esdfs_list_lock);
+	list_del(&sbi->s_list);
+	spin_unlock(&esdfs_list_lock);
+}
+
+void esdfs_truncate_share(struct super_block *sb, struct inode *lower_inode,
+				loff_t newsize)
+{
+	struct list_head *p;
+	struct esdfs_sb_info *sbi;
+	struct super_block *lower_sb = lower_inode->i_sb;
+	struct inode *inode;
+
+	spin_lock(&esdfs_list_lock);
+	p = esdfs_list.next;
+	while (p != &esdfs_list) {
+		sbi = list_entry(p, struct esdfs_sb_info, s_list);
+		if (sbi->s_sb == sb || sbi->lower_sb != lower_sb) {
+			p = p->next;
+			continue;
+		}
+		spin_unlock(&esdfs_list_lock);
+		inode = ilookup(sbi->s_sb, lower_inode->i_ino);
+		if (inode) {
+			truncate_setsize(inode, newsize);
+			iput(inode);
+		}
+		spin_lock(&esdfs_list_lock);
+		p = p->next;
+	}
+	spin_unlock(&esdfs_list_lock);
+}
+
+/* final actions when unmounting a file system */
+static void esdfs_put_super(struct super_block *sb)
+{
+	struct esdfs_sb_info *spd;
+	struct super_block *s;
+
+	spd = ESDFS_SB(sb);
+	if (!spd)
+		return;
+
+	/* decrement lower super references */
+	s = esdfs_lower_super(sb);
+	esdfs_set_lower_super(sb, NULL);
+	atomic_dec(&s->s_active);
+
+	esdfs_remove_super(spd);
+
+	kfree(spd);
+	sb->s_fs_info = NULL;
+}
+
+static int esdfs_statfs(struct dentry *dentry, struct kstatfs *buf)
+{
+	int err;
+	struct path lower_path;
+	struct inode *inode = dentry->d_inode;
+
+	if (test_opt(ESDFS_SB(inode->i_sb), ACCESS_DISABLE))
+		return -ENOENT;
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	err = vfs_statfs(&lower_path, buf);
+	esdfs_put_lower_path(dentry, &lower_path);
+
+	/* set return buf to our f/s to avoid confusing user-level utils */
+	buf->f_type = ESDFS_SUPER_MAGIC;
+
+	return err;
+}
+
+/*
+ * @flags: numeric mount options
+ * @options: mount options string
+ */
+static int esdfs_remount_fs(struct super_block *sb, int *flags, char *options)
+{
+	int err = 0;
+
+	/*
+	 * The VFS will take care of "ro" and "rw" flags among others.  We
+	 * can safely accept a few flags (RDONLY, MANDLOCK), and honor
+	 * SILENT, but anything else left over is an error.
+	 */
+	if ((*flags & ~(MS_RDONLY | MS_MANDLOCK | MS_SILENT)) != 0) {
+		esdfs_msg(sb, KERN_ERR, "remount flags 0x%x unsupported\n",
+			*flags);
+		err = -EINVAL;
+	}
+
+	return err;
+}
+
+/*
+ * Called by iput() when the inode reference count reached zero
+ * and the inode is not hashed anywhere.  Used to clear anything
+ * that needs to be, before the inode is completely destroyed and put
+ * on the inode free list.
+ */
+static void esdfs_evict_inode(struct inode *inode)
+{
+	struct inode *lower_inode;
+
+	truncate_inode_pages(&inode->i_data, 0);
+	clear_inode(inode);
+	/*
+	 * Decrement a reference to a lower_inode, which was incremented
+	 * by our read_inode when it was created initially.
+	 */
+	lower_inode = esdfs_lower_inode(inode);
+	esdfs_set_lower_inode(inode, NULL);
+	iput(lower_inode);
+}
+
+static struct inode *esdfs_alloc_inode(struct super_block *sb)
+{
+	struct esdfs_inode_info *i;
+
+	i = kmem_cache_alloc(esdfs_inode_cachep, GFP_KERNEL);
+	if (!i)
+		return NULL;
+
+	/* memset everything up to the inode to 0 */
+	memset(i, 0, offsetof(struct esdfs_inode_info, vfs_inode));
+
+	inode_set_iversion(&i->vfs_inode, 1);
+	return &i->vfs_inode;
+}
+
+static void i_callback(struct rcu_head *head)
+{
+	struct inode *inode = container_of(head, struct inode, i_rcu);
+
+	kmem_cache_free(esdfs_inode_cachep, ESDFS_I(inode));
+}
+
+static void esdfs_destroy_inode(struct inode *inode)
+{
+	call_rcu(&inode->i_rcu, i_callback);
+}
+
+/* esdfs inode cache constructor */
+static void init_once(void *obj)
+{
+	struct esdfs_inode_info *i = obj;
+
+	inode_init_once(&i->vfs_inode);
+}
+
+int esdfs_init_inode_cache(void)
+{
+	int err = 0;
+
+	esdfs_inode_cachep =
+		kmem_cache_create("esdfs_inode_cache",
+				  sizeof(struct esdfs_inode_info), 0,
+				  SLAB_RECLAIM_ACCOUNT, init_once);
+	if (!esdfs_inode_cachep)
+		err = -ENOMEM;
+	return err;
+}
+
+/* esdfs inode cache destructor */
+void esdfs_destroy_inode_cache(void)
+{
+	if (esdfs_inode_cachep)
+		kmem_cache_destroy(esdfs_inode_cachep);
+}
+
+/*
+ * Used only in nfs, to kill any pending RPC tasks, so that subsequent
+ * code can actually succeed and won't leave tasks that need handling.
+ */
+static void esdfs_umount_begin(struct super_block *sb)
+{
+	struct super_block *lower_sb;
+
+	lower_sb = esdfs_lower_super(sb);
+	if (lower_sb && lower_sb->s_op && lower_sb->s_op->umount_begin)
+		lower_sb->s_op->umount_begin(lower_sb);
+}
+
+static int esdfs_show_options(struct seq_file *seq, struct dentry *root)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(root->d_sb);
+
+	if (memcmp(&sbi->lower_perms,
+		   &esdfs_perms_table[ESDFS_PERMS_LOWER_DEFAULT],
+		   sizeof(struct esdfs_perms)))
+		seq_printf(seq, ",lower=%u:%u:%ho:%ho",
+				sbi->lower_perms.raw_uid,
+				sbi->lower_perms.raw_gid,
+				sbi->lower_perms.fmask,
+				sbi->lower_perms.dmask);
+
+	if (memcmp(&sbi->upper_perms,
+		   &esdfs_perms_table[ESDFS_PERMS_UPPER_LEGACY],
+		   sizeof(struct esdfs_perms)))
+		seq_printf(seq, ",upper=%u:%u:%ho:%ho",
+				sbi->upper_perms.raw_uid,
+				sbi->upper_perms.raw_gid,
+				sbi->upper_perms.fmask,
+				sbi->upper_perms.dmask);
+
+	if (test_opt(sbi, DERIVE_PUBLIC))
+		seq_puts(seq, ",derive=public");
+	else if (test_opt(sbi, DERIVE_MULTI))
+		seq_puts(seq, ",derive=multi");
+	else if (test_opt(sbi, DERIVE_UNIFIED))
+		seq_puts(seq, ",derive=unified");
+	else if (test_opt(sbi, DERIVE_LEGACY))
+		seq_puts(seq, ",derive=legacy");
+	else
+		seq_puts(seq, ",derive=none");
+
+	if (test_opt(sbi, DERIVE_CONFINE))
+		seq_puts(seq, ",confine");
+	else
+		seq_puts(seq, ",noconfine");
+	if (test_opt(sbi, GID_DERIVATION))
+		seq_puts(seq, ",derive_gid");
+	if (test_opt(sbi, DEFAULT_NORMAL))
+		seq_puts(seq, ",default_normal");
+	if (test_opt(sbi, SPECIAL_DOWNLOAD)) {
+		seq_printf(seq, ",dl_loc=%s", sbi->dl_loc);
+		seq_printf(seq, ",dl_uid=%d", sbi->lower_dl_perms.raw_uid);
+		seq_printf(seq, ",dl_gid=%d", sbi->lower_dl_perms.raw_gid);
+	}
+	return 0;
+}
+
+const struct super_operations esdfs_sops = {
+	.put_super	= esdfs_put_super,
+	.statfs		= esdfs_statfs,
+	.remount_fs	= esdfs_remount_fs,
+	.evict_inode	= esdfs_evict_inode,
+	.umount_begin	= esdfs_umount_begin,
+	.show_options	= esdfs_show_options,
+	.alloc_inode	= esdfs_alloc_inode,
+	.destroy_inode	= esdfs_destroy_inode,
+	.drop_inode	= generic_delete_inode,
+};
Index: rpi4-kernel/include/linux/low-mem-notify.h
===================================================================
--- /dev/null
+++ rpi4-kernel/include/linux/low-mem-notify.h
@@ -0,0 +1,22 @@
+#ifndef _LINUX_LOW_MEM_NOTIFY_H
+#define _LINUX_LOW_MEM_NOTIFY_H
+
+#include <linux/types.h>
+
+#ifdef CONFIG_LOW_MEM_NOTIFY
+extern const struct file_operations low_mem_notify_fops;
+
+void low_mem_notify(void);
+bool low_mem_check(void);
+#else
+static inline void low_mem_notify(void)
+{
+}
+
+static inline bool low_mem_check(void)
+{
+	return false;
+}
+#endif
+
+#endif
Index: rpi4-kernel/include/linux/mm.h
===================================================================
--- rpi4-kernel.orig/include/linux/mm.h
+++ rpi4-kernel/include/linux/mm.h
@@ -202,6 +202,7 @@ static inline void __mm_zero_struct_page
 #define DEFAULT_MAX_MAP_COUNT	(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)
 
 extern int sysctl_max_map_count;
+extern int sysctl_mmap_noexec_taint;
 
 extern unsigned long sysctl_user_reserve_kbytes;
 extern unsigned long sysctl_admin_reserve_kbytes;
@@ -2555,7 +2556,7 @@ static inline int vma_adjust(struct vm_a
 extern struct vm_area_struct *vma_merge(struct mm_struct *,
 	struct vm_area_struct *prev, unsigned long addr, unsigned long end,
 	unsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,
-	struct mempolicy *, struct vm_userfaultfd_ctx);
+	struct mempolicy *, struct vm_userfaultfd_ctx, const char __user *);
 extern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);
 extern int __split_vma(struct mm_struct *, struct vm_area_struct *,
 	unsigned long addr, int new_below);
@@ -3262,6 +3263,9 @@ unsigned long wp_shared_mapping_range(st
 #endif
 
 extern int sysctl_nr_trim_pages;
+extern int min_filelist_kbytes;
+extern int min_filelist_kbytes_handler(struct ctl_table *table, int write,
+  void *buf, size_t *len, loff_t *pos);
 
 #ifdef CONFIG_PRINTK
 void mem_dump_obj(void *object);
Index: rpi4-kernel/mm/vmscan.c
===================================================================
--- rpi4-kernel.orig/mm/vmscan.c
+++ rpi4-kernel/mm/vmscan.c
@@ -188,6 +188,28 @@ static void set_task_reclaim_state(struc
 	task->reclaim_state = rs;
 }
 
+int min_filelist_kbytes_handler(struct ctl_table *table, int write,
+        void *buf, size_t *len, loff_t *pos)
+{
+  size_t written;
+
+  if (!lru_gen_enabled() || write)
+    return proc_dointvec(table, write, buf, len, pos);
+
+  if (!*len || *pos) {
+    *len = 0;
+    return 0;
+  }
+
+  written = min_t(size_t, 2, *len);
+  memcpy(buf, "0\n", written);
+
+  *len = written;
+  *pos = written;
+
+  return 0;
+}
+
 static LIST_HEAD(shrinker_list);
 static DECLARE_RWSEM(shrinker_rwsem);
 
@@ -2498,6 +2520,28 @@ enum scan_balance {
 };
 
 /*
+ * Low watermark used to prevent fscache thrashing during low memory.
+ */
+int min_filelist_kbytes;
+
+/*
+ * Check low watermark used to prevent fscache thrashing during low memory.
+ */
+static int file_is_low(struct lruvec *lruvec)
+{
+  unsigned long size;
+
+  if (!mem_cgroup_disabled())
+    return false;
+
+  size = node_page_state(lruvec_pgdat(lruvec), NR_ACTIVE_FILE);
+  size += node_page_state(lruvec_pgdat(lruvec), NR_INACTIVE_FILE);
+  size <<= (PAGE_SHIFT - 10);
+
+  return size < min_filelist_kbytes;
+}
+
+/*
  * Determine how aggressively the anon and file LRU lists should be
  * scanned.  The relative value of each set of LRU lists is determined
  * by looking at the fraction of the pages scanned we did rotate back
@@ -2537,6 +2581,15 @@ static void get_scan_count(struct lruvec
 		goto out;
 	}
 
+  /*
+   * Do not scan file pages when swap is allowed by __GFP_IO and
+   * file page count is low.
+   */
+  if ((sc->gfp_mask & __GFP_IO) && file_is_low(lruvec)) {
+    scan_balance = SCAN_ANON;
+    goto out;
+  }
+
 	/*
 	 * Do not apply any pressure balancing cleverness when the
 	 * system is close to OOM, scan both anon and file equally
Index: rpi4-kernel/fs/overlayfs/super.c
===================================================================
--- rpi4-kernel.orig/fs/overlayfs/super.c
+++ rpi4-kernel/fs/overlayfs/super.c
@@ -53,6 +53,11 @@ module_param_named(xino_auto, ovl_xino_a
 MODULE_PARM_DESC(xino_auto,
 		 "Auto enable xino feature");
 
+static bool __read_mostly ovl_override_creds_def = true;
+module_param_named(override_creds, ovl_override_creds_def, bool, 0644);
+MODULE_PARM_DESC(ovl_override_creds_def,
+		 "Use mounter's credentials for accesses");
+
 static void ovl_entry_stack_free(struct ovl_entry *oe)
 {
 	unsigned int i;
@@ -388,6 +393,9 @@ static int ovl_show_options(struct seq_f
 		seq_puts(m, ",volatile");
 	if (ofs->config.userxattr)
 		seq_puts(m, ",userxattr");
+  if (ofs->config.override_creds != ovl_override_creds_def)
+    seq_show_option(m, "override_creds",
+      ofs->config.override_creds ? "on" : "off");
 	return 0;
 }
 
@@ -443,6 +451,8 @@ enum {
 	OPT_METACOPY_ON,
 	OPT_METACOPY_OFF,
 	OPT_VOLATILE,
+	OPT_OVERRIDE_CREDS_ON,
+	OPT_OVERRIDE_CREDS_OFF,
 	OPT_ERR,
 };
 
@@ -465,6 +475,8 @@ static const match_table_t ovl_tokens =
 	{OPT_METACOPY_ON,		"metacopy=on"},
 	{OPT_METACOPY_OFF,		"metacopy=off"},
 	{OPT_VOLATILE,			"volatile"},
+	{OPT_OVERRIDE_CREDS_ON,		"override_creds=on"},
+	{OPT_OVERRIDE_CREDS_OFF,	"override_creds=off"},
 	{OPT_ERR,			NULL}
 };
 
@@ -524,6 +536,7 @@ static int ovl_parse_opt(char *opt, stru
 	config->redirect_mode = kstrdup(ovl_redirect_mode_def(), GFP_KERNEL);
 	if (!config->redirect_mode)
 		return -ENOMEM;
+	config->override_creds = ovl_override_creds_def;
 
 	while ((p = ovl_next_opt(&opt)) != NULL) {
 		int token;
@@ -625,6 +638,14 @@ static int ovl_parse_opt(char *opt, stru
 			config->userxattr = true;
 			break;
 
+		case OPT_OVERRIDE_CREDS_ON:
+			config->override_creds = true;
+			break;
+
+		case OPT_OVERRIDE_CREDS_OFF:
+			config->override_creds = false;
+			break;
+
 		default:
 			pr_err("unrecognized mount option \"%s\" or missing value\n",
 					p);
@@ -793,14 +814,10 @@ retry:
 			goto retry;
 		}
 
-		err = ovl_mkdir_real(dir, &work, attr.ia_mode);
-		if (err)
-			goto out_dput;
-
-		/* Weird filesystem returning with hashed negative (kernfs)? */
-		err = -EINVAL;
-		if (d_really_is_negative(work))
-			goto out_dput;
+		work = ovl_create_real(dir, work, OVL_CATTR(attr.ia_mode));
+		err = PTR_ERR(work);
+		if (IS_ERR(work))
+			goto out_err;
 
 		/*
 		 * Try to remove POSIX ACL xattrs from workdir.  We are good if:
@@ -1012,7 +1029,7 @@ ovl_posix_acl_xattr_get(const struct xat
 			struct dentry *dentry, struct inode *inode,
 			const char *name, void *buffer, size_t size, int flags)
 {
-	return ovl_xattr_get(dentry, inode, handler->name, buffer, size, int flags);
+	return ovl_xattr_get(dentry, inode, handler->name, buffer, size, flags);
 }
 
 static int __maybe_unused
@@ -1073,7 +1090,8 @@ out_acl_release:
 
 static int ovl_own_xattr_get(const struct xattr_handler *handler,
 			     struct dentry *dentry, struct inode *inode,
-			     const char *name, void *buffer, size_t size)
+			     const char *name, void *buffer, size_t size,
+			     int flags)
 {
 	return -EOPNOTSUPP;
 }
@@ -1091,7 +1109,7 @@ static int ovl_other_xattr_get(const str
 			       struct dentry *dentry, struct inode *inode,
 			       const char *name, void *buffer, size_t size, int flags)
 {
-	return ovl_xattr_get(dentry, inode, name, buffer, size, int flags);
+	return ovl_xattr_get(dentry, inode, name, buffer, size, flags);
 }
 
 static int ovl_other_xattr_set(const struct xattr_handler *handler,
@@ -2151,7 +2169,6 @@ static int ovl_fill_super(struct super_b
 	kfree(splitlower);
 
 	sb->s_root = root_dentry;
-
 	return 0;
 
 out_free_oe:
Index: rpi4-kernel/fs/overlayfs/overlayfs.h
===================================================================
--- rpi4-kernel.orig/fs/overlayfs/overlayfs.h
+++ rpi4-kernel/fs/overlayfs/overlayfs.h
@@ -187,7 +187,8 @@ static inline ssize_t ovl_do_getxattr(st
 				      size_t size)
 {
 	const char *name = ovl_xattr(ofs, ox);
-	int err = vfs_getxattr(&init_user_ns, dentry, name, value, size);
+  struct inode *ip = d_inode(dentry);
+	int err = __vfs_getxattr(&init_user_ns, dentry, ip, name, value, size, XATTR_NOSECURITY);
 	int len = (value && err > 0) ? err : 0;
 
 	pr_debug("getxattr(%pd2, \"%s\", \"%*pE\", %zu, 0) = %i\n",
@@ -280,6 +281,7 @@ int ovl_want_write(struct dentry *dentry
 void ovl_drop_write(struct dentry *dentry);
 struct dentry *ovl_workdir(struct dentry *dentry);
 const struct cred *ovl_override_creds(struct super_block *sb);
+void ovl_revert_creds(struct super_block *sb, const struct cred *oldcred);
 int ovl_can_decode_fh(struct super_block *sb);
 struct dentry *ovl_indexdir(struct super_block *sb);
 bool ovl_index_all(struct super_block *sb);
@@ -563,7 +565,6 @@ struct ovl_cattr {
 
 #define OVL_CATTR(m) (&(struct ovl_cattr) { .mode = (m) })
 
-int ovl_mkdir_real(struct inode *dir, struct dentry **newdentry, umode_t mode);
 struct dentry *ovl_create_real(struct inode *dir, struct dentry *newdentry,
 			       struct ovl_cattr *attr);
 int ovl_cleanup(struct inode *dir, struct dentry *dentry);
Index: rpi4-kernel/fs/kernfs/inode.c
===================================================================
--- rpi4-kernel.orig/fs/kernfs/inode.c
+++ rpi4-kernel/fs/kernfs/inode.c
@@ -313,7 +313,7 @@ int kernfs_xattr_set(struct kernfs_node
 
 static int kernfs_vfs_xattr_get(const struct xattr_handler *handler,
 				struct dentry *unused, struct inode *inode,
-				const char *suffix, void *value, size_t size)
+				const char *suffix, void *value, size_t size, int flags)
 {
 	const char *name = xattr_full_name(handler, suffix);
 	struct kernfs_node *kn = inode->i_private;
Index: rpi4-kernel/mm/shmem.c
===================================================================
--- rpi4-kernel.orig/mm/shmem.c
+++ rpi4-kernel/mm/shmem.c
@@ -3208,7 +3208,8 @@ static int shmem_initxattrs(struct inode
 
 static int shmem_xattr_handler_get(const struct xattr_handler *handler,
 				   struct dentry *unused, struct inode *inode,
-				   const char *name, void *buffer, size_t size)
+				   const char *name, void *buffer, size_t size,
+           int flags)
 {
 	struct shmem_inode_info *info = SHMEM_I(inode);
 
Index: rpi4-kernel/fs/ext4/xattr_hurd.c
===================================================================
--- rpi4-kernel.orig/fs/ext4/xattr_hurd.c
+++ rpi4-kernel/fs/ext4/xattr_hurd.c
@@ -21,7 +21,8 @@ ext4_xattr_hurd_list(struct dentry *dent
 static int
 ext4_xattr_hurd_get(const struct xattr_handler *handler,
 		    struct dentry *unused, struct inode *inode,
-		    const char *name, void *buffer, size_t size)
+		    const char *name, void *buffer, size_t size,
+        int flags)
 {
 	if (!test_opt(inode->i_sb, XATTR_USER))
 		return -EOPNOTSUPP;
Index: rpi4-kernel/fs/ext4/xattr_trusted.c
===================================================================
--- rpi4-kernel.orig/fs/ext4/xattr_trusted.c
+++ rpi4-kernel/fs/ext4/xattr_trusted.c
@@ -22,7 +22,7 @@ ext4_xattr_trusted_list(struct dentry *d
 static int
 ext4_xattr_trusted_get(const struct xattr_handler *handler,
 		       struct dentry *unused, struct inode *inode,
-		       const char *name, void *buffer, size_t size)
+		       const char *name, void *buffer, size_t size, int flags)
 {
 	return ext4_xattr_get(inode, EXT4_XATTR_INDEX_TRUSTED,
 			      name, buffer, size);
Index: rpi4-kernel/fs/ext4/xattr_user.c
===================================================================
--- rpi4-kernel.orig/fs/ext4/xattr_user.c
+++ rpi4-kernel/fs/ext4/xattr_user.c
@@ -21,7 +21,8 @@ ext4_xattr_user_list(struct dentry *dent
 static int
 ext4_xattr_user_get(const struct xattr_handler *handler,
 		    struct dentry *unused, struct inode *inode,
-		    const char *name, void *buffer, size_t size)
+		    const char *name, void *buffer, size_t size,
+        int flags)
 {
 	if (!test_opt(inode->i_sb, XATTR_USER))
 		return -EOPNOTSUPP;
Index: rpi4-kernel/include/linux/mm_types.h
===================================================================
--- rpi4-kernel.orig/include/linux/mm_types.h
+++ rpi4-kernel/include/linux/mm_types.h
@@ -351,11 +351,13 @@ struct vm_area_struct {
 	 * For areas with an address space and backing store,
 	 * linkage into the address_space->i_mmap interval tree.
 	 */
-	struct {
-		struct rb_node rb;
-		unsigned long rb_subtree_last;
-	} shared;
-
+  union {
+  	struct {
+  		struct rb_node rb;
+  		unsigned long rb_subtree_last;
+  	} shared;
+    const char __user *anon_name;
+  };
 	/*
 	 * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma
 	 * list, after a COW of one of the file pages.	A MAP_SHARED vma
@@ -809,4 +811,13 @@ typedef struct {
 	unsigned long val;
 } swp_entry_t;
 
+/* Return the name for an anonymous mapping or NULL for a file-backed mapping */
+static inline const char __user *vma_get_anon_name(struct vm_area_struct *vma)
+{
+  if (vma->vm_file)
+    return NULL;
+
+  return vma->anon_name;
+}
+
 #endif /* _LINUX_MM_TYPES_H */
Index: rpi4-kernel/mm/mempolicy.c
===================================================================
--- rpi4-kernel.orig/mm/mempolicy.c
+++ rpi4-kernel/mm/mempolicy.c
@@ -808,7 +808,8 @@ static int mbind_range(struct mm_struct
 			((vmstart - vma->vm_start) >> PAGE_SHIFT);
 		prev = vma_merge(mm, prev, vmstart, vmend, vma->vm_flags,
 				 vma->anon_vma, vma->vm_file, pgoff,
-				 new_pol, vma->vm_userfaultfd_ctx);
+				 new_pol, vma->vm_userfaultfd_ctx,
+         vma_get_anon_name(vma));
 		if (prev) {
 			vma = prev;
 			goto replace;
Index: rpi4-kernel/mm/mmap.c
===================================================================
--- rpi4-kernel.orig/mm/mmap.c
+++ rpi4-kernel/mm/mmap.c
@@ -1029,7 +1029,8 @@ again:
  */
 static inline int is_mergeable_vma(struct vm_area_struct *vma,
 				struct file *file, unsigned long vm_flags,
-				struct vm_userfaultfd_ctx vm_userfaultfd_ctx)
+				struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
+        const char __user *anon_name)
 {
 	/*
 	 * VM_SOFTDIRTY should not prevent from VMA merging, if we
@@ -1047,6 +1048,8 @@ static inline int is_mergeable_vma(struc
 		return 0;
 	if (!is_mergeable_vm_userfaultfd_ctx(vma, vm_userfaultfd_ctx))
 		return 0;
+  if (vma_get_anon_name(vma) != anon_name)
+    return 0;
 	return 1;
 }
 
@@ -1079,9 +1082,10 @@ static int
 can_vma_merge_before(struct vm_area_struct *vma, unsigned long vm_flags,
 		     struct anon_vma *anon_vma, struct file *file,
 		     pgoff_t vm_pgoff,
-		     struct vm_userfaultfd_ctx vm_userfaultfd_ctx)
+		     struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
+         const char __user *anon_name)
 {
-	if (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx) &&
+	if (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx, anon_name) &&
 	    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {
 		if (vma->vm_pgoff == vm_pgoff)
 			return 1;
@@ -1100,9 +1104,10 @@ static int
 can_vma_merge_after(struct vm_area_struct *vma, unsigned long vm_flags,
 		    struct anon_vma *anon_vma, struct file *file,
 		    pgoff_t vm_pgoff,
-		    struct vm_userfaultfd_ctx vm_userfaultfd_ctx)
+		    struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
+        const char __user *anon_name)
 {
-	if (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx) &&
+	if (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx, anon_name) &&
 	    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {
 		pgoff_t vm_pglen;
 		vm_pglen = vma_pages(vma);
@@ -1160,7 +1165,8 @@ struct vm_area_struct *vma_merge(struct
 			unsigned long end, unsigned long vm_flags,
 			struct anon_vma *anon_vma, struct file *file,
 			pgoff_t pgoff, struct mempolicy *policy,
-			struct vm_userfaultfd_ctx vm_userfaultfd_ctx)
+			struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
+      const char __user *anon_name)
 {
 	pgoff_t pglen = (end - addr) >> PAGE_SHIFT;
 	struct vm_area_struct *area, *next;
@@ -1190,7 +1196,8 @@ struct vm_area_struct *vma_merge(struct
 			mpol_equal(vma_policy(prev), policy) &&
 			can_vma_merge_after(prev, vm_flags,
 					    anon_vma, file, pgoff,
-					    vm_userfaultfd_ctx)) {
+					    vm_userfaultfd_ctx,
+              anon_name)) {
 		/*
 		 * OK, it can.  Can we now merge in the successor as well?
 		 */
@@ -1199,7 +1206,8 @@ struct vm_area_struct *vma_merge(struct
 				can_vma_merge_before(next, vm_flags,
 						     anon_vma, file,
 						     pgoff+pglen,
-						     vm_userfaultfd_ctx) &&
+						     vm_userfaultfd_ctx,
+                 anon_name) &&
 				is_mergeable_anon_vma(prev->anon_vma,
 						      next->anon_vma, NULL)) {
 							/* cases 1, 6 */
@@ -1222,7 +1230,8 @@ struct vm_area_struct *vma_merge(struct
 			mpol_equal(policy, vma_policy(next)) &&
 			can_vma_merge_before(next, vm_flags,
 					     anon_vma, file, pgoff+pglen,
-					     vm_userfaultfd_ctx)) {
+					     vm_userfaultfd_ctx,
+               anon_name)) {
 		if (prev && addr < prev->vm_end)	/* case 4 */
 			err = __vma_adjust(prev, prev->vm_start,
 					 addr, prev->vm_pgoff, NULL, next);
@@ -1524,7 +1533,8 @@ unsigned long do_mmap(struct file *file,
 			if (path_noexec(&file->f_path)) {
 				if (vm_flags & VM_EXEC)
 					return -EPERM;
-				vm_flags &= ~VM_MAYEXEC;
+        if (sysctl_mmap_noexec_taint)
+				  vm_flags &= ~VM_MAYEXEC;
 			}
 
 			if (!file->f_op->mmap)
@@ -1759,7 +1769,7 @@ unsigned long mmap_region(struct file *f
 	 * Can we just expand an old mapping?
 	 */
 	vma = vma_merge(mm, prev, addr, addr + len, vm_flags,
-			NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX);
+			NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX, NULL);
 	if (vma)
 		goto out;
 
@@ -1808,7 +1818,8 @@ unsigned long mmap_region(struct file *f
 		 */
 		if (unlikely(vm_flags != vma->vm_flags && prev)) {
 			merge = vma_merge(mm, prev, vma->vm_start, vma->vm_end, vma->vm_flags,
-				NULL, vma->vm_file, vma->vm_pgoff, NULL, NULL_VM_UFFD_CTX);
+				NULL, vma->vm_file, vma->vm_pgoff, NULL, NULL_VM_UFFD_CTX,
+        vma_get_anon_name(vma));
 			if (merge) {
 				/* ->mmap() can change vma->vm_file and fput the original file. So
 				 * fput the vma->vm_file here or we would add an extra fput for file
@@ -3072,7 +3083,7 @@ static int do_brk_flags(unsigned long ad
 
 	/* Can we just expand an old private anonymous mapping? */
 	vma = vma_merge(mm, prev, addr, addr + len, flags,
-			NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX);
+			NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX, NULL);
 	if (vma)
 		goto out;
 
@@ -3265,7 +3276,7 @@ struct vm_area_struct *copy_vma(struct v
 		return NULL;	/* should never get here */
 	new_vma = vma_merge(mm, prev, addr, addr + len, vma->vm_flags,
 			    vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
-			    vma->vm_userfaultfd_ctx);
+			    vma->vm_userfaultfd_ctx, vma_get_anon_name(vma));
 	if (new_vma) {
 		/*
 		 * Source vma may have been merged into new_vma
Index: rpi4-kernel/mm/mprotect.c
===================================================================
--- rpi4-kernel.orig/mm/mprotect.c
+++ rpi4-kernel/mm/mprotect.c
@@ -464,7 +464,7 @@ mprotect_fixup(struct vm_area_struct *vm
 	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
 	*pprev = vma_merge(mm, *pprev, start, end, newflags,
 			   vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
-			   vma->vm_userfaultfd_ctx);
+			   vma->vm_userfaultfd_ctx, vma_get_anon_name(vma));
 	if (*pprev) {
 		vma = *pprev;
 		VM_WARN_ON((vma->vm_flags ^ newflags) & ~VM_SOFTDIRTY);
Index: rpi4-kernel/mm/mlock.c
===================================================================
--- rpi4-kernel.orig/mm/mlock.c
+++ rpi4-kernel/mm/mlock.c
@@ -511,7 +511,7 @@ static int mlock_fixup(struct vm_area_st
 	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
 	*prev = vma_merge(mm, *prev, start, end, newflags, vma->anon_vma,
 			  vma->vm_file, pgoff, vma_policy(vma),
-			  vma->vm_userfaultfd_ctx);
+			  vma->vm_userfaultfd_ctx, vma_get_anon_name(vma));
 	if (*prev) {
 		vma = *prev;
 		goto success;
Index: rpi4-kernel/fs/userfaultfd.c
===================================================================
--- rpi4-kernel.orig/fs/userfaultfd.c
+++ rpi4-kernel/fs/userfaultfd.c
@@ -877,7 +877,8 @@ static int userfaultfd_release(struct in
 				 new_flags, vma->anon_vma,
 				 vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
-				 NULL_VM_UFFD_CTX);
+				 NULL_VM_UFFD_CTX,
+         vma_get_anon_name(vma));
 		if (prev)
 			vma = prev;
 		else
@@ -1436,7 +1437,8 @@ static int userfaultfd_register(struct u
 		prev = vma_merge(mm, prev, start, vma_end, new_flags,
 				 vma->anon_vma, vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
-				 ((struct vm_userfaultfd_ctx){ ctx }));
+				 ((struct vm_userfaultfd_ctx){ ctx }),
+         vma_get_anon_name(vma));
 		if (prev) {
 			vma = prev;
 			goto next;
@@ -1613,7 +1615,8 @@ static int userfaultfd_unregister(struct
 		prev = vma_merge(mm, prev, start, vma_end, new_flags,
 				 vma->anon_vma, vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
-				 NULL_VM_UFFD_CTX);
+				 NULL_VM_UFFD_CTX,
+         vma_get_anon_name(vma));
 		if (prev) {
 			vma = prev;
 			goto next;
Index: rpi4-kernel/fs/proc/task_mmu.c
===================================================================
--- rpi4-kernel.orig/fs/proc/task_mmu.c
+++ rpi4-kernel/fs/proc/task_mmu.c
@@ -123,6 +123,56 @@ static void release_task_mempolicy(struc
 }
 #endif
 
+static void seq_print_vma_name(struct seq_file *m, struct vm_area_struct *vma)
+{
+  const char __user *name = vma_get_anon_name(vma);
+  struct mm_struct *mm = vma->vm_mm;
+
+  unsigned long page_start_vaddr;
+  unsigned long page_offset;
+  unsigned long num_pages;
+  unsigned long max_len = NAME_MAX;
+  int i;
+
+  page_start_vaddr = (unsigned long)name & PAGE_MASK;
+  page_offset = (unsigned long)name - page_start_vaddr;
+  num_pages = DIV_ROUND_UP(page_offset + max_len, PAGE_SIZE);
+
+  seq_puts(m, "[anon:");
+
+  for (i = 0; i < num_pages; i++) {
+    int len;
+    int write_len;
+    const char *kaddr;
+    long pages_pinned;
+    struct page *page;
+
+    pages_pinned = get_user_pages_remote(mm, page_start_vaddr, 1, 0,
+                 &page, NULL, NULL);
+    if (pages_pinned < 1) {
+      seq_puts(m, "<fault>]");
+      return;
+    }
+
+    kaddr = (const char *)kmap(page);
+    len = min(max_len, PAGE_SIZE - page_offset);
+    write_len = strnlen(kaddr + page_offset, len);
+    seq_write(m, kaddr + page_offset, write_len);
+    kunmap(page);
+    put_page(page);
+
+    /* if strnlen hit a null terminator then we're done */
+    if (write_len != len)
+      break;
+
+    max_len -= len;
+    page_offset = 0;
+    page_start_vaddr += PAGE_SIZE;
+  }
+
+  seq_putc(m, ']');
+}
+
 static void *m_start(struct seq_file *m, loff_t *ppos)
 {
 	struct proc_maps_private *priv = m->private;
@@ -319,8 +369,15 @@ show_map_vma(struct seq_file *m, struct
 			goto done;
 		}
 
-		if (is_stack(vma))
+		if (is_stack(vma)) {
 			name = "[stack]";
+      goto done;
+    }
+
+    if (vma_get_anon_name(vma)) {
+      seq_pad(m, ' ');
+      seq_print_vma_name(m, vma);
+    }
 	}
 
 done:
@@ -837,6 +894,11 @@ static int show_smap(struct seq_file *m,
 	smap_gather_stats(vma, &mss, 0);
 
 	show_map_vma(m, vma);
+  if (vma_get_anon_name(vma)) {
+    seq_puts(m, "Name:           ");
+    seq_print_vma_name(m, vma);
+    seq_putc(m, '\n');
+  }
 
 	SEQ_PUT_DEC("Size:           ", vma->vm_end - vma->vm_start);
 	SEQ_PUT_DEC(" kB\nKernelPageSize: ", vma_kernel_pagesize(vma));
Index: rpi4-kernel/include/linux/netfilter/xt_qtaguid.h
===================================================================
--- /dev/null
+++ rpi4-kernel/include/linux/netfilter/xt_qtaguid.h
@@ -0,0 +1,15 @@
+#ifndef _XT_QTAGUID_MATCH_H
+#define _XT_QTAGUID_MATCH_H
+
+/* For now we just replace the xt_owner.
+ * FIXME: make iptables aware of qtaguid. */
+#include <linux/net.h>
+#include <linux/netfilter/xt_owner.h>
+
+#define XT_QTAGUID_UID    XT_OWNER_UID
+#define XT_QTAGUID_GID    XT_OWNER_GID
+#define XT_QTAGUID_SOCKET XT_OWNER_SOCKET
+#define xt_qtaguid_match_info xt_owner_match_info
+
+int qtaguid_untag(struct socket *sock, bool kernel);
+#endif /* _XT_QTAGUID_MATCH_H */
Index: rpi4-kernel/net/netfilter/xt_qtaguid.c
===================================================================
--- /dev/null
+++ rpi4-kernel/net/netfilter/xt_qtaguid.c
@@ -0,0 +1,3328 @@
+/*
+ * Kernel iptables module to track stats for packets based on user tags.
+ *
+ * (C) 2011 Google, Inc
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/*
+ * There are run-time debug flags enabled via the debug_mask module param, or
+ * via the DEFAULT_DEBUG_MASK. See xt_qtaguid_internal.h.
+ */
+#define DEBUG
+
+#include <linux/file.h>
+#include <linux/inetdevice.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/miscdevice.h>
+#include <linux/netfilter/x_tables.h>
+#include <linux/netfilter/xt_qtaguid.h>
+#include <linux/ratelimit.h>
+#include <linux/seq_file.h>
+#include <linux/skbuff.h>
+#include <linux/workqueue.h>
+#include <net/addrconf.h>
+#include <net/net_namespace.h>
+#include <net/netns/generic.h>
+#include <net/sock.h>
+#include <net/tcp.h>
+#include <net/udp.h>
+#include <net/netfilter/nf_socket.h>
+
+#if defined(CONFIG_IP6_NF_IPTABLES) || defined(CONFIG_IP6_NF_IPTABLES_MODULE)
+#include <linux/netfilter_ipv6/ip6_tables.h>
+#endif
+
+#include <linux/netfilter/xt_socket.h>
+#include "xt_qtaguid_internal.h"
+#include "xt_qtaguid_print.h"
+#include "../../fs/proc/internal.h"
+
+/*
+ * We only use the xt_socket funcs within a similar context to avoid unexpected
+ * return values.
+ */
+#define XT_SOCKET_SUPPORTED_HOOKS \
+	((1 << NF_INET_PRE_ROUTING) | (1 << NF_INET_LOCAL_IN))
+
+
+static unsigned int proc_iface_perms = S_IRUGO;
+module_param_named(iface_perms, proc_iface_perms, uint, S_IRUGO | S_IWUSR);
+
+static unsigned int proc_stats_perms = S_IRUGO;
+module_param_named(stats_perms, proc_stats_perms, uint, S_IRUGO | S_IWUSR);
+
+/* Everybody can write. But proc_ctrl_write_limited is true by default which
+ * limits what can be controlled. See the can_*() functions.
+ */
+static unsigned int proc_ctrl_perms = S_IRUGO | S_IWUGO;
+module_param_named(ctrl_perms, proc_ctrl_perms, uint, S_IRUGO | S_IWUSR);
+
+/* Limited by default, so the gid of the ctrl and stats proc entries
+ * will limit what can be done. See the can_*() functions.
+ */
+static bool proc_stats_readall_limited = true;
+static bool proc_ctrl_write_limited = true;
+
+module_param_named(stats_readall_limited, proc_stats_readall_limited, bool,
+		   S_IRUGO | S_IWUSR);
+module_param_named(ctrl_write_limited, proc_ctrl_write_limited, bool,
+		   S_IRUGO | S_IWUSR);
+
+/*
+ * Limit the number of active tags (via socket tags) for a given UID.
+ * Multiple processes could share the UID.
+ */
+static int max_sock_tags = DEFAULT_MAX_SOCK_TAGS;
+module_param(max_sock_tags, int, S_IRUGO | S_IWUSR);
+
+/*
+ * After the kernel has initiallized this module, it is still possible
+ * to make it passive.
+ * Setting passive to Y:
+ *  - the iface stats handling will not act on notifications.
+ *  - iptables matches will never match.
+ *  - ctrl commands silently succeed.
+ *  - stats are always empty.
+ * This is mostly usefull when a bug is suspected.
+ */
+static bool module_passive;
+module_param_named(passive, module_passive, bool, S_IRUGO | S_IWUSR);
+
+/*
+ * Control how qtaguid data is tracked per proc/uid.
+ * Setting tag_tracking_passive to Y:
+ *  - don't create proc specific structs to track tags
+ *  - don't check that active tag stats exceed some limits.
+ *  - don't clean up socket tags on process exits.
+ * This is mostly usefull when a bug is suspected.
+ */
+static bool qtu_proc_handling_passive;
+module_param_named(tag_tracking_passive, qtu_proc_handling_passive, bool,
+		   S_IRUGO | S_IWUSR);
+
+#define QTU_DEV_NAME "xt_qtaguid"
+
+struct qtaguid_net {
+	struct proc_dir_entry *procdir;
+	struct proc_dir_entry *ctrl_file;
+	struct proc_dir_entry *stats_file;
+	struct proc_dir_entry *iface_stat_procdir;
+
+	/* iface_stat_all will go away once userspace gets use to the new
+	 * fields that have a format line.
+	 */
+	struct proc_dir_entry *iface_stat_all_procfile;
+	struct proc_dir_entry *iface_stat_fmt_procfile;
+
+	struct list_head iface_stat_list;
+	spinlock_t iface_stat_list_lock;
+
+	struct rb_root sock_tag_tree;
+	spinlock_t sock_tag_list_lock;
+
+	struct rb_root tag_counter_set_tree;
+	spinlock_t tag_counter_set_list_lock;
+
+	struct rb_root uid_tag_data_tree;
+	spinlock_t uid_tag_data_tree_lock;
+
+	struct rb_root proc_qtu_data_tree;
+	/* No proc_qtu_data_tree_lock; use uid_tag_data_tree_lock */
+
+	struct qtaguid_event_counts qtu_events;
+};
+
+static int qtaguid_net_id;
+static inline struct qtaguid_net *qtaguid_pernet(const struct net *net)
+{
+	return net_generic(net, qtaguid_net_id);
+}
+
+uint qtaguid_debug_mask = DEFAULT_DEBUG_MASK;
+module_param_named(debug_mask, qtaguid_debug_mask, uint, S_IRUGO | S_IWUSR);
+
+/*----------------------------------------------*/
+
+static bool can_manipulate_uids(const struct net *net)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+
+	/* root pwnd */
+	return in_egroup_p(qtaguid_net->ctrl_file->gid) ||
+	       unlikely(!from_kuid(net->user_ns, current_fsuid())) ||
+	       unlikely(!proc_ctrl_write_limited) ||
+	       unlikely(uid_eq(current_fsuid(), qtaguid_net->ctrl_file->uid));
+}
+
+static bool can_impersonate_uid(const struct net *net, kuid_t uid)
+{
+	return uid_eq(uid, current_fsuid()) || can_manipulate_uids(net);
+}
+
+static bool can_read_other_uid_stats(const struct net *net, kuid_t uid)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+
+	/* root pwnd */
+	return in_egroup_p(qtaguid_net->stats_file->gid) ||
+	       unlikely(!from_kuid(net->user_ns, current_fsuid())) ||
+	       uid_eq(uid, current_fsuid()) ||
+	       unlikely(!proc_stats_readall_limited) ||
+	       unlikely(uid_eq(current_fsuid(),
+			       qtaguid_net->ctrl_file->uid));
+}
+
+static inline void dc_add_byte_packets(struct data_counters *counters, int set,
+				  enum ifs_tx_rx direction,
+				  enum ifs_proto ifs_proto,
+				  int bytes,
+				  int packets)
+{
+	counters->bpc[set][direction][ifs_proto].bytes += bytes;
+	counters->bpc[set][direction][ifs_proto].packets += packets;
+}
+
+static struct tag_node *tag_node_tree_search(struct rb_root *root, tag_t tag)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct tag_node *data = rb_entry(node, struct tag_node, node);
+		int result;
+		RB_DEBUG("qtaguid: tag_node_tree_search(0x%llx): "
+			 " node=%p data=%p\n", tag, node, data);
+		result = tag_compare(tag, data->tag);
+		RB_DEBUG("qtaguid: tag_node_tree_search(0x%llx): "
+			 " data.tag=0x%llx (uid=%u) res=%d\n",
+			 tag, data->tag, get_uid_from_tag(data->tag), result);
+		if (result < 0)
+			node = node->rb_left;
+		else if (result > 0)
+			node = node->rb_right;
+		else
+			return data;
+	}
+	return NULL;
+}
+
+static void tag_node_tree_insert(struct tag_node *data, struct rb_root *root)
+{
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+	/* Figure out where to put new node */
+	while (*new) {
+		struct tag_node *this = rb_entry(*new, struct tag_node,
+						 node);
+		int result = tag_compare(data->tag, this->tag);
+		RB_DEBUG("qtaguid: %s(): tag=0x%llx"
+			 " (uid=%u)\n", __func__,
+			 this->tag,
+			 get_uid_from_tag(this->tag));
+		parent = *new;
+		if (result < 0)
+			new = &((*new)->rb_left);
+		else if (result > 0)
+			new = &((*new)->rb_right);
+		else
+			BUG();
+	}
+
+	/* Add new node and rebalance tree. */
+	rb_link_node(&data->node, parent, new);
+	rb_insert_color(&data->node, root);
+}
+
+static void tag_stat_tree_insert(struct tag_stat *data, struct rb_root *root)
+{
+	tag_node_tree_insert(&data->tn, root);
+}
+
+static struct tag_stat *tag_stat_tree_search(struct rb_root *root, tag_t tag)
+{
+	struct tag_node *node = tag_node_tree_search(root, tag);
+	if (!node)
+		return NULL;
+	return rb_entry(&node->node, struct tag_stat, tn.node);
+}
+
+static void tag_stat_tree_erase(struct rb_root *root)
+{
+	struct rb_node *node;
+
+	for (node = rb_first(root); node; ) {
+		struct tag_stat *entry =
+			rb_entry(node, struct tag_stat, tn.node);
+		node = rb_next(node);
+		rb_erase(&entry->tn.node, root);
+		kfree(entry);
+	}
+}
+
+static void tag_counter_set_tree_insert(struct tag_counter_set *data,
+					struct rb_root *root)
+{
+	tag_node_tree_insert(&data->tn, root);
+}
+
+static struct tag_counter_set *tag_counter_set_tree_search(struct rb_root *root,
+							   tag_t tag)
+{
+	struct tag_node *node = tag_node_tree_search(root, tag);
+	if (!node)
+		return NULL;
+	return rb_entry(&node->node, struct tag_counter_set, tn.node);
+
+}
+
+static void tag_counter_set_tree_erase(struct rb_root *root)
+{
+	struct rb_node *node;
+
+	for (node = rb_first(root); node; ) {
+		struct tag_counter_set *entry =
+			rb_entry(node, struct tag_counter_set, tn.node);
+		node = rb_next(node);
+		rb_erase(&entry->tn.node, root);
+		kfree(entry);
+	}
+}
+
+static void tag_ref_tree_insert(struct tag_ref *data, struct rb_root *root)
+{
+	tag_node_tree_insert(&data->tn, root);
+}
+
+static struct tag_ref *tag_ref_tree_search(struct rb_root *root, tag_t tag)
+{
+	struct tag_node *node = tag_node_tree_search(root, tag);
+	if (!node)
+		return NULL;
+	return rb_entry(&node->node, struct tag_ref, tn.node);
+}
+
+static void tag_ref_set_tree_erase(struct rb_root *root)
+{
+	struct rb_node *node;
+
+	for (node = rb_first(root); node; ) {
+		struct tag_ref *entry =
+			rb_entry(node, struct tag_ref, tn.node);
+		node = rb_next(node);
+		rb_erase(&entry->tn.node, root);
+		kfree(entry);
+	}
+}
+
+static struct sock_tag *sock_tag_tree_search(struct rb_root *root,
+					     const struct sock *sk)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct sock_tag *data = rb_entry(node, struct sock_tag,
+						 sock_node);
+		if (sk < data->sk)
+			node = node->rb_left;
+		else if (sk > data->sk)
+			node = node->rb_right;
+		else
+			return data;
+	}
+	return NULL;
+}
+
+static void sock_tag_tree_insert(struct sock_tag *data, struct rb_root *root)
+{
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+	/* Figure out where to put new node */
+	while (*new) {
+		struct sock_tag *this = rb_entry(*new, struct sock_tag,
+						 sock_node);
+		parent = *new;
+		if (data->sk < this->sk)
+			new = &((*new)->rb_left);
+		else if (data->sk > this->sk)
+			new = &((*new)->rb_right);
+		else
+			BUG();
+	}
+
+	/* Add new node and rebalance tree. */
+	rb_link_node(&data->sock_node, parent, new);
+	rb_insert_color(&data->sock_node, root);
+}
+
+static void sock_tag_tree_erase(struct rb_root *st_to_free_tree)
+{
+	struct rb_node *node;
+	struct sock_tag *st_entry;
+
+	node = rb_first(st_to_free_tree);
+	while (node) {
+		st_entry = rb_entry(node, struct sock_tag, sock_node);
+		node = rb_next(node);
+		CT_DEBUG("qtaguid: %s(): "
+			 "erase st: sk=%p tag=0x%llx (uid=%u)\n", __func__,
+			 st_entry->sk,
+			 st_entry->tag,
+			 get_uid_from_tag(st_entry->tag));
+		rb_erase(&st_entry->sock_node, st_to_free_tree);
+		sock_put(st_entry->sk);
+		kfree(st_entry);
+	}
+}
+
+static struct proc_qtu_data *proc_qtu_data_tree_search(struct rb_root *root,
+						       const pid_t pid)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct proc_qtu_data *data = rb_entry(node,
+						      struct proc_qtu_data,
+						      node);
+		if (pid < data->pid)
+			node = node->rb_left;
+		else if (pid > data->pid)
+			node = node->rb_right;
+		else
+			return data;
+	}
+	return NULL;
+}
+
+static void proc_qtu_data_tree_insert(struct proc_qtu_data *data,
+				      struct rb_root *root)
+{
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+	/* Figure out where to put new node */
+	while (*new) {
+		struct proc_qtu_data *this = rb_entry(*new,
+						      struct proc_qtu_data,
+						      node);
+		parent = *new;
+		if (data->pid < this->pid)
+			new = &((*new)->rb_left);
+		else if (data->pid > this->pid)
+			new = &((*new)->rb_right);
+		else
+			BUG();
+	}
+
+	/* Add new node and rebalance tree. */
+	rb_link_node(&data->node, parent, new);
+	rb_insert_color(&data->node, root);
+}
+
+static void uid_tag_data_tree_insert(struct uid_tag_data *data,
+				     struct rb_root *root)
+{
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+	/* Figure out where to put new node */
+	while (*new) {
+		struct uid_tag_data *this = rb_entry(*new,
+						     struct uid_tag_data,
+						     node);
+		parent = *new;
+		if (data->uid < this->uid)
+			new = &((*new)->rb_left);
+		else if (data->uid > this->uid)
+			new = &((*new)->rb_right);
+		else
+			BUG();
+	}
+
+	/* Add new node and rebalance tree. */
+	rb_link_node(&data->node, parent, new);
+	rb_insert_color(&data->node, root);
+}
+
+static struct uid_tag_data *uid_tag_data_tree_search(struct rb_root *root,
+						     uid_t uid)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct uid_tag_data *data = rb_entry(node,
+						     struct uid_tag_data,
+						     node);
+		if (uid < data->uid)
+			node = node->rb_left;
+		else if (uid > data->uid)
+			node = node->rb_right;
+		else
+			return data;
+	}
+	return NULL;
+}
+
+static void uid_tag_data_tree_erase(struct rb_root *root)
+{
+	struct rb_node *node;
+
+	for (node = rb_first(root); node; ) {
+		struct uid_tag_data *entry =
+			rb_entry(node, struct uid_tag_data, node);
+		node = rb_next(node);
+		tag_ref_set_tree_erase(&entry->tag_ref_tree);
+		rb_erase(&entry->node, root);
+		kfree(entry);
+	}
+}
+
+/*
+ * Allocates a new uid_tag_data struct if needed.
+ * Returns a pointer to the found or allocated uid_tag_data.
+ * Returns a PTR_ERR on failures, and lock is not held.
+ * If found is not NULL:
+ *   sets *found to true if not allocated.
+ *   sets *found to false if allocated.
+ */
+static struct uid_tag_data *get_uid_data(struct qtaguid_net *qtaguid_net,
+					 uid_t uid, bool *found_res)
+{
+	struct uid_tag_data *utd_entry;
+
+	/* Look for top level uid_tag_data for the UID */
+	utd_entry = uid_tag_data_tree_search(&qtaguid_net->uid_tag_data_tree,
+					     uid);
+	DR_DEBUG("qtaguid: get_uid_data(%u) utd=%p\n", uid, utd_entry);
+
+	if (found_res)
+		*found_res = utd_entry;
+	if (utd_entry)
+		return utd_entry;
+
+	utd_entry = kzalloc(sizeof(*utd_entry), GFP_ATOMIC);
+	if (!utd_entry) {
+		pr_err("qtaguid: get_uid_data(%u): "
+		       "tag data alloc failed\n", uid);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	utd_entry->uid = uid;
+	utd_entry->tag_ref_tree = RB_ROOT;
+	uid_tag_data_tree_insert(utd_entry, &qtaguid_net->uid_tag_data_tree);
+	DR_DEBUG("qtaguid: get_uid_data(%u) new utd=%p\n", uid, utd_entry);
+	return utd_entry;
+}
+
+/* Never returns NULL. Either PTR_ERR or a valid ptr. */
+static struct tag_ref *new_tag_ref(tag_t new_tag,
+				   struct uid_tag_data *utd_entry)
+{
+	struct tag_ref *tr_entry;
+	int res;
+
+	if (utd_entry->num_active_tags + 1 > max_sock_tags) {
+		pr_info("qtaguid: new_tag_ref(0x%llx): "
+			"tag ref alloc quota exceeded. max=%d\n",
+			new_tag, max_sock_tags);
+		res = -EMFILE;
+		goto err_res;
+
+	}
+
+	tr_entry = kzalloc(sizeof(*tr_entry), GFP_ATOMIC);
+	if (!tr_entry) {
+		pr_err("qtaguid: new_tag_ref(0x%llx): "
+		       "tag ref alloc failed\n",
+		       new_tag);
+		res = -ENOMEM;
+		goto err_res;
+	}
+	tr_entry->tn.tag = new_tag;
+	/* tr_entry->num_sock_tags  handled by caller */
+	utd_entry->num_active_tags++;
+	tag_ref_tree_insert(tr_entry, &utd_entry->tag_ref_tree);
+	DR_DEBUG("qtaguid: new_tag_ref(0x%llx): "
+		 " inserted new tag ref %p\n",
+		 new_tag, tr_entry);
+	return tr_entry;
+
+err_res:
+	return ERR_PTR(res);
+}
+
+static struct tag_ref *lookup_tag_ref(struct qtaguid_net *qtaguid_net,
+				      tag_t full_tag,
+				      struct uid_tag_data **utd_res)
+{
+	struct uid_tag_data *utd_entry;
+	struct tag_ref *tr_entry;
+	bool found_utd;
+	uid_t uid = get_uid_from_tag(full_tag);
+
+	DR_DEBUG("qtaguid: lookup_tag_ref(tag=0x%llx (uid=%u))\n",
+		 full_tag, uid);
+
+	utd_entry = get_uid_data(qtaguid_net, uid, &found_utd);
+	if (IS_ERR_OR_NULL(utd_entry)) {
+		if (utd_res)
+			*utd_res = utd_entry;
+		return NULL;
+	}
+
+	tr_entry = tag_ref_tree_search(&utd_entry->tag_ref_tree, full_tag);
+	if (utd_res)
+		*utd_res = utd_entry;
+	DR_DEBUG("qtaguid: lookup_tag_ref(0x%llx) utd_entry=%p tr_entry=%p\n",
+		 full_tag, utd_entry, tr_entry);
+	return tr_entry;
+}
+
+/* Never returns NULL. Either PTR_ERR or a valid ptr. */
+static struct tag_ref *get_tag_ref(struct qtaguid_net *qtaguid_net,
+				   tag_t full_tag,
+				   struct uid_tag_data **utd_res)
+{
+	struct uid_tag_data *utd_entry;
+	struct tag_ref *tr_entry;
+
+	DR_DEBUG("qtaguid: get_tag_ref(0x%llx)\n",
+		 full_tag);
+	tr_entry = lookup_tag_ref(qtaguid_net, full_tag, &utd_entry);
+	BUG_ON(IS_ERR_OR_NULL(utd_entry));
+	if (!tr_entry)
+		tr_entry = new_tag_ref(full_tag, utd_entry);
+
+	if (utd_res)
+		*utd_res = utd_entry;
+	DR_DEBUG("qtaguid: get_tag_ref(0x%llx) utd=%p tr=%p\n",
+		 full_tag, utd_entry, tr_entry);
+	return tr_entry;
+}
+
+/* Checks and maybe frees the UID Tag Data entry */
+static void put_utd_entry(const struct net *net,
+			  struct uid_tag_data *utd_entry)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+
+	/* Are we done with the UID tag data entry? */
+	if (RB_EMPTY_ROOT(&utd_entry->tag_ref_tree) &&
+		!utd_entry->num_pqd) {
+		DR_DEBUG("qtaguid: %s(): "
+			 "erase utd_entry=%p uid=%u "
+			 "by pid=%u tgid=%u uid=%u\n", __func__,
+			 utd_entry, utd_entry->uid,
+			 current->pid, current->tgid,
+			 from_kuid(net->user_ns, current_fsuid()));
+		BUG_ON(utd_entry->num_active_tags);
+		rb_erase(&utd_entry->node, &qtaguid_net->uid_tag_data_tree);
+		kfree(utd_entry);
+	} else {
+		DR_DEBUG("qtaguid: %s(): "
+			 "utd_entry=%p still has %d tags %d proc_qtu_data\n",
+			 __func__, utd_entry, utd_entry->num_active_tags,
+			 utd_entry->num_pqd);
+		BUG_ON(!(utd_entry->num_active_tags ||
+			 utd_entry->num_pqd));
+	}
+}
+
+/*
+ * If no sock_tags are using this tag_ref,
+ * decrements refcount of utd_entry, removes tr_entry
+ * from utd_entry->tag_ref_tree and frees.
+ */
+static void free_tag_ref_from_utd_entry(struct tag_ref *tr_entry,
+					struct uid_tag_data *utd_entry)
+{
+	DR_DEBUG("qtaguid: %s(): %p tag=0x%llx (uid=%u)\n", __func__,
+		 tr_entry, tr_entry->tn.tag,
+		 get_uid_from_tag(tr_entry->tn.tag));
+	if (!tr_entry->num_sock_tags) {
+		BUG_ON(!utd_entry->num_active_tags);
+		utd_entry->num_active_tags--;
+		rb_erase(&tr_entry->tn.node, &utd_entry->tag_ref_tree);
+		DR_DEBUG("qtaguid: %s(): erased %p\n", __func__, tr_entry);
+		kfree(tr_entry);
+	}
+}
+
+static void put_tag_ref_tree(tag_t full_tag, struct uid_tag_data *utd_entry)
+{
+	struct rb_node *node;
+	struct tag_ref *tr_entry;
+	tag_t acct_tag;
+
+	DR_DEBUG("qtaguid: %s(tag=0x%llx (uid=%u))\n", __func__,
+		 full_tag, get_uid_from_tag(full_tag));
+	acct_tag = get_atag_from_tag(full_tag);
+	node = rb_first(&utd_entry->tag_ref_tree);
+	while (node) {
+		tr_entry = rb_entry(node, struct tag_ref, tn.node);
+		node = rb_next(node);
+		if (!acct_tag || tr_entry->tn.tag == full_tag)
+			free_tag_ref_from_utd_entry(tr_entry, utd_entry);
+	}
+}
+
+static ssize_t read_proc_u64(struct file *file, char __user *buf,
+			 size_t size, loff_t *ppos)
+{
+	uint64_t *valuep = PDE_DATA(file_inode(file));
+	char tmp[24];
+	size_t tmp_size;
+
+	tmp_size = scnprintf(tmp, sizeof(tmp), "%llu\n", *valuep);
+	return simple_read_from_buffer(buf, size, ppos, tmp, tmp_size);
+}
+
+static ssize_t read_proc_bool(struct file *file, char __user *buf,
+			  size_t size, loff_t *ppos)
+{
+	bool *valuep = PDE_DATA(file_inode(file));
+	char tmp[24];
+	size_t tmp_size;
+
+	tmp_size = scnprintf(tmp, sizeof(tmp), "%u\n", *valuep);
+	return simple_read_from_buffer(buf, size, ppos, tmp, tmp_size);
+}
+
+static int get_active_counter_set(struct qtaguid_net *qtaguid_net, tag_t tag)
+{
+	int active_set = 0;
+	struct tag_counter_set *tcs;
+
+	MT_DEBUG("qtaguid: get_active_counter_set(tag=0x%llx)"
+		 " (uid=%u)\n",
+		 tag, get_uid_from_tag(tag));
+	/* For now we only handle UID tags for active sets */
+	tag = get_utag_from_tag(tag);
+	spin_lock_bh(&qtaguid_net->tag_counter_set_list_lock);
+	tcs = tag_counter_set_tree_search(&qtaguid_net->tag_counter_set_tree,
+					  tag);
+	if (tcs)
+		active_set = tcs->active_set;
+	spin_unlock_bh(&qtaguid_net->tag_counter_set_list_lock);
+	return active_set;
+}
+
+/*
+ * Find the entry for tracking the specified interface.
+ * Caller must hold iface_stat_list_lock
+ */
+static struct iface_stat *get_iface_entry(struct qtaguid_net *qtaguid_net,
+					  const char *ifname)
+{
+	struct iface_stat *iface_entry;
+
+	/* Find the entry for tracking the specified tag within the interface */
+	if (ifname == NULL) {
+		pr_info("qtaguid: iface_stat: get() NULL device name\n");
+		return NULL;
+	}
+
+	/* Iterate over interfaces */
+	list_for_each_entry(iface_entry, &qtaguid_net->iface_stat_list, list) {
+		if (!strcmp(ifname, iface_entry->ifname))
+			goto done;
+	}
+	iface_entry = NULL;
+done:
+	return iface_entry;
+}
+
+/* This is for fmt2 only */
+static void pp_iface_stat_header(struct seq_file *m)
+{
+	seq_puts(m,
+		 "ifname "
+		 "total_skb_rx_bytes total_skb_rx_packets "
+		 "total_skb_tx_bytes total_skb_tx_packets "
+		 "rx_tcp_bytes rx_tcp_packets "
+		 "rx_udp_bytes rx_udp_packets "
+		 "rx_other_bytes rx_other_packets "
+		 "tx_tcp_bytes tx_tcp_packets "
+		 "tx_udp_bytes tx_udp_packets "
+		 "tx_other_bytes tx_other_packets\n"
+	);
+}
+
+static void pp_iface_stat_line(struct seq_file *m,
+			       struct iface_stat *iface_entry)
+{
+	struct data_counters *cnts;
+	int cnt_set = 0;   /* We only use one set for the device */
+	cnts = &iface_entry->totals_via_skb;
+	seq_printf(m, "%s %llu %llu %llu %llu %llu %llu %llu %llu "
+		   "%llu %llu %llu %llu %llu %llu %llu %llu\n",
+		   iface_entry->ifname,
+		   dc_sum_bytes(cnts, cnt_set, IFS_RX),
+		   dc_sum_packets(cnts, cnt_set, IFS_RX),
+		   dc_sum_bytes(cnts, cnt_set, IFS_TX),
+		   dc_sum_packets(cnts, cnt_set, IFS_TX),
+		   cnts->bpc[cnt_set][IFS_RX][IFS_TCP].bytes,
+		   cnts->bpc[cnt_set][IFS_RX][IFS_TCP].packets,
+		   cnts->bpc[cnt_set][IFS_RX][IFS_UDP].bytes,
+		   cnts->bpc[cnt_set][IFS_RX][IFS_UDP].packets,
+		   cnts->bpc[cnt_set][IFS_RX][IFS_PROTO_OTHER].bytes,
+		   cnts->bpc[cnt_set][IFS_RX][IFS_PROTO_OTHER].packets,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_TCP].bytes,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_TCP].packets,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_UDP].bytes,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_UDP].packets,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_PROTO_OTHER].bytes,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_PROTO_OTHER].packets);
+}
+
+struct proc_iface_stat_fmt_info {
+	struct net *net;
+	int fmt;
+};
+
+static void *iface_stat_fmt_proc_start(struct seq_file *m, loff_t *pos)
+{
+	struct proc_iface_stat_fmt_info *p = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(p->net);
+	loff_t n = *pos;
+
+	/*
+	 * This lock will prevent iface_stat_update() from changing active,
+	 * and in turn prevent an interface from unregistering itself.
+	 */
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+
+	if (unlikely(module_passive))
+		return NULL;
+
+	if (!n && p->fmt == 2)
+		pp_iface_stat_header(m);
+
+	return seq_list_start(&qtaguid_net->iface_stat_list, n);
+}
+
+static void *iface_stat_fmt_proc_next(struct seq_file *m, void *p, loff_t *pos)
+{
+	struct proc_iface_stat_fmt_info *fmt_info = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(fmt_info->net);
+
+	return seq_list_next(p, &qtaguid_net->iface_stat_list, pos);
+}
+
+static void iface_stat_fmt_proc_stop(struct seq_file *m, void *p)
+{
+	struct proc_iface_stat_fmt_info *fmt_info = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(fmt_info->net);
+
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+}
+
+static int iface_stat_fmt_proc_show(struct seq_file *m, void *v)
+{
+	struct proc_iface_stat_fmt_info *p = m->private;
+	struct iface_stat *iface_entry;
+	struct rtnl_link_stats64 dev_stats, *stats;
+	struct rtnl_link_stats64 no_dev_stats = {0};
+
+
+	CT_DEBUG("qtaguid:proc iface_stat_fmt pid=%u tgid=%u uid=%u\n",
+		 current->pid, current->tgid,
+		 from_kuid(p->net->user_ns, current_fsuid()));
+
+	iface_entry = list_entry(v, struct iface_stat, list);
+
+	if (iface_entry->active) {
+		stats = dev_get_stats(iface_entry->net_dev,
+				      &dev_stats);
+	} else {
+		stats = &no_dev_stats;
+	}
+	/*
+	 * If the meaning of the data changes, then update the fmtX
+	 * string.
+	 */
+	if (p->fmt == 1) {
+		seq_printf(m, "%s %d %llu %llu %llu %llu %llu %llu %llu %llu\n",
+			   iface_entry->ifname,
+			   iface_entry->active,
+			   iface_entry->totals_via_dev[IFS_RX].bytes,
+			   iface_entry->totals_via_dev[IFS_RX].packets,
+			   iface_entry->totals_via_dev[IFS_TX].bytes,
+			   iface_entry->totals_via_dev[IFS_TX].packets,
+			   stats->rx_bytes, stats->rx_packets,
+			   stats->tx_bytes, stats->tx_packets
+			   );
+	} else {
+		pp_iface_stat_line(m, iface_entry);
+	}
+	return 0;
+}
+
+static const struct proc_ops read_u64_fops = {
+	.proc_read	= read_proc_u64,
+	.proc_lseek	= default_llseek,
+};
+
+static const struct proc_ops read_bool_fops = {
+	.proc_read	= read_proc_bool,
+	.proc_lseek	= default_llseek,
+};
+
+static void iface_create_proc_worker(struct work_struct *work)
+{
+	struct proc_dir_entry *proc_entry;
+	struct iface_stat_work *isw = container_of(work, struct iface_stat_work,
+						   iface_work);
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(dev_net(isw->net_dev));
+	struct iface_stat *new_iface  = isw->iface_entry;
+
+	/* iface_entries are not deleted, so safe to manipulate. */
+	proc_entry = proc_mkdir(new_iface->ifname,
+				qtaguid_net->iface_stat_procdir);
+	if (IS_ERR_OR_NULL(proc_entry)) {
+		pr_err("qtaguid: iface_stat: create_proc(): alloc failed.\n");
+		goto abort;
+	}
+
+	new_iface->proc_ptr = proc_entry;
+
+	proc_create_data("tx_bytes", proc_iface_perms, proc_entry,
+			 &read_u64_fops,
+			 &new_iface->totals_via_dev[IFS_TX].bytes);
+	proc_create_data("rx_bytes", proc_iface_perms, proc_entry,
+			 &read_u64_fops,
+			 &new_iface->totals_via_dev[IFS_RX].bytes);
+	proc_create_data("tx_packets", proc_iface_perms, proc_entry,
+			 &read_u64_fops,
+			 &new_iface->totals_via_dev[IFS_TX].packets);
+	proc_create_data("rx_packets", proc_iface_perms, proc_entry,
+			 &read_u64_fops,
+			 &new_iface->totals_via_dev[IFS_RX].packets);
+	proc_create_data("active", proc_iface_perms, proc_entry,
+			 &read_bool_fops, &new_iface->active);
+
+	IF_DEBUG("qtaguid: iface_stat: create_proc(): done "
+		 "entry=%p dev=%s\n", new_iface, new_iface->ifname);
+abort:
+	dev_put(isw->net_dev);
+	kfree(isw);
+}
+
+static void iface_delete_proc(struct qtaguid_net *qtaguid_net,
+			      struct iface_stat *iface_entry)
+{
+	struct proc_dir_entry *proc_entry = iface_entry->proc_ptr;
+
+	if (!proc_entry)
+		return;
+
+	remove_proc_entry("active", proc_entry);
+	remove_proc_entry("rx_packets", proc_entry);
+	remove_proc_entry("tx_packets", proc_entry);
+	remove_proc_entry("rx_bytes", proc_entry);
+	remove_proc_entry("tx_bytes", proc_entry);
+	remove_proc_entry(iface_entry->ifname, qtaguid_net->iface_stat_procdir);
+}
+
+/*
+ * Will set the entry's active state, and
+ * update the net_dev accordingly also.
+ */
+static void _iface_stat_set_active(struct iface_stat *entry,
+				   struct net_device *net_dev,
+				   bool activate)
+{
+	if (activate) {
+		entry->net_dev = net_dev;
+		entry->active = true;
+		IF_DEBUG("qtaguid: %s(%s): "
+			 "enable tracking. rfcnt=%d\n", __func__,
+			 entry->ifname,
+			 __this_cpu_read(*net_dev->pcpu_refcnt));
+	} else {
+		entry->active = false;
+		entry->net_dev = NULL;
+		IF_DEBUG("qtaguid: %s(%s): "
+			 "disable tracking. rfcnt=%d\n", __func__,
+			 entry->ifname,
+			 __this_cpu_read(*net_dev->pcpu_refcnt));
+	}
+}
+
+/* Caller must hold iface_stat_list_lock */
+static struct iface_stat *iface_alloc(struct net_device *net_dev)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(dev_net(net_dev));
+	struct iface_stat *new_iface;
+	struct iface_stat_work *isw;
+
+	new_iface = kzalloc(sizeof(*new_iface), GFP_ATOMIC);
+	if (new_iface == NULL) {
+		pr_err("qtaguid: iface_stat: create(%s): "
+		       "iface_stat alloc failed\n", net_dev->name);
+		return NULL;
+	}
+	new_iface->ifname = kstrdup(net_dev->name, GFP_ATOMIC);
+	if (new_iface->ifname == NULL) {
+		pr_err("qtaguid: iface_stat: create(%s): "
+		       "ifname alloc failed\n", net_dev->name);
+		kfree(new_iface);
+		return NULL;
+	}
+	spin_lock_init(&new_iface->tag_stat_list_lock);
+	new_iface->tag_stat_tree = RB_ROOT;
+	_iface_stat_set_active(new_iface, net_dev, true);
+
+	/*
+	 * ipv6 notifier chains are atomic :(
+	 * No create_proc_read_entry() for you!
+	 */
+	isw = kmalloc(sizeof(*isw), GFP_ATOMIC);
+	if (!isw) {
+		pr_err("qtaguid: iface_stat: create(%s): "
+		       "work alloc failed\n", new_iface->ifname);
+		_iface_stat_set_active(new_iface, net_dev, false);
+		kfree(new_iface->ifname);
+		kfree(new_iface);
+		return NULL;
+	}
+	isw->iface_entry = new_iface;
+	dev_hold(net_dev);
+	isw->net_dev = net_dev;
+	INIT_WORK(&isw->iface_work, iface_create_proc_worker);
+	schedule_work(&isw->iface_work);
+	list_add(&new_iface->list, &qtaguid_net->iface_stat_list);
+	return new_iface;
+}
+
+static void iface_check_stats_reset_and_adjust(struct net_device *net_dev,
+					       struct iface_stat *iface)
+{
+	struct rtnl_link_stats64 dev_stats, *stats;
+	bool stats_rewound;
+
+	stats = dev_get_stats(net_dev, &dev_stats);
+	/* No empty packets */
+	stats_rewound =
+		(stats->rx_bytes < iface->last_known[IFS_RX].bytes)
+		|| (stats->tx_bytes < iface->last_known[IFS_TX].bytes);
+
+	IF_DEBUG("qtaguid: %s(%s): iface=%p netdev=%p "
+		 "bytes rx/tx=%llu/%llu "
+		 "active=%d last_known=%d "
+		 "stats_rewound=%d\n", __func__,
+		 net_dev ? net_dev->name : "?",
+		 iface, net_dev,
+		 stats->rx_bytes, stats->tx_bytes,
+		 iface->active, iface->last_known_valid, stats_rewound);
+
+	if (iface->active && iface->last_known_valid && stats_rewound) {
+		pr_warn_once("qtaguid: iface_stat: %s(%s): "
+			     "iface reset its stats unexpectedly\n", __func__,
+			     net_dev->name);
+
+		iface->totals_via_dev[IFS_TX].bytes +=
+			iface->last_known[IFS_TX].bytes;
+		iface->totals_via_dev[IFS_TX].packets +=
+			iface->last_known[IFS_TX].packets;
+		iface->totals_via_dev[IFS_RX].bytes +=
+			iface->last_known[IFS_RX].bytes;
+		iface->totals_via_dev[IFS_RX].packets +=
+			iface->last_known[IFS_RX].packets;
+		iface->last_known_valid = false;
+		IF_DEBUG("qtaguid: %s(%s): iface=%p "
+			 "used last known bytes rx/tx=%llu/%llu\n", __func__,
+			 iface->ifname, iface, iface->last_known[IFS_RX].bytes,
+			 iface->last_known[IFS_TX].bytes);
+	}
+}
+
+/*
+ * Create a new entry for tracking the specified interface.
+ * Do nothing if the entry already exists.
+ * Called when an interface is configured with a valid IP address.
+ */
+static void iface_stat_create(struct net_device *net_dev,
+			      struct in_ifaddr *ifa)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(dev_net(net_dev));
+	struct in_device *in_dev = NULL;
+	const char *ifname;
+	struct iface_stat *entry;
+	__be32 ipaddr = 0;
+	struct iface_stat *new_iface;
+
+	IF_DEBUG("qtaguid: iface_stat: create(%s): ifa=%p netdev=%p\n",
+		 net_dev ? net_dev->name : "?",
+		 ifa, net_dev);
+	if (!net_dev) {
+		pr_err("qtaguid: iface_stat: create(): no net dev\n");
+		return;
+	}
+
+	ifname = net_dev->name;
+	if (!ifa) {
+		in_dev = in_dev_get(net_dev);
+		if (!in_dev) {
+			pr_err("qtaguid: iface_stat: create(%s): no inet dev\n",
+			       ifname);
+			return;
+		}
+		IF_DEBUG("qtaguid: iface_stat: create(%s): in_dev=%p\n",
+			 ifname, in_dev);
+		for (ifa = in_dev->ifa_list; ifa; ifa = ifa->ifa_next) {
+			IF_DEBUG("qtaguid: iface_stat: create(%s): "
+				 "ifa=%p ifa_label=%s\n",
+				 ifname, ifa, ifa->ifa_label);
+			if (!strcmp(ifname, ifa->ifa_label))
+				break;
+		}
+	}
+
+	if (!ifa) {
+		IF_DEBUG("qtaguid: iface_stat: create(%s): no matching IP\n",
+			 ifname);
+		goto done_put;
+	}
+	ipaddr = ifa->ifa_local;
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	entry = get_iface_entry(qtaguid_net, ifname);
+	if (entry != NULL) {
+		IF_DEBUG("qtaguid: iface_stat: create(%s): entry=%p\n",
+			 ifname, entry);
+		iface_check_stats_reset_and_adjust(net_dev, entry);
+		_iface_stat_set_active(entry, net_dev, true);
+		IF_DEBUG("qtaguid: %s(%s): "
+			 "tracking now %d on ip=%pI4\n", __func__,
+			 entry->ifname, true, &ipaddr);
+		goto done_unlock_put;
+	}
+
+	new_iface = iface_alloc(net_dev);
+	IF_DEBUG("qtaguid: iface_stat: create(%s): done "
+		 "entry=%p ip=%pI4\n", ifname, new_iface, &ipaddr);
+done_unlock_put:
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+done_put:
+	if (in_dev)
+		in_dev_put(in_dev);
+}
+
+static void iface_stat_create_ipv6(struct net_device *net_dev,
+				   struct inet6_ifaddr *ifa)
+{
+	struct qtaguid_net *qtaguid_net =
+		qtaguid_pernet(dev_net(net_dev));
+	struct in_device *in_dev;
+	const char *ifname;
+	struct iface_stat *entry;
+	struct iface_stat *new_iface;
+	int addr_type;
+
+	IF_DEBUG("qtaguid: iface_stat: create6(): ifa=%p netdev=%p->name=%s\n",
+		 ifa, net_dev, net_dev ? net_dev->name : "");
+	if (!net_dev) {
+		pr_err("qtaguid: iface_stat: create6(): no net dev!\n");
+		return;
+	}
+	ifname = net_dev->name;
+
+	in_dev = in_dev_get(net_dev);
+	if (!in_dev) {
+		pr_err("qtaguid: iface_stat: create6(%s): no inet dev\n",
+		       ifname);
+		return;
+	}
+
+	IF_DEBUG("qtaguid: iface_stat: create6(%s): in_dev=%p\n",
+		 ifname, in_dev);
+
+	if (!ifa) {
+		IF_DEBUG("qtaguid: iface_stat: create6(%s): no matching IP\n",
+			 ifname);
+		goto done_put;
+	}
+	addr_type = ipv6_addr_type(&ifa->addr);
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	entry = get_iface_entry(qtaguid_net, ifname);
+	if (entry != NULL) {
+		IF_DEBUG("qtaguid: %s(%s): entry=%p\n", __func__,
+			 ifname, entry);
+		iface_check_stats_reset_and_adjust(net_dev, entry);
+		_iface_stat_set_active(entry, net_dev, true);
+		IF_DEBUG("qtaguid: %s(%s): "
+			 "tracking now %d on ip=%pI6c\n", __func__,
+			 entry->ifname, true, &ifa->addr);
+		goto done_unlock_put;
+	}
+
+	new_iface = iface_alloc(net_dev);
+	IF_DEBUG("qtaguid: iface_stat: create6(%s): done "
+		 "entry=%p ip=%pI6c\n", ifname, new_iface, &ifa->addr);
+
+done_unlock_put:
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+done_put:
+	in_dev_put(in_dev);
+}
+
+static struct sock_tag *get_sock_stat_nl(struct qtaguid_net *qtaguid_net,
+					 const struct sock *sk)
+{
+	MT_DEBUG("qtaguid: get_sock_stat_nl(sk=%p)\n", sk);
+	return sock_tag_tree_search(&qtaguid_net->sock_tag_tree, sk);
+}
+
+static int ipx_proto(const struct sk_buff *skb,
+		     struct xt_action_param *par)
+{
+	int thoff = 0, tproto;
+
+	switch (par->state->pf) {
+	case NFPROTO_IPV6:
+		tproto = ipv6_find_hdr(skb, &thoff, -1, NULL, NULL);
+		if (tproto < 0)
+			MT_DEBUG("%s(): transport header not found in ipv6"
+				 " skb=%p\n", __func__, skb);
+		break;
+	case NFPROTO_IPV4:
+		tproto = ip_hdr(skb)->protocol;
+		break;
+	default:
+		tproto = IPPROTO_RAW;
+	}
+	return tproto;
+}
+
+static void
+data_counters_update(struct data_counters *dc, int set,
+		     enum ifs_tx_rx direction, int proto, int bytes)
+{
+	switch (proto) {
+	case IPPROTO_TCP:
+		dc_add_byte_packets(dc, set, direction, IFS_TCP, bytes, 1);
+		break;
+	case IPPROTO_UDP:
+		dc_add_byte_packets(dc, set, direction, IFS_UDP, bytes, 1);
+		break;
+	case IPPROTO_IP:
+	default:
+		dc_add_byte_packets(dc, set, direction, IFS_PROTO_OTHER, bytes,
+				    1);
+		break;
+	}
+}
+
+/*
+ * Update stats for the specified interface. Do nothing if the entry
+ * does not exist (when a device was never configured with an IP address).
+ * Called when an device is being unregistered.
+ */
+static void iface_stat_update(struct net_device *net_dev, bool stash_only)
+{
+	struct qtaguid_net *qtaguid_net =
+		qtaguid_pernet(dev_net(net_dev));
+	struct rtnl_link_stats64 dev_stats, *stats;
+	struct iface_stat *entry;
+
+	stats = dev_get_stats(net_dev, &dev_stats);
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	entry = get_iface_entry(qtaguid_net, net_dev->name);
+	if (entry == NULL) {
+		IF_DEBUG("qtaguid: iface_stat: update(%s): not tracked\n",
+			 net_dev->name);
+		spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+		return;
+	}
+
+	IF_DEBUG("qtaguid: %s(%s): entry=%p\n", __func__,
+		 net_dev->name, entry);
+	if (!entry->active) {
+		IF_DEBUG("qtaguid: %s(%s): already disabled\n", __func__,
+			 net_dev->name);
+		spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+		return;
+	}
+
+	if (stash_only) {
+		entry->last_known[IFS_TX].bytes = stats->tx_bytes;
+		entry->last_known[IFS_TX].packets = stats->tx_packets;
+		entry->last_known[IFS_RX].bytes = stats->rx_bytes;
+		entry->last_known[IFS_RX].packets = stats->rx_packets;
+		entry->last_known_valid = true;
+		IF_DEBUG("qtaguid: %s(%s): "
+			 "dev stats stashed rx/tx=%llu/%llu\n", __func__,
+			 net_dev->name, stats->rx_bytes, stats->tx_bytes);
+		spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+		return;
+	}
+	entry->totals_via_dev[IFS_TX].bytes += stats->tx_bytes;
+	entry->totals_via_dev[IFS_TX].packets += stats->tx_packets;
+	entry->totals_via_dev[IFS_RX].bytes += stats->rx_bytes;
+	entry->totals_via_dev[IFS_RX].packets += stats->rx_packets;
+	/* We don't need the last_known[] anymore */
+	entry->last_known_valid = false;
+	_iface_stat_set_active(entry, net_dev, false);
+	IF_DEBUG("qtaguid: %s(%s): "
+		 "disable tracking. rx/tx=%llu/%llu\n", __func__,
+		 net_dev->name, stats->rx_bytes, stats->tx_bytes);
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+}
+
+/* Guarantied to return a net_device that has a name */
+static void get_dev_and_dir(const struct sk_buff *skb,
+			    struct xt_action_param *par,
+			    enum ifs_tx_rx *direction,
+			    const struct net_device **el_dev)
+{
+	const struct nf_hook_state *parst = par->state;
+
+	BUG_ON(!direction || !el_dev);
+
+	if (parst->in) {
+		*el_dev = parst->in;
+		*direction = IFS_RX;
+	} else if (parst->out) {
+		*el_dev = parst->out;
+		*direction = IFS_TX;
+	} else {
+		pr_err("qtaguid[%d]: %s(): no par->state->in/out?!!\n",
+		       parst->hook, __func__);
+		BUG();
+	}
+	if (skb->dev && *el_dev != skb->dev) {
+		MT_DEBUG("qtaguid[%d]: skb->dev=%p %s vs par->%s=%p %s\n",
+			 parst->hook, skb->dev, skb->dev->name,
+			 *direction == IFS_RX ? "in" : "out",  *el_dev,
+			 (*el_dev)->name);
+	}
+}
+
+/*
+ * Update stats for the specified interface from the skb.
+ * Do nothing if the entry
+ * does not exist (when a device was never configured with an IP address).
+ * Called on each sk.
+ */
+static void iface_stat_update_from_skb(const struct net *net,
+				       const struct sk_buff *skb,
+				       struct xt_action_param *par)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	const struct nf_hook_state *parst = par->state;
+	struct iface_stat *entry;
+	const struct net_device *el_dev;
+	enum ifs_tx_rx direction;
+	int bytes = skb->len;
+	int proto;
+
+	get_dev_and_dir(skb, par, &direction, &el_dev);
+	proto = ipx_proto(skb, par);
+	MT_DEBUG("qtaguid[%d]: iface_stat: %s(%s): "
+		 "type=%d fam=%d proto=%d dir=%d\n",
+		 parst->hook, __func__, el_dev->name, el_dev->type,
+		 parst->pf, proto, direction);
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	entry = get_iface_entry(qtaguid_net, el_dev->name);
+	if (entry == NULL) {
+		IF_DEBUG("qtaguid[%d]: iface_stat: %s(%s): not tracked\n",
+			 parst->hook, __func__, el_dev->name);
+		spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+		return;
+	}
+
+	IF_DEBUG("qtaguid[%d]: %s(%s): entry=%p\n", parst->hook,  __func__,
+		 el_dev->name, entry);
+
+	data_counters_update(&entry->totals_via_skb, 0, direction, proto,
+			     bytes);
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+}
+
+static void tag_stat_update(struct qtaguid_net *qtaguid_net,
+			    struct tag_stat *tag_entry,
+			    enum ifs_tx_rx direction, int proto, int bytes)
+{
+	int active_set;
+	active_set = get_active_counter_set(qtaguid_net, tag_entry->tn.tag);
+	MT_DEBUG("qtaguid: tag_stat_update(tag=0x%llx (uid=%u) set=%d "
+		 "dir=%d proto=%d bytes=%d)\n",
+		 tag_entry->tn.tag, get_uid_from_tag(tag_entry->tn.tag),
+		 active_set, direction, proto, bytes);
+	data_counters_update(&tag_entry->counters, active_set, direction,
+			     proto, bytes);
+	if (tag_entry->parent_counters)
+		data_counters_update(tag_entry->parent_counters, active_set,
+				     direction, proto, bytes);
+}
+
+/*
+ * Create a new entry for tracking the specified {acct_tag,uid_tag} within
+ * the interface.
+ * iface_entry->tag_stat_list_lock should be held.
+ */
+static struct tag_stat *create_if_tag_stat(struct iface_stat *iface_entry,
+					   tag_t tag)
+{
+	struct tag_stat *new_tag_stat_entry = NULL;
+	IF_DEBUG("qtaguid: iface_stat: %s(): ife=%p tag=0x%llx"
+		 " (uid=%u)\n", __func__,
+		 iface_entry, tag, get_uid_from_tag(tag));
+	new_tag_stat_entry = kzalloc(sizeof(*new_tag_stat_entry), GFP_ATOMIC);
+	if (!new_tag_stat_entry) {
+		pr_err("qtaguid: iface_stat: tag stat alloc failed\n");
+		goto done;
+	}
+	new_tag_stat_entry->tn.tag = tag;
+	tag_stat_tree_insert(new_tag_stat_entry, &iface_entry->tag_stat_tree);
+done:
+	return new_tag_stat_entry;
+}
+
+static void if_tag_stat_update(const struct net_device *net_dev, uid_t uid,
+			       const struct sock *sk, enum ifs_tx_rx direction,
+			       int proto, int bytes)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(dev_net(net_dev));
+	struct tag_stat *tag_stat_entry;
+	tag_t tag, acct_tag;
+	tag_t uid_tag;
+	struct data_counters *uid_tag_counters;
+	struct sock_tag *sock_tag_entry;
+	struct iface_stat *iface_entry;
+	struct tag_stat *new_tag_stat = NULL;
+	MT_DEBUG("qtaguid: if_tag_stat_update(ifname=%s "
+		"uid=%u sk=%p dir=%d proto=%d bytes=%d)\n",
+		 net_dev->name, uid, sk, direction, proto, bytes);
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	iface_entry = get_iface_entry(qtaguid_net, net_dev->name);
+	if (!iface_entry) {
+		pr_err_ratelimited("qtaguid: tag_stat: stat_update() "
+				   "%s not found\n", net_dev->name);
+		spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+		return;
+	}
+	/* It is ok to process data when an iface_entry is inactive */
+
+	MT_DEBUG("qtaguid: tag_stat: stat_update() dev=%s entry=%p\n",
+		 net_dev->name, iface_entry);
+
+	/*
+	 * Look for a tagged sock.
+	 * It will have an acct_uid.
+	 */
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	sock_tag_entry = sk ? get_sock_stat_nl(qtaguid_net, sk) : NULL;
+	if (sock_tag_entry) {
+		tag = sock_tag_entry->tag;
+		acct_tag = get_atag_from_tag(tag);
+		uid_tag = get_utag_from_tag(tag);
+	}
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+	if (!sock_tag_entry) {
+		acct_tag = make_atag_from_value(0);
+		tag = combine_atag_with_uid(acct_tag, uid);
+		uid_tag = make_tag_from_uid(uid);
+	}
+	MT_DEBUG("qtaguid: tag_stat: stat_update(): "
+		 " looking for tag=0x%llx (uid=%u) in ife=%p\n",
+		 tag, get_uid_from_tag(tag), iface_entry);
+	/* Loop over tag list under this interface for {acct_tag,uid_tag} */
+	spin_lock_bh(&iface_entry->tag_stat_list_lock);
+
+	tag_stat_entry = tag_stat_tree_search(&iface_entry->tag_stat_tree,
+					      tag);
+	if (tag_stat_entry) {
+		/*
+		 * Updating the {acct_tag, uid_tag} entry handles both stats:
+		 * {0, uid_tag} will also get updated.
+		 */
+		tag_stat_update(qtaguid_net, tag_stat_entry, direction, proto,
+				bytes);
+		goto unlock;
+	}
+
+	/* Loop over tag list under this interface for {0,uid_tag} */
+	tag_stat_entry = tag_stat_tree_search(&iface_entry->tag_stat_tree,
+					      uid_tag);
+	if (!tag_stat_entry) {
+		/* Here: the base uid_tag did not exist */
+		/*
+		 * No parent counters. So
+		 *  - No {0, uid_tag} stats and no {acc_tag, uid_tag} stats.
+		 */
+		new_tag_stat = create_if_tag_stat(iface_entry, uid_tag);
+		if (!new_tag_stat)
+			goto unlock;
+		uid_tag_counters = &new_tag_stat->counters;
+	} else {
+		uid_tag_counters = &tag_stat_entry->counters;
+	}
+
+	if (acct_tag) {
+		/* Create the child {acct_tag, uid_tag} and hook up parent. */
+		new_tag_stat = create_if_tag_stat(iface_entry, tag);
+		if (!new_tag_stat)
+			goto unlock;
+		new_tag_stat->parent_counters = uid_tag_counters;
+	} else {
+		/*
+		 * For new_tag_stat to be still NULL here would require:
+		 *  {0, uid_tag} exists
+		 *  and {acct_tag, uid_tag} doesn't exist
+		 *  AND acct_tag == 0.
+		 * Impossible. This reassures us that new_tag_stat
+		 * below will always be assigned.
+		 */
+		BUG_ON(!new_tag_stat);
+	}
+	tag_stat_update(qtaguid_net, new_tag_stat, direction, proto, bytes);
+unlock:
+	spin_unlock_bh(&iface_entry->tag_stat_list_lock);
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+}
+
+static int iface_netdev_event_handler(struct notifier_block *nb,
+				      unsigned long event, void *ptr) {
+	struct net_device *dev = netdev_notifier_info_to_dev(ptr);
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(dev_net(dev));
+
+	if (unlikely(module_passive))
+		return NOTIFY_DONE;
+
+	IF_DEBUG("qtaguid: iface_stat: netdev_event(): "
+		 "ev=0x%lx/%s netdev=%p->name=%s\n",
+		 event, netdev_evt_str(event), dev, dev ? dev->name : "");
+
+	switch (event) {
+	case NETDEV_UP:
+		iface_stat_create(dev, NULL);
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	case NETDEV_DOWN:
+	case NETDEV_UNREGISTER:
+		iface_stat_update(dev, event == NETDEV_DOWN);
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	}
+	return NOTIFY_DONE;
+}
+
+static int iface_inet6addr_event_handler(struct notifier_block *nb,
+					 unsigned long event, void *ptr)
+{
+	struct inet6_ifaddr *ifa = ptr;
+	struct net_device *dev;
+	struct qtaguid_net *qtaguid_net;
+
+	if (unlikely(module_passive))
+		return NOTIFY_DONE;
+
+	IF_DEBUG("qtaguid: iface_stat: inet6addr_event(): "
+		 "ev=0x%lx/%s ifa=%p\n",
+		 event, netdev_evt_str(event), ifa);
+
+	switch (event) {
+	case NETDEV_UP:
+		BUG_ON(!ifa || !ifa->idev);
+		dev = (struct net_device *)ifa->idev->dev;
+		iface_stat_create_ipv6(dev, ifa);
+		qtaguid_net = qtaguid_pernet(dev_net(dev));
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	case NETDEV_DOWN:
+	case NETDEV_UNREGISTER:
+		BUG_ON(!ifa || !ifa->idev);
+		dev = (struct net_device *)ifa->idev->dev;
+		iface_stat_update(dev, event == NETDEV_DOWN);
+		qtaguid_net = qtaguid_pernet(dev_net(dev));
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	}
+	return NOTIFY_DONE;
+}
+
+static int iface_inetaddr_event_handler(struct notifier_block *nb,
+					unsigned long event, void *ptr)
+{
+	struct in_ifaddr *ifa = ptr;
+	struct net_device *dev;
+	struct qtaguid_net *qtaguid_net;
+
+	if (unlikely(module_passive))
+		return NOTIFY_DONE;
+
+	IF_DEBUG("qtaguid: iface_stat: inetaddr_event(): "
+		 "ev=0x%lx/%s ifa=%p\n",
+		 event, netdev_evt_str(event), ifa);
+
+	switch (event) {
+	case NETDEV_UP:
+		BUG_ON(!ifa || !ifa->ifa_dev);
+		dev = ifa->ifa_dev->dev;
+		iface_stat_create(dev, ifa);
+		qtaguid_net = qtaguid_pernet(dev_net(dev));
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	case NETDEV_DOWN:
+	case NETDEV_UNREGISTER:
+		BUG_ON(!ifa || !ifa->ifa_dev);
+		dev = ifa->ifa_dev->dev;
+		iface_stat_update(dev, event == NETDEV_DOWN);
+		qtaguid_net = qtaguid_pernet(dev_net(dev));
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block iface_netdev_notifier_blk = {
+	.notifier_call = iface_netdev_event_handler,
+};
+
+static struct notifier_block iface_inetaddr_notifier_blk = {
+	.notifier_call = iface_inetaddr_event_handler,
+};
+
+static struct notifier_block iface_inet6addr_notifier_blk = {
+	.notifier_call = iface_inet6addr_event_handler,
+};
+
+static const struct seq_operations iface_stat_fmt_proc_seq_ops = {
+	.start	= iface_stat_fmt_proc_start,
+	.next	= iface_stat_fmt_proc_next,
+	.stop	= iface_stat_fmt_proc_stop,
+	.show	= iface_stat_fmt_proc_show,
+};
+
+static int proc_iface_stat_all_open(struct inode *inode, struct file *file)
+{
+	struct proc_iface_stat_fmt_info *s;
+
+	s = __seq_open_private(file, &iface_stat_fmt_proc_seq_ops, sizeof(*s));
+	if (!s)
+		return -ENOMEM;
+
+	s->fmt = 1;
+	s->net = PDE_DATA(inode);
+	return 0;
+}
+
+static int proc_iface_stat_fmt_open(struct inode *inode, struct file *file)
+{
+	struct proc_iface_stat_fmt_info *s;
+
+	s = __seq_open_private(file, &iface_stat_fmt_proc_seq_ops, sizeof(*s));
+	if (!s)
+		return -ENOMEM;
+
+	s->fmt = 2;
+	s->net = PDE_DATA(inode);
+	return 0;
+}
+
+static const struct proc_ops proc_iface_stat_all_fops = {
+	.proc_open	= proc_iface_stat_all_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= seq_release_private,
+};
+
+static const struct proc_ops proc_iface_stat_fmt_fops = {
+	.proc_open	= proc_iface_stat_fmt_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= seq_release_private,
+};
+
+static struct sock *qtaguid_find_sk(const struct sk_buff *skb,
+				    struct xt_action_param *par)
+{
+	const struct nf_hook_state *parst = par->state;
+	struct sock *sk;
+	unsigned int hook_mask = (1 << parst->hook);
+
+	MT_DEBUG("qtaguid[%d]: find_sk(skb=%p) family=%d\n",
+		 parst->hook, skb, parst->pf);
+
+	/*
+	 * Let's not abuse the the xt_socket_get*_sk(), or else it will
+	 * return garbage SKs.
+	 */
+	if (!(hook_mask & XT_SOCKET_SUPPORTED_HOOKS))
+		return NULL;
+
+	switch (parst->pf) {
+	case NFPROTO_IPV6:
+		sk = nf_sk_lookup_slow_v6(dev_net(skb->dev), skb, parst->in);
+		break;
+	case NFPROTO_IPV4:
+		sk = nf_sk_lookup_slow_v4(dev_net(skb->dev), skb, parst->in);
+		break;
+	default:
+		return NULL;
+	}
+
+	if (sk) {
+		MT_DEBUG("qtaguid[%d]: %p->sk_proto=%u->sk_state=%d\n",
+			 parst->hook, sk, sk->sk_protocol, sk->sk_state);
+	}
+	return sk;
+}
+
+static void account_for_uid(const struct sk_buff *skb,
+			    const struct sock *alternate_sk, uid_t uid,
+			    struct xt_action_param *par)
+{
+	const struct net_device *el_dev;
+	enum ifs_tx_rx direction;
+	int proto;
+
+	get_dev_and_dir(skb, par, &direction, &el_dev);
+	proto = ipx_proto(skb, par);
+	MT_DEBUG("qtaguid[%d]: dev name=%s type=%d fam=%d proto=%d dir=%d\n",
+		 par->state->hook, el_dev->name, el_dev->type,
+		 par->state->pf, proto, direction);
+
+	if_tag_stat_update(el_dev, uid,
+			   skb->sk ? skb->sk : alternate_sk,
+			   direction,
+			   proto, skb->len);
+}
+
+/* This function is based on xt_owner.c:owner_check(). */
+static int qtaguid_check(const struct xt_mtchk_param *par)
+{
+	struct xt_qtaguid_match_info *info = par->matchinfo;
+	struct net *net = par->net;
+
+	/* Only allow the common case where the userns of the writer
+	 * matches the userns of the network namespace.
+	 */
+	if ((info->match & (XT_QTAGUID_UID | XT_QTAGUID_GID)) &&
+	    (current_user_ns() != net->user_ns))
+		return -EINVAL;
+
+	/* Ensure the uids are valid */
+	if (info->match & XT_QTAGUID_UID) {
+		kuid_t uid_min = make_kuid(net->user_ns, info->uid_min);
+		kuid_t uid_max = make_kuid(net->user_ns, info->uid_max);
+
+		if (!uid_valid(uid_min) || !uid_valid(uid_max) ||
+		    (info->uid_max < info->uid_min) ||
+		    uid_lt(uid_max, uid_min)) {
+			return -EINVAL;
+		}
+	}
+
+	/* Ensure the gids are valid */
+	if (info->match & XT_QTAGUID_GID) {
+		kgid_t gid_min = make_kgid(net->user_ns, info->gid_min);
+		kgid_t gid_max = make_kgid(net->user_ns, info->gid_max);
+
+		if (!gid_valid(gid_min) || !gid_valid(gid_max) ||
+		    (info->gid_max < info->gid_min) ||
+		    gid_lt(gid_max, gid_min)) {
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static bool qtaguid_mt(const struct sk_buff *skb, struct xt_action_param *par)
+{
+	const struct xt_qtaguid_match_info *info = par->matchinfo;
+	const struct nf_hook_state *parst = par->state;
+	const struct file *filp;
+	const struct net *net = dev_net(xt_in(par) ? xt_in(par) : xt_out(par));
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	bool got_sock = false;
+	struct sock *sk;
+	kuid_t sock_uid;
+	bool res;
+	bool set_sk_callback_lock = false;
+	/*
+	 * TODO: unhack how to force just accounting.
+	 * For now we only do tag stats when the uid-owner is not requested
+	 */
+	bool do_tag_stat = !(info->match & XT_QTAGUID_UID);
+
+	if (unlikely(module_passive))
+		return (info->match ^ info->invert) == 0;
+
+	MT_DEBUG("qtaguid[%d]: entered skb=%p par->in=%p/out=%p fam=%d\n",
+		 parst->hook, skb, parst->in, parst->out, parst->pf);
+
+	atomic64_inc(&qtaguid_net->qtu_events.match_calls);
+	if (skb == NULL) {
+		res = (info->match ^ info->invert) == 0;
+		goto ret_res;
+	}
+
+	switch (parst->hook) {
+	case NF_INET_PRE_ROUTING:
+	case NF_INET_POST_ROUTING:
+		atomic64_inc(&qtaguid_net->qtu_events.match_calls_prepost);
+		iface_stat_update_from_skb(net, skb, par);
+		/*
+		 * We are done in pre/post. The skb will get processed
+		 * further alter.
+		 */
+		res = (info->match ^ info->invert);
+		goto ret_res;
+		break;
+	/* default: Fall through and do UID releated work */
+	}
+
+	sk = skb_to_full_sk(skb);
+	/*
+	 * When in TCP_TIME_WAIT the sk is not a "struct sock" but
+	 * "struct inet_timewait_sock" which is missing fields.
+	 * So we ignore it.
+	 */
+	if (sk && sk->sk_state == TCP_TIME_WAIT)
+		sk = NULL;
+	if (sk == NULL) {
+		/*
+		 * A missing sk->sk_socket happens when packets are in-flight
+		 * and the matching socket is already closed and gone.
+		 */
+		sk = qtaguid_find_sk(skb, par);
+		/*
+		 * TCP_NEW_SYN_RECV are not "struct sock" but "struct request_sock"
+		 * where we can get a pointer to a full socket to retrieve uid/gid.
+		 * When in TCP_TIME_WAIT, sk is a struct inet_timewait_sock
+		 * which is missing fields and does not contain any reference
+		 * to a full socket, so just ignore the socket.
+		 */
+		if (sk && sk->sk_state == TCP_NEW_SYN_RECV) {
+			sock_gen_put(sk);
+			sk = sk_to_full_sk(sk);
+		} else if (sk && (!sk_fullsock(sk) || sk->sk_state == TCP_TIME_WAIT)) {
+			sock_gen_put(sk);
+			sk = NULL;
+		} else {
+			/*
+			 * If we got the socket from the find_sk(), we will need to put
+			 * it back, as nf_tproxy_get_sock_v4() got it.
+			 */
+			got_sock = sk;
+		}
+		if (sk)
+			atomic64_inc(&qtaguid_net->qtu_events.match_found_sk_in_ct);
+		else
+			atomic64_inc(&qtaguid_net->qtu_events.match_found_no_sk_in_ct);
+	} else {
+		atomic64_inc(&qtaguid_net->qtu_events.match_found_sk);
+	}
+	MT_DEBUG("qtaguid[%d]: sk=%p got_sock=%d fam=%d proto=%d\n",
+		 parst->hook, sk, got_sock, parst->pf, ipx_proto(skb, par));
+
+	if (!sk) {
+		/*
+		 * Here, the qtaguid_find_sk() using connection tracking
+		 * couldn't find the owner, so for now we just count them
+		 * against the system.
+		 */
+		if (do_tag_stat)
+			account_for_uid(skb, sk, 0, par);
+		MT_DEBUG("qtaguid[%d]: leaving (sk=NULL)\n", parst->hook);
+		res = (info->match ^ info->invert) == 0;
+		atomic64_inc(&qtaguid_net->qtu_events.match_no_sk);
+		goto put_sock_ret_res;
+	} else if (info->match & info->invert & XT_QTAGUID_SOCKET) {
+		res = false;
+		goto put_sock_ret_res;
+	}
+	sock_uid = sk->sk_uid;
+	net = sock_net(sk);
+	if (do_tag_stat)
+		account_for_uid(skb, sk, from_kuid(net->user_ns, sock_uid),
+				par);
+
+	/*
+	 * The following two tests fail the match when:
+	 *    id not in range AND no inverted condition requested
+	 * or id     in range AND    inverted condition requested
+	 * Thus (!a && b) || (a && !b) == a ^ b
+	 */
+	if (info->match & XT_QTAGUID_UID) {
+		kuid_t uid_min = make_kuid(net->user_ns, info->uid_min);
+		kuid_t uid_max = make_kuid(net->user_ns, info->uid_max);
+
+		if ((uid_gte(sock_uid, uid_min) &&
+		     uid_lte(sock_uid, uid_max)) ^
+		    !(info->invert & XT_QTAGUID_UID)) {
+			MT_DEBUG("qtaguid[%d]: leaving uid not matching\n",
+				 parst->hook);
+			res = false;
+			goto put_sock_ret_res;
+		}
+	}
+	if (info->match & XT_QTAGUID_GID) {
+		kgid_t gid_min = make_kgid(net->user_ns, info->gid_min);
+		kgid_t gid_max = make_kgid(net->user_ns, info->gid_max);
+		set_sk_callback_lock = true;
+		read_lock_bh(&sk->sk_callback_lock);
+		MT_DEBUG("qtaguid[%d]: sk=%p->sk_socket=%p->file=%p\n",
+			 parst->hook, sk, sk->sk_socket,
+			 sk->sk_socket ? sk->sk_socket->file : (void *)-1LL);
+		filp = sk->sk_socket ? sk->sk_socket->file : NULL;
+		if (!filp) {
+			res = ((info->match ^ info->invert) &
+			       XT_QTAGUID_GID) == 0;
+			atomic64_inc(&qtaguid_net->qtu_events.match_no_sk_gid);
+			goto put_sock_ret_res;
+		}
+		MT_DEBUG("qtaguid[%d]: filp...uid=%u\n",
+			 parst->hook, filp ?
+			 from_kuid(net->user_ns, filp->f_cred->fsuid) : -1);
+
+		if ((gid_gte(filp->f_cred->fsgid, gid_min) &&
+				gid_lte(filp->f_cred->fsgid, gid_max)) ^
+			!(info->invert & XT_QTAGUID_GID)) {
+			MT_DEBUG("qtaguid[%d]: leaving gid not matching\n",
+				parst->hook);
+			res = false;
+			goto put_sock_ret_res;
+		}
+	}
+	MT_DEBUG("qtaguid[%d]: leaving matched\n", parst->hook);
+	res = true;
+
+put_sock_ret_res:
+	if (got_sock)
+		sock_gen_put(sk);
+	if (set_sk_callback_lock)
+		read_unlock_bh(&sk->sk_callback_lock);
+ret_res:
+	MT_DEBUG("qtaguid[%d]: left %d\n", parst->hook, res);
+	return res;
+}
+
+#ifdef DDEBUG
+/*
+ * This function is not in xt_qtaguid_print.c because of locks visibility.
+ * The lock of sock_tag_list must be aquired before calling this function
+ */
+static void prdebug_full_state_locked(struct qtaguid_net *qtaguid_net,
+				      int indent_level, const char *fmt, ...)
+{
+	va_list args;
+	char *fmt_buff;
+	char *buff;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	fmt_buff = kasprintf(GFP_ATOMIC,
+			     "qtaguid: %s(): %s {\n", __func__, fmt);
+	BUG_ON(!fmt_buff);
+	va_start(args, fmt);
+	buff = kvasprintf(GFP_ATOMIC,
+			  fmt_buff, args);
+	BUG_ON(!buff);
+	pr_debug("%s", buff);
+	kfree(fmt_buff);
+	kfree(buff);
+	va_end(args);
+
+	prdebug_sock_tag_tree(indent_level, &qtaguid_net->sock_tag_tree);
+
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	prdebug_uid_tag_data_tree(indent_level,
+				  &qtaguid_net->uid_tag_data_tree);
+	prdebug_proc_qtu_data_tree(indent_level,
+				   &qtaguid_net->proc_qtu_data_tree);
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	prdebug_iface_stat_list(indent_level, &qtaguid_net->iface_stat_list);
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+
+	pr_debug("qtaguid: %s(): }\n", __func__);
+}
+#else
+static void prdebug_full_state_locked(struct qtaguid_net *qtaguid_net,
+				      int indent_level, const char *fmt, ...) {}
+#endif
+
+struct proc_ctrl_print_info {
+	struct net *net;
+	struct sock *sk; /* socket found by reading to sk_pos */
+	loff_t sk_pos;
+};
+
+static void *qtaguid_ctrl_proc_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	struct proc_ctrl_print_info *pcpi = m->private;
+	struct sock_tag *sock_tag_entry = v;
+	struct rb_node *node;
+
+	(*pos)++;
+
+	if (!v || v  == SEQ_START_TOKEN)
+		return NULL;
+
+	node = rb_next(&sock_tag_entry->sock_node);
+	if (!node) {
+		pcpi->sk = NULL;
+		sock_tag_entry = SEQ_START_TOKEN;
+	} else {
+		sock_tag_entry = rb_entry(node, struct sock_tag, sock_node);
+		pcpi->sk = sock_tag_entry->sk;
+	}
+	pcpi->sk_pos = *pos;
+	return sock_tag_entry;
+}
+
+static void *qtaguid_ctrl_proc_start(struct seq_file *m, loff_t *pos)
+{
+	struct proc_ctrl_print_info *pcpi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(pcpi->net);
+	struct sock_tag *sock_tag_entry;
+	struct rb_node *node;
+
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+
+	if (unlikely(module_passive))
+		return NULL;
+
+	if (*pos == 0) {
+		pcpi->sk_pos = 0;
+		node = rb_first(&qtaguid_net->sock_tag_tree);
+		if (!node) {
+			pcpi->sk = NULL;
+			return SEQ_START_TOKEN;
+		}
+		sock_tag_entry = rb_entry(node, struct sock_tag, sock_node);
+		pcpi->sk = sock_tag_entry->sk;
+	} else {
+		sock_tag_entry = (pcpi->sk ?
+			get_sock_stat_nl(qtaguid_net, pcpi->sk) :
+			NULL) ?: SEQ_START_TOKEN;
+		if (*pos != pcpi->sk_pos) {
+			/* seq_read skipped a next call */
+			*pos = pcpi->sk_pos;
+			return qtaguid_ctrl_proc_next(m, sock_tag_entry, pos);
+		}
+	}
+	return sock_tag_entry;
+}
+
+static void qtaguid_ctrl_proc_stop(struct seq_file *m, void *v)
+{
+	struct proc_ctrl_print_info *pcpi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(pcpi->net);
+
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+}
+
+/*
+ * Procfs reader to get all active socket tags using style "1)" as described in
+ * fs/proc/generic.c
+ */
+static int qtaguid_ctrl_proc_show(struct seq_file *m, void *v)
+{
+	struct proc_ctrl_print_info *pcpi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(pcpi->net);
+	struct sock_tag *sock_tag_entry = v;
+	uid_t uid;
+
+	CT_DEBUG("qtaguid: proc ctrl pid=%u tgid=%u uid=%u\n",
+		 current->pid, current->tgid,
+		 from_kuid(pcpi->net->user_ns, current_fsuid()));
+
+	if (sock_tag_entry != SEQ_START_TOKEN) {
+		int sk_ref_count;
+		uid = get_uid_from_tag(sock_tag_entry->tag);
+		CT_DEBUG("qtaguid: proc_read(): sk=%p tag=0x%llx (uid=%u) "
+			 "pid=%u\n",
+			 sock_tag_entry->sk,
+			 sock_tag_entry->tag,
+			 uid,
+			 sock_tag_entry->pid
+			);
+		sk_ref_count = refcount_read(
+			&sock_tag_entry->sk->sk_refcnt);
+		seq_printf(m, "sock=%pK tag=0x%llx (uid=%u) pid=%u "
+			   "f_count=%d\n",
+			   sock_tag_entry->sk,
+			   sock_tag_entry->tag, uid,
+			   sock_tag_entry->pid, sk_ref_count);
+	} else {
+		seq_printf(m, "events: sockets_tagged=%llu "
+			   "sockets_untagged=%llu "
+			   "counter_set_changes=%llu "
+			   "delete_cmds=%llu "
+			   "iface_events=%llu "
+			   "match_calls=%llu "
+			   "match_calls_prepost=%llu "
+			   "match_found_sk=%llu "
+			   "match_found_sk_in_ct=%llu "
+			   "match_found_no_sk_in_ct=%llu "
+			   "match_no_sk=%llu "
+			   "match_no_sk_gid=%llu\n",
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.sockets_tagged),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.sockets_untagged),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.counter_set_changes),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.delete_cmds),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.iface_events),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_calls),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_calls_prepost),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_found_sk),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_found_sk_in_ct),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_found_no_sk_in_ct),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_no_sk),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_no_sk_gid));
+
+		/* Count the following as part of the last item_index. No need
+		 * to lock the sock_tag_list here since it is already locked when
+		 * starting the seq_file operation
+		 */
+		prdebug_full_state_locked(qtaguid_net, 0, "proc ctrl");
+	}
+
+	return 0;
+}
+
+/*
+ * Delete socket tags, and stat tags associated with a given
+ * accouting tag and uid.
+ */
+static int ctrl_cmd_delete(struct net *net, const char *input)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	char cmd;
+	int uid_int;
+	kuid_t uid;
+	uid_t entry_uid;
+	tag_t acct_tag;
+	tag_t tag;
+	int res, argc;
+	struct iface_stat *iface_entry;
+	struct rb_node *node;
+	struct sock_tag *st_entry;
+	struct rb_root st_to_free_tree = RB_ROOT;
+	struct tag_stat *ts_entry;
+	struct tag_counter_set *tcs_entry;
+	struct tag_ref *tr_entry;
+	struct uid_tag_data *utd_entry;
+
+	argc = sscanf(input, "%c %llu %u", &cmd, &acct_tag, &uid_int);
+	uid = make_kuid(net->user_ns, uid_int);
+	CT_DEBUG("qtaguid: ctrl_delete(%s): argc=%d cmd=%c "
+		 "user_tag=0x%llx uid=%u\n", input, argc, cmd,
+		 acct_tag, uid_int);
+	if (argc < 2) {
+		res = -EINVAL;
+		goto err;
+	}
+	if (!valid_atag(acct_tag)) {
+		pr_info("qtaguid: ctrl_delete(%s): invalid tag\n", input);
+		res = -EINVAL;
+		goto err;
+	}
+	if (argc < 3) {
+		uid = current_fsuid();
+		uid_int = from_kuid(net->user_ns, uid);
+	} else if (!can_impersonate_uid(net, uid)) {
+		pr_info("qtaguid: ctrl_delete(%s): "
+			"insufficient priv from pid=%u tgid=%u uid=%u\n",
+			input, current->pid, current->tgid,
+			from_kuid(net->user_ns, current_fsuid()));
+		res = -EPERM;
+		goto err;
+	}
+
+	tag = combine_atag_with_uid(acct_tag, uid_int);
+	CT_DEBUG("qtaguid: ctrl_delete(%s): "
+		 "looking for tag=0x%llx (uid=%u)\n",
+		 input, tag, uid_int);
+
+	/* Delete socket tags */
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	node = rb_first(&qtaguid_net->sock_tag_tree);
+	while (node) {
+		st_entry = rb_entry(node, struct sock_tag, sock_node);
+		entry_uid = get_uid_from_tag(st_entry->tag);
+		node = rb_next(node);
+		if (entry_uid != uid_int)
+			continue;
+
+		CT_DEBUG("qtaguid: ctrl_delete(%s): st tag=0x%llx (uid=%u)\n",
+			 input, st_entry->tag, entry_uid);
+
+		if (!acct_tag || st_entry->tag == tag) {
+			rb_erase(&st_entry->sock_node,
+				 &qtaguid_net->sock_tag_tree);
+			/* Can't sockfd_put() within spinlock, do it later. */
+			sock_tag_tree_insert(st_entry, &st_to_free_tree);
+			tr_entry = lookup_tag_ref(qtaguid_net, st_entry->tag,
+						  NULL);
+			BUG_ON(tr_entry->num_sock_tags <= 0);
+			tr_entry->num_sock_tags--;
+			/*
+			 * TODO: remove if, and start failing.
+			 * This is a hack to work around the fact that in some
+			 * places we have "if (IS_ERR_OR_NULL(pqd_entry))"
+			 * and are trying to work around apps
+			 * that didn't open the /dev/xt_qtaguid.
+			 */
+			if (st_entry->list.next && st_entry->list.prev)
+				list_del(&st_entry->list);
+		}
+	}
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+
+	sock_tag_tree_erase(&st_to_free_tree);
+
+	/* Delete tag counter-sets */
+	spin_lock_bh(&qtaguid_net->tag_counter_set_list_lock);
+	/* Counter sets are only on the uid tag, not full tag */
+	tcs_entry = tag_counter_set_tree_search(
+		&qtaguid_net->tag_counter_set_tree, tag);
+	if (tcs_entry) {
+		CT_DEBUG("qtaguid: ctrl_delete(%s): "
+			 "erase tcs: tag=0x%llx (uid=%u) set=%d\n",
+			 input,
+			 tcs_entry->tn.tag,
+			 get_uid_from_tag(tcs_entry->tn.tag),
+			 tcs_entry->active_set);
+		rb_erase(&tcs_entry->tn.node,
+			 &qtaguid_net->tag_counter_set_tree);
+		kfree(tcs_entry);
+	}
+	spin_unlock_bh(&qtaguid_net->tag_counter_set_list_lock);
+
+	/*
+	 * If acct_tag is 0, then all entries belonging to uid are
+	 * erased.
+	 */
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	list_for_each_entry(iface_entry, &qtaguid_net->iface_stat_list, list) {
+		spin_lock_bh(&iface_entry->tag_stat_list_lock);
+		node = rb_first(&iface_entry->tag_stat_tree);
+		while (node) {
+			ts_entry = rb_entry(node, struct tag_stat, tn.node);
+			entry_uid = get_uid_from_tag(ts_entry->tn.tag);
+			node = rb_next(node);
+
+			CT_DEBUG("qtaguid: ctrl_delete(%s): "
+				 "ts tag=0x%llx (uid=%u)\n",
+				 input, ts_entry->tn.tag, entry_uid);
+
+			if (entry_uid != uid_int)
+				continue;
+			if (!acct_tag || ts_entry->tn.tag == tag) {
+				CT_DEBUG("qtaguid: ctrl_delete(%s): "
+					 "erase ts: %s 0x%llx %u\n",
+					 input, iface_entry->ifname,
+					 get_atag_from_tag(ts_entry->tn.tag),
+					 entry_uid);
+				rb_erase(&ts_entry->tn.node,
+					 &iface_entry->tag_stat_tree);
+				kfree(ts_entry);
+			}
+		}
+		spin_unlock_bh(&iface_entry->tag_stat_list_lock);
+	}
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+
+	/* Cleanup the uid_tag_data */
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	node = rb_first(&qtaguid_net->uid_tag_data_tree);
+	while (node) {
+		utd_entry = rb_entry(node, struct uid_tag_data, node);
+		entry_uid = utd_entry->uid;
+		node = rb_next(node);
+
+		CT_DEBUG("qtaguid: ctrl_delete(%s): "
+			 "utd uid=%u\n",
+			 input, entry_uid);
+
+		if (entry_uid != uid_int)
+			continue;
+		/*
+		 * Go over the tag_refs, and those that don't have
+		 * sock_tags using them are freed.
+		 */
+		put_tag_ref_tree(tag, utd_entry);
+		put_utd_entry(net, utd_entry);
+	}
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+
+	atomic64_inc(&qtaguid_net->qtu_events.delete_cmds);
+	res = 0;
+
+err:
+	return res;
+}
+
+static int ctrl_cmd_counter_set(struct net *net, const char *input)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	char cmd;
+	uid_t uid = 0;
+	tag_t tag;
+	int res, argc;
+	struct tag_counter_set *tcs;
+	int counter_set;
+
+	argc = sscanf(input, "%c %d %u", &cmd, &counter_set, &uid);
+	CT_DEBUG("qtaguid: ctrl_counterset(%s): argc=%d cmd=%c "
+		 "set=%d uid=%u\n", input, argc, cmd,
+		 counter_set, uid);
+	if (argc != 3) {
+		res = -EINVAL;
+		goto err;
+	}
+	if (counter_set < 0 || counter_set >= IFS_MAX_COUNTER_SETS) {
+		pr_info("qtaguid: ctrl_counterset(%s): invalid counter_set range\n",
+			input);
+		res = -EINVAL;
+		goto err;
+	}
+	if (!can_manipulate_uids(net)) {
+		pr_info("qtaguid: ctrl_counterset(%s): "
+			"insufficient priv from pid=%u tgid=%u uid=%u\n",
+			input, current->pid, current->tgid,
+			from_kuid(net->user_ns, current_fsuid()));
+		res = -EPERM;
+		goto err;
+	}
+
+	tag = make_tag_from_uid(uid);
+	spin_lock_bh(&qtaguid_net->tag_counter_set_list_lock);
+	tcs = tag_counter_set_tree_search(&qtaguid_net->tag_counter_set_tree,
+					  tag);
+	if (!tcs) {
+		tcs = kzalloc(sizeof(*tcs), GFP_ATOMIC);
+		if (!tcs) {
+			spin_unlock_bh(&qtaguid_net->tag_counter_set_list_lock);
+			pr_err("qtaguid: ctrl_counterset(%s): "
+			       "failed to alloc counter set\n",
+			       input);
+			res = -ENOMEM;
+			goto err;
+		}
+		tcs->tn.tag = tag;
+		tag_counter_set_tree_insert(tcs, &qtaguid_net->
+					    tag_counter_set_tree);
+		CT_DEBUG("qtaguid: ctrl_counterset(%s): added tcs tag=0x%llx "
+			 "(uid=%u) set=%d\n",
+			 input, tag, get_uid_from_tag(tag), counter_set);
+	}
+	tcs->active_set = counter_set;
+	spin_unlock_bh(&qtaguid_net->tag_counter_set_list_lock);
+	atomic64_inc(&qtaguid_net->qtu_events.counter_set_changes);
+	res = 0;
+
+err:
+	return res;
+}
+
+static int ctrl_cmd_tag(struct net *net, const char *input)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	char cmd;
+	int sock_fd = 0;
+	kuid_t uid;
+	unsigned int uid_int = 0;
+	tag_t acct_tag = make_atag_from_value(0);
+	tag_t full_tag;
+	struct socket *el_socket;
+	int res, argc;
+	struct sock_tag *sock_tag_entry;
+	struct tag_ref *tag_ref_entry;
+	struct uid_tag_data *uid_tag_data_entry;
+	struct proc_qtu_data *pqd_entry;
+
+	/* Unassigned args will get defaulted later. */
+	argc = sscanf(input, "%c %d %llu %u", &cmd, &sock_fd, &acct_tag, &uid_int);
+	uid = make_kuid(net->user_ns, uid_int);
+	CT_DEBUG("qtaguid: ctrl_tag(%s): argc=%d cmd=%c sock_fd=%d "
+		 "acct_tag=0x%llx uid=%u\n", input, argc, cmd, sock_fd,
+		 acct_tag, uid_int);
+	if (argc < 2) {
+		res = -EINVAL;
+		goto err;
+	}
+	el_socket = sockfd_lookup(sock_fd, &res);  /* This locks the file */
+	if (!el_socket) {
+		pr_info("qtaguid: ctrl_tag(%s): failed to lookup"
+			" sock_fd=%d err=%d pid=%u tgid=%u uid=%u\n",
+			input, sock_fd, res, current->pid, current->tgid,
+			from_kuid(net->user_ns, current_fsuid()));
+		goto err;
+	}
+	CT_DEBUG("qtaguid: ctrl_tag(%s): socket->...->sk_refcnt=%d ->sk=%p\n",
+		 input, refcount_read(&el_socket->sk->sk_refcnt),
+		 el_socket->sk);
+	if (argc < 3) {
+		acct_tag = make_atag_from_value(0);
+	} else if (!valid_atag(acct_tag)) {
+		pr_info("qtaguid: ctrl_tag(%s): invalid tag\n", input);
+		res = -EINVAL;
+		goto err_put;
+	}
+	CT_DEBUG("qtaguid: ctrl_tag(%s): pid=%u tgid=%u uid=%u euid=%u fsuid=%u ctrl.gid=%u in_group()=%d in_egroup()=%d\n",
+		 input, current->pid, current->tgid,
+		 from_kuid(net->user_ns, current_uid()),
+		 from_kuid(net->user_ns, current_euid()),
+		 from_kuid(net->user_ns, current_fsuid()),
+		 from_kgid(net->user_ns, qtaguid_net->ctrl_file->gid),
+		 in_group_p(qtaguid_net->ctrl_file->gid),
+		 in_egroup_p(qtaguid_net->ctrl_file->gid));
+	if (argc < 4) {
+		uid = current_fsuid();
+		uid_int = from_kuid(net->user_ns, uid);
+	} else if (!can_impersonate_uid(net, uid)) {
+		pr_info("qtaguid: ctrl_tag(%s): insufficient priv from pid=%u tgid=%u uid=%u\n",
+			input, current->pid, current->tgid,
+			from_kuid(net->user_ns, current_fsuid()));
+		res = -EPERM;
+		goto err_put;
+	}
+	full_tag = combine_atag_with_uid(acct_tag, uid_int);
+
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	sock_tag_entry = get_sock_stat_nl(qtaguid_net, el_socket->sk);
+	tag_ref_entry = get_tag_ref(qtaguid_net, full_tag, &uid_tag_data_entry);
+	if (IS_ERR(tag_ref_entry)) {
+		res = PTR_ERR(tag_ref_entry);
+		spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+		spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+		goto err_put;
+	}
+	tag_ref_entry->num_sock_tags++;
+	if (sock_tag_entry) {
+		struct tag_ref *prev_tag_ref_entry;
+
+		CT_DEBUG("qtaguid: ctrl_tag(%s): retag for sk=%p "
+			 "st@%p ...->sk_refcnt=%d\n",
+			 input, el_socket->sk, sock_tag_entry,
+			 refcount_read(&el_socket->sk->sk_refcnt));
+		prev_tag_ref_entry = lookup_tag_ref(qtaguid_net,
+						    sock_tag_entry->tag,
+						    &uid_tag_data_entry);
+		BUG_ON(IS_ERR_OR_NULL(prev_tag_ref_entry));
+		BUG_ON(prev_tag_ref_entry->num_sock_tags <= 0);
+		prev_tag_ref_entry->num_sock_tags--;
+		sock_tag_entry->tag = full_tag;
+	} else {
+		CT_DEBUG("qtaguid: ctrl_tag(%s): newtag for sk=%p\n",
+			 input, el_socket->sk);
+		sock_tag_entry = kzalloc(sizeof(*sock_tag_entry),
+					 GFP_ATOMIC);
+		if (!sock_tag_entry) {
+			pr_err("qtaguid: ctrl_tag(%s): "
+			       "socket tag alloc failed\n",
+			       input);
+			BUG_ON(tag_ref_entry->num_sock_tags <= 0);
+			tag_ref_entry->num_sock_tags--;
+			free_tag_ref_from_utd_entry(tag_ref_entry,
+						    uid_tag_data_entry);
+			spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+			spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+			res = -ENOMEM;
+			goto err_put;
+		}
+		/*
+		 * Hold the sk refcount here to make sure the sk pointer cannot
+		 * be freed and reused
+		 */
+		sock_hold(el_socket->sk);
+		sock_tag_entry->sk = el_socket->sk;
+		sock_tag_entry->pid = current->tgid;
+		sock_tag_entry->tag = combine_atag_with_uid(acct_tag, uid_int);
+		pqd_entry = proc_qtu_data_tree_search(
+			&qtaguid_net->proc_qtu_data_tree, current->tgid);
+		/*
+		 * TODO: remove if, and start failing.
+		 * At first, we want to catch user-space code that is not
+		 * opening the /dev/xt_qtaguid.
+		 */
+		if (IS_ERR_OR_NULL(pqd_entry))
+			pr_warn_once(
+				"qtaguid: %s(): "
+				"User space forgot to open /dev/xt_qtaguid? "
+				"pid=%u tgid=%u uid=%u\n", __func__,
+				current->pid, current->tgid,
+				from_kuid(net->user_ns, current_fsuid()));
+		else
+			list_add(&sock_tag_entry->list,
+				 &pqd_entry->sock_tag_list);
+
+		sock_tag_tree_insert(sock_tag_entry,
+				     &qtaguid_net->sock_tag_tree);
+		atomic64_inc(&qtaguid_net->qtu_events.sockets_tagged);
+	}
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+	/* We keep the ref to the sk until it is untagged */
+	CT_DEBUG("qtaguid: ctrl_tag(%s): done st@%p ...->sk_refcnt=%d\n",
+		 input, sock_tag_entry,
+		 refcount_read(&el_socket->sk->sk_refcnt));
+	sockfd_put(el_socket);
+	return 0;
+
+err_put:
+	CT_DEBUG("qtaguid: ctrl_tag(%s): done. ...->sk_refcnt=%d\n",
+		 input, refcount_read(&el_socket->sk->sk_refcnt) - 1);
+	/* Release the sock_fd that was grabbed by sockfd_lookup(). */
+	sockfd_put(el_socket);
+	return res;
+
+err:
+	CT_DEBUG("qtaguid: ctrl_tag(%s): done.\n", input);
+	return res;
+}
+
+static int ctrl_cmd_untag(struct net *net, const char *input)
+{
+	char cmd;
+	int sock_fd = 0;
+	struct socket *el_socket;
+	int res, argc;
+
+	argc = sscanf(input, "%c %d", &cmd, &sock_fd);
+	CT_DEBUG("qtaguid: ctrl_untag(%s): argc=%d cmd=%c sock_fd=%d\n",
+		 input, argc, cmd, sock_fd);
+	if (argc < 2) {
+		res = -EINVAL;
+		return res;
+	}
+	el_socket = sockfd_lookup(sock_fd, &res);  /* This locks the file */
+	if (!el_socket) {
+		pr_info("qtaguid: ctrl_untag(%s): failed to lookup"
+			" sock_fd=%d err=%d pid=%u tgid=%u uid=%u\n",
+			input, sock_fd, res, current->pid, current->tgid,
+			from_kuid(net->user_ns, current_fsuid()));
+		return res;
+	}
+	CT_DEBUG("qtaguid: ctrl_untag(%s): socket->...->f_count=%ld ->sk=%p\n",
+		 input, atomic_long_read(&el_socket->file->f_count),
+		 el_socket->sk);
+	res = qtaguid_untag(el_socket, false);
+	sockfd_put(el_socket);
+	return res;
+}
+
+int qtaguid_untag(struct socket *el_socket, bool kernel)
+{
+	struct sock *sk = el_socket->sk;
+	struct net *net = sock_net(sk);
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	int res;
+	pid_t pid;
+	struct sock_tag *sock_tag_entry;
+	struct tag_ref *tag_ref_entry;
+	struct uid_tag_data *utd_entry;
+	struct proc_qtu_data *pqd_entry;
+
+	/* qtaguid_untag() may be called from inet_release(), which in turn
+	 * may be called from the error handler in setup_net() if creating
+	 * the network namespace failed. In this case, there is no guarantee
+	 * that qtaguid_net was ever initialized, and qtaguid_net may be NULL.
+	 */
+	if (!qtaguid_net)
+		return -EINVAL;
+
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	sock_tag_entry = get_sock_stat_nl(qtaguid_net, el_socket->sk);
+	if (!sock_tag_entry) {
+		spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+		res = -EINVAL;
+		return res;
+	}
+	/*
+	 * The socket already belongs to the current process
+	 * so it can do whatever it wants to it.
+	 */
+	rb_erase(&sock_tag_entry->sock_node, &qtaguid_net->sock_tag_tree);
+
+	tag_ref_entry = lookup_tag_ref(qtaguid_net, sock_tag_entry->tag,
+				       &utd_entry);
+	BUG_ON(!tag_ref_entry);
+	BUG_ON(tag_ref_entry->num_sock_tags <= 0);
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	if (kernel)
+		pid = sock_tag_entry->pid;
+	else
+		pid = current->tgid;
+	pqd_entry = proc_qtu_data_tree_search(
+		&qtaguid_net->proc_qtu_data_tree, pid);
+	/*
+	 * TODO: remove if, and start failing.
+	 * At first, we want to catch user-space code that is not
+	 * opening the /dev/xt_qtaguid.
+	 */
+	if (IS_ERR_OR_NULL(pqd_entry))
+		pr_warn_once("qtaguid: %s(): "
+			     "User space forgot to open /dev/xt_qtaguid? "
+			     "pid=%u tgid=%u sk_pid=%u, uid=%u\n", __func__,
+			     current->pid, current->tgid, sock_tag_entry->pid,
+			     from_kuid(net->user_ns, current_fsuid()));
+	/*
+	 * This check is needed because tagging from a process that
+	 * didnt open /dev/xt_qtaguid still adds the sock_tag_entry
+	 * to sock_tag_tree.
+	 */
+	if (sock_tag_entry->list.next)
+		list_del(&sock_tag_entry->list);
+
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	/*
+	 * We don't free tag_ref from the utd_entry here,
+	 * only during a cmd_delete().
+	 */
+	tag_ref_entry->num_sock_tags--;
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+	/*
+	 * Release the sock_fd that was grabbed at tag time.
+	 */
+	sock_put(sock_tag_entry->sk);
+	CT_DEBUG("qtaguid: done. st@%p ...->sk_refcnt=%d\n",
+		 sock_tag_entry,
+		 refcount_read(&el_socket->sk->sk_refcnt));
+
+	kfree(sock_tag_entry);
+	atomic64_inc(&qtaguid_net->qtu_events.sockets_untagged);
+
+	return 0;
+}
+
+static ssize_t qtaguid_ctrl_parse(struct net *net,
+				  const char *input,
+				  size_t count)
+{
+	char cmd;
+	ssize_t res;
+
+	CT_DEBUG("qtaguid: ctrl(%s): pid=%u tgid=%u uid=%u\n",
+		 input, current->pid, current->tgid,
+		 from_kuid(net->user_ns, current_fsuid()));
+
+	cmd = input[0];
+	/* Collect params for commands */
+	switch (cmd) {
+	case 'd':
+		res = ctrl_cmd_delete(net, input);
+		break;
+
+	case 's':
+		res = ctrl_cmd_counter_set(net, input);
+		break;
+
+	case 't':
+		res = ctrl_cmd_tag(net, input);
+		break;
+
+	case 'u':
+		res = ctrl_cmd_untag(net, input);
+		break;
+
+	default:
+		res = -EINVAL;
+		goto err;
+	}
+	if (!res)
+		res = count;
+err:
+	CT_DEBUG("qtaguid: ctrl(%s): res=%zd\n", input, res);
+	return res;
+}
+
+#define MAX_QTAGUID_CTRL_INPUT_LEN 255
+static ssize_t qtaguid_ctrl_proc_write(struct file *file, const char __user *buffer,
+				   size_t count, loff_t *offp)
+{
+	struct net *net = PDE_DATA(file_inode(file));
+	char input_buf[MAX_QTAGUID_CTRL_INPUT_LEN];
+
+	if (unlikely(module_passive))
+		return count;
+
+	if (count >= MAX_QTAGUID_CTRL_INPUT_LEN)
+		return -EINVAL;
+
+	if (copy_from_user(input_buf, buffer, count))
+		return -EFAULT;
+
+	input_buf[count] = '\0';
+	return qtaguid_ctrl_parse(net, input_buf, count);
+}
+
+struct proc_print_info {
+	struct net *net;
+	struct iface_stat *iface_entry;
+	int item_index;
+	tag_t tag; /* tag found by reading to tag_pos */
+	off_t tag_pos;
+	int tag_item_index;
+};
+
+static void pp_stats_header(struct seq_file *m)
+{
+	seq_puts(m,
+		 "idx iface acct_tag_hex uid_tag_int cnt_set "
+		 "rx_bytes rx_packets "
+		 "tx_bytes tx_packets "
+		 "rx_tcp_bytes rx_tcp_packets "
+		 "rx_udp_bytes rx_udp_packets "
+		 "rx_other_bytes rx_other_packets "
+		 "tx_tcp_bytes tx_tcp_packets "
+		 "tx_udp_bytes tx_udp_packets "
+		 "tx_other_bytes tx_other_packets\n");
+}
+
+static int pp_stats_line(struct seq_file *m, struct tag_stat *ts_entry,
+			 int cnt_set)
+{
+	struct data_counters *cnts;
+	tag_t tag = ts_entry->tn.tag;
+	uid_t stat_uid = get_uid_from_tag(tag);
+	struct proc_print_info *ppi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(ppi->net);
+
+	/* Detailed tags are not available to everybody */
+	if (!can_read_other_uid_stats(ppi->net,
+				      make_kuid(ppi->net->user_ns, stat_uid))) {
+		CT_DEBUG("qtaguid: stats line: "
+			 "%s 0x%llx %u: insufficient priv "
+			 "from pid=%u tgid=%u uid=%u stats.gid=%u\n",
+			 ppi->iface_entry->ifname,
+			 get_atag_from_tag(tag), stat_uid,
+			 current->pid, current->tgid,
+			 from_kuid(ppi->net->user_ns, current_fsuid()),
+			 from_kgid(ppi->net->user_ns,
+				   qtaguid_net->stats_file->gid));
+		return 0;
+	}
+	ppi->item_index++;
+	cnts = &ts_entry->counters;
+	seq_printf(m, "%d %s 0x%llx %u %u "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu\n",
+		ppi->item_index,
+		ppi->iface_entry->ifname,
+		get_atag_from_tag(tag),
+		stat_uid,
+		cnt_set,
+		dc_sum_bytes(cnts, cnt_set, IFS_RX),
+		dc_sum_packets(cnts, cnt_set, IFS_RX),
+		dc_sum_bytes(cnts, cnt_set, IFS_TX),
+		dc_sum_packets(cnts, cnt_set, IFS_TX),
+		cnts->bpc[cnt_set][IFS_RX][IFS_TCP].bytes,
+		cnts->bpc[cnt_set][IFS_RX][IFS_TCP].packets,
+		cnts->bpc[cnt_set][IFS_RX][IFS_UDP].bytes,
+		cnts->bpc[cnt_set][IFS_RX][IFS_UDP].packets,
+		cnts->bpc[cnt_set][IFS_RX][IFS_PROTO_OTHER].bytes,
+		cnts->bpc[cnt_set][IFS_RX][IFS_PROTO_OTHER].packets,
+		cnts->bpc[cnt_set][IFS_TX][IFS_TCP].bytes,
+		cnts->bpc[cnt_set][IFS_TX][IFS_TCP].packets,
+		cnts->bpc[cnt_set][IFS_TX][IFS_UDP].bytes,
+		cnts->bpc[cnt_set][IFS_TX][IFS_UDP].packets,
+		cnts->bpc[cnt_set][IFS_TX][IFS_PROTO_OTHER].bytes,
+		cnts->bpc[cnt_set][IFS_TX][IFS_PROTO_OTHER].packets);
+	return seq_has_overflowed(m) ? -ENOSPC : 1;
+}
+
+static bool pp_sets(struct seq_file *m, struct tag_stat *ts_entry)
+{
+	int ret;
+	int counter_set;
+	for (counter_set = 0; counter_set < IFS_MAX_COUNTER_SETS;
+	     counter_set++) {
+		ret = pp_stats_line(m, ts_entry, counter_set);
+		if (ret < 0)
+			return false;
+	}
+	return true;
+}
+
+static int qtaguid_stats_proc_iface_stat_ptr_valid(
+	struct qtaguid_net *qtaguid_net,
+	struct iface_stat *ptr)
+{
+	struct iface_stat *iface_entry;
+
+	if (!ptr)
+		return false;
+
+	list_for_each_entry(iface_entry, &qtaguid_net->iface_stat_list, list)
+		if (iface_entry == ptr)
+			return true;
+	return false;
+}
+
+static void qtaguid_stats_proc_next_iface_entry(struct qtaguid_net *qtaguid_net,
+						struct proc_print_info *ppi)
+{
+	spin_unlock_bh(&ppi->iface_entry->tag_stat_list_lock);
+	list_for_each_entry_continue(ppi->iface_entry,
+				     &qtaguid_net->iface_stat_list, list) {
+		spin_lock_bh(&ppi->iface_entry->tag_stat_list_lock);
+		return;
+	}
+	ppi->iface_entry = NULL;
+}
+
+static void *qtaguid_stats_proc_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	struct proc_print_info *ppi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(ppi->net);
+	struct tag_stat *ts_entry;
+	struct rb_node *node;
+
+	if (!v) {
+		pr_err("qtaguid: %s(): unexpected v: NULL\n", __func__);
+		return NULL;
+	}
+
+	(*pos)++;
+
+	if (!ppi->iface_entry || unlikely(module_passive))
+		return NULL;
+
+	if (v == SEQ_START_TOKEN)
+		node = rb_first(&ppi->iface_entry->tag_stat_tree);
+	else
+		node = rb_next(&((struct tag_stat *)v)->tn.node);
+
+	while (!node) {
+		qtaguid_stats_proc_next_iface_entry(qtaguid_net, ppi);
+		if (!ppi->iface_entry)
+			return NULL;
+		node = rb_first(&ppi->iface_entry->tag_stat_tree);
+	}
+
+	ts_entry = rb_entry(node, struct tag_stat, tn.node);
+	ppi->tag = ts_entry->tn.tag;
+	ppi->tag_pos = *pos;
+	ppi->tag_item_index = ppi->item_index;
+	return ts_entry;
+}
+
+static void *qtaguid_stats_proc_start(struct seq_file *m, loff_t *pos)
+{
+	struct proc_print_info *ppi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(ppi->net);
+	struct tag_stat *ts_entry = NULL;
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+
+	if (*pos == 0) {
+		ppi->item_index = 1;
+		ppi->tag_pos = 0;
+		if (list_empty(&qtaguid_net->iface_stat_list)) {
+			ppi->iface_entry = NULL;
+		} else {
+			ppi->iface_entry =
+				list_first_entry(&qtaguid_net->iface_stat_list,
+						 struct iface_stat,
+						 list);
+			spin_lock_bh(&ppi->iface_entry->tag_stat_list_lock);
+		}
+		return SEQ_START_TOKEN;
+	}
+	if (!qtaguid_stats_proc_iface_stat_ptr_valid(qtaguid_net,
+						     ppi->iface_entry)) {
+		if (ppi->iface_entry) {
+			pr_err("qtaguid: %s(): iface_entry %p not found\n",
+			       __func__, ppi->iface_entry);
+			ppi->iface_entry = NULL;
+		}
+		return NULL;
+	}
+
+	spin_lock_bh(&ppi->iface_entry->tag_stat_list_lock);
+
+	if (!ppi->tag_pos) {
+		/* seq_read skipped first next call */
+		ts_entry = SEQ_START_TOKEN;
+	} else {
+		ts_entry = tag_stat_tree_search(
+				&ppi->iface_entry->tag_stat_tree, ppi->tag);
+		if (!ts_entry) {
+			pr_info("qtaguid: %s(): tag_stat.tag 0x%llx not found. Abort.\n",
+				__func__, ppi->tag);
+			return NULL;
+		}
+	}
+
+	if (*pos == ppi->tag_pos) { /* normal resume */
+		ppi->item_index = ppi->tag_item_index;
+	} else {
+		/* seq_read skipped a next call */
+		*pos = ppi->tag_pos;
+		ts_entry = qtaguid_stats_proc_next(m, ts_entry, pos);
+	}
+
+	return ts_entry;
+}
+
+static void qtaguid_stats_proc_stop(struct seq_file *m, void *v)
+{
+	struct proc_print_info *ppi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(ppi->net);
+	if (ppi->iface_entry)
+		spin_unlock_bh(&ppi->iface_entry->tag_stat_list_lock);
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+}
+
+/*
+ * Procfs reader to get all tag stats using style "1)" as described in
+ * fs/proc/generic.c
+ * Groups all protocols tx/rx bytes.
+ */
+static int qtaguid_stats_proc_show(struct seq_file *m, void *v)
+{
+	struct tag_stat *ts_entry = v;
+
+	if (v == SEQ_START_TOKEN)
+		pp_stats_header(m);
+	else
+		pp_sets(m, ts_entry);
+
+	return 0;
+}
+
+/*------------------------------------------*/
+static int qtudev_open(struct inode *inode, struct file *file)
+{
+	struct net *net = current->nsproxy->net_ns;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	struct uid_tag_data *utd_entry;
+	struct proc_qtu_data *pqd_entry;
+	struct proc_qtu_data *new_pqd_entry;
+	int res;
+	bool utd_entry_found;
+
+	if (unlikely(qtu_proc_handling_passive))
+		return 0;
+
+	DR_DEBUG("qtaguid: qtudev_open(): pid=%u tgid=%u uid=%u\n",
+		 current->pid, current->tgid,
+		 from_kuid(net->user_ns, current_fsuid()));
+
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+
+	/* Look for existing uid data, or alloc one. */
+	utd_entry = get_uid_data(qtaguid_net,
+				 from_kuid(net->user_ns, current_fsuid()),
+				 &utd_entry_found);
+	if (IS_ERR_OR_NULL(utd_entry)) {
+		res = PTR_ERR(utd_entry);
+		goto err_unlock;
+	}
+
+	/* Look for existing PID based proc_data */
+	pqd_entry = proc_qtu_data_tree_search(&qtaguid_net->proc_qtu_data_tree,
+					      current->tgid);
+	if (pqd_entry) {
+		pr_err("qtaguid: qtudev_open(): %u/%u %u "
+		       "%s already opened\n",
+		       current->pid, current->tgid,
+		       from_kuid(net->user_ns, current_fsuid()),
+		       QTU_DEV_NAME);
+		res = -EBUSY;
+		goto err_unlock_free_utd;
+	}
+
+	new_pqd_entry = kzalloc(sizeof(*new_pqd_entry), GFP_ATOMIC);
+	if (!new_pqd_entry) {
+		pr_err("qtaguid: qtudev_open(): %u/%u %u: "
+		       "proc data alloc failed\n",
+		       current->pid, current->tgid,
+		       from_kuid(net->user_ns, current_fsuid()));
+		res = -ENOMEM;
+		goto err_unlock_free_utd;
+	}
+	new_pqd_entry->pid = current->tgid;
+	new_pqd_entry->net = get_net(net);
+	INIT_LIST_HEAD(&new_pqd_entry->sock_tag_list);
+	new_pqd_entry->parent_tag_data = utd_entry;
+	utd_entry->num_pqd++;
+
+	proc_qtu_data_tree_insert(new_pqd_entry,
+				  &qtaguid_net->proc_qtu_data_tree);
+
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	DR_DEBUG("qtaguid: tracking data for uid=%u in pqd=%p\n",
+		 from_kuid(net->user_ns, current_fsuid()), new_pqd_entry);
+	file->private_data = new_pqd_entry;
+	return 0;
+
+err_unlock_free_utd:
+	if (!utd_entry_found) {
+		rb_erase(&utd_entry->node, &qtaguid_net->uid_tag_data_tree);
+		kfree(utd_entry);
+	}
+err_unlock:
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	return res;
+}
+
+static int qtudev_release(struct inode *inode, struct file *file)
+{
+	struct proc_qtu_data *pqd_entry = file->private_data;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(pqd_entry->net);
+	struct uid_tag_data *utd_entry = pqd_entry->parent_tag_data;
+	struct sock_tag *st_entry;
+	struct rb_root st_to_free_tree = RB_ROOT;
+	struct list_head *entry, *next;
+	struct tag_ref *tr;
+
+	if (unlikely(qtu_proc_handling_passive))
+		return 0;
+
+	/*
+	 * Do not trust the current->pid, it might just be a kworker cleaning
+	 * up after a dead proc.
+	 */
+	DR_DEBUG("qtaguid: qtudev_release(): "
+		 "pid=%u tgid=%u uid=%u "
+		 "pqd_entry=%p->pid=%u utd_entry=%p->active_tags=%d\n",
+		 current->pid, current->tgid, pqd_entry->parent_tag_data->uid,
+		 pqd_entry, pqd_entry->pid, utd_entry,
+		 utd_entry->num_active_tags);
+
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+
+	list_for_each_safe(entry, next, &pqd_entry->sock_tag_list) {
+		st_entry = list_entry(entry, struct sock_tag, list);
+		DR_DEBUG("qtaguid: %s(): "
+			 "erase sock_tag=%p->sk=%p pid=%u tgid=%u uid=%u\n",
+			 __func__,
+			 st_entry, st_entry->sk,
+			 current->pid, current->tgid,
+			 pqd_entry->parent_tag_data->uid);
+
+		utd_entry = uid_tag_data_tree_search(
+			&qtaguid_net->uid_tag_data_tree,
+			get_uid_from_tag(st_entry->tag));
+		BUG_ON(IS_ERR_OR_NULL(utd_entry));
+		DR_DEBUG("qtaguid: %s(): "
+			 "looking for tag=0x%llx in utd_entry=%p\n", __func__,
+			 st_entry->tag, utd_entry);
+		tr = tag_ref_tree_search(&utd_entry->tag_ref_tree,
+					 st_entry->tag);
+		BUG_ON(!tr);
+		BUG_ON(tr->num_sock_tags <= 0);
+		tr->num_sock_tags--;
+		free_tag_ref_from_utd_entry(tr, utd_entry);
+
+		rb_erase(&st_entry->sock_node, &qtaguid_net->sock_tag_tree);
+		list_del(&st_entry->list);
+		/* Can't sockfd_put() within spinlock, do it later. */
+		sock_tag_tree_insert(st_entry, &st_to_free_tree);
+
+		/*
+		 * Try to free the utd_entry if no other proc_qtu_data is
+		 * using it (num_pqd is 0) and it doesn't have active tags
+		 * (num_active_tags is 0).
+		 */
+		put_utd_entry(pqd_entry->net, utd_entry);
+	}
+
+	rb_erase(&pqd_entry->node, &qtaguid_net->proc_qtu_data_tree);
+	BUG_ON(pqd_entry->parent_tag_data->num_pqd < 1);
+	pqd_entry->parent_tag_data->num_pqd--;
+	put_utd_entry(pqd_entry->net, pqd_entry->parent_tag_data);
+	put_net(pqd_entry->net);
+	kfree(pqd_entry);
+	file->private_data = NULL;
+
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+
+	sock_tag_tree_erase(&st_to_free_tree);
+
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	prdebug_full_state_locked(qtaguid_net, 0, "%s(): pid=%u tgid=%u",
+				  __func__, current->pid, current->tgid);
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+	return 0;
+}
+
+/*------------------------------------------*/
+static const struct file_operations qtudev_fops = {
+	.owner = THIS_MODULE,
+	.open = qtudev_open,
+	.release = qtudev_release,
+};
+
+static struct miscdevice qtu_device = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = QTU_DEV_NAME,
+	.fops = &qtudev_fops,
+	/* How sad it doesn't allow for defaults: .mode = S_IRUGO | S_IWUSR */
+};
+
+static const struct seq_operations proc_qtaguid_ctrl_seqops = {
+	.start = qtaguid_ctrl_proc_start,
+	.next = qtaguid_ctrl_proc_next,
+	.stop = qtaguid_ctrl_proc_stop,
+	.show = qtaguid_ctrl_proc_show,
+};
+
+static int proc_qtaguid_ctrl_open(struct inode *inode, struct file *file)
+{
+	struct proc_ctrl_print_info *pcpi;
+
+	pcpi = __seq_open_private(file, &proc_qtaguid_ctrl_seqops,
+				  sizeof(*pcpi));
+	if (!pcpi)
+		return -ENOMEM;
+
+	pcpi->net = PDE_DATA(inode);
+	return 0;
+}
+
+static const struct proc_ops proc_qtaguid_ctrl_fops = {
+	.proc_open	= proc_qtaguid_ctrl_open,
+	.proc_read	= seq_read,
+	.proc_write	= qtaguid_ctrl_proc_write,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= seq_release_private,
+};
+
+static const struct seq_operations proc_qtaguid_stats_seqops = {
+	.start = qtaguid_stats_proc_start,
+	.next = qtaguid_stats_proc_next,
+	.stop = qtaguid_stats_proc_stop,
+	.show = qtaguid_stats_proc_show,
+};
+
+static int proc_qtaguid_stats_open(struct inode *inode, struct file *file)
+{
+	struct proc_print_info *ppi;
+
+	ppi = __seq_open_private(file, &proc_qtaguid_stats_seqops,
+				 sizeof(*ppi));
+	if (!ppi)
+		return -ENOMEM;
+
+	ppi->net = PDE_DATA(inode);
+	return 0;
+}
+
+static const struct proc_ops proc_qtaguid_stats_fops = {
+	.proc_open	= proc_qtaguid_stats_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= seq_release_private,
+};
+
+/*------------------------------------------*/
+static int __net_init qtaguid_proc_register(struct net *net)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+
+	qtaguid_net->procdir = proc_mkdir("xt_qtaguid", net->proc_net);
+	if (!qtaguid_net->procdir)
+		goto out1;
+
+	qtaguid_net->ctrl_file =
+		proc_create_data("ctrl", proc_ctrl_perms,
+				 qtaguid_net->procdir,
+				 &proc_qtaguid_ctrl_fops, net);
+	if (!qtaguid_net->ctrl_file)
+		goto out2;
+
+	qtaguid_net->stats_file =
+		proc_create_data("stats", proc_stats_perms,
+				 qtaguid_net->procdir,
+				 &proc_qtaguid_stats_fops, net);
+	if (!qtaguid_net->stats_file)
+		goto out3;
+
+	qtaguid_net->iface_stat_procdir =
+		proc_mkdir("iface_stat", qtaguid_net->procdir);
+	if (!qtaguid_net->iface_stat_procdir)
+		goto out4;
+
+	qtaguid_net->iface_stat_all_procfile =
+		proc_create_data("iface_stat_all", proc_iface_perms,
+				 qtaguid_net->procdir,
+				 &proc_iface_stat_all_fops, net);
+	if (!qtaguid_net->iface_stat_all_procfile)
+		goto out5;
+
+	qtaguid_net->iface_stat_fmt_procfile =
+		proc_create_data("iface_stat_fmt", proc_iface_perms,
+				 qtaguid_net->procdir,
+				 &proc_iface_stat_fmt_fops, net);
+	if (!qtaguid_net->iface_stat_fmt_procfile)
+		goto out6;
+
+	return 0;
+
+out6:
+	remove_proc_entry("iface_stat_all", qtaguid_net->procdir);
+out5:
+	remove_proc_entry("iface_stat", qtaguid_net->procdir);
+out4:
+	remove_proc_entry("stats", qtaguid_net->procdir);
+out3:
+	remove_proc_entry("ctrl", qtaguid_net->procdir);
+out2:
+	remove_proc_entry("xt_qtaguid", net->proc_net);
+out1:
+	return -ENOMEM;
+}
+
+static int __net_init qtaguid_net_init(struct net *net)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+
+	INIT_LIST_HEAD(&qtaguid_net->iface_stat_list);
+	spin_lock_init(&qtaguid_net->iface_stat_list_lock);
+
+	qtaguid_net->sock_tag_tree = RB_ROOT;
+	spin_lock_init(&qtaguid_net->sock_tag_list_lock);
+
+	qtaguid_net->tag_counter_set_tree = RB_ROOT;
+	spin_lock_init(&qtaguid_net->tag_counter_set_list_lock);
+
+	qtaguid_net->uid_tag_data_tree = RB_ROOT;
+	spin_lock_init(&qtaguid_net->uid_tag_data_tree_lock);
+
+	if (qtaguid_proc_register(net) < 0)
+		return -EACCES;
+
+	return 0;
+}
+
+static void __net_exit qtaguid_net_exit(struct net *net)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	struct iface_stat *iface_entry, *tmp;
+
+	list_for_each_entry_safe(iface_entry, tmp,
+				 &qtaguid_net->iface_stat_list, list) {
+		iface_delete_proc(qtaguid_net, iface_entry);
+		tag_stat_tree_erase(&iface_entry->tag_stat_tree);
+		kfree(iface_entry->ifname);
+		kfree(iface_entry);
+	}
+
+	remove_proc_entry("iface_stat_fmt", qtaguid_net->procdir);
+	remove_proc_entry("iface_stat_all", qtaguid_net->procdir);
+	remove_proc_entry("iface_stat", qtaguid_net->procdir);
+	remove_proc_entry("stats", qtaguid_net->procdir);
+	remove_proc_entry("ctrl", qtaguid_net->procdir);
+	remove_proc_entry("xt_qtaguid", net->proc_net);
+
+	sock_tag_tree_erase(&qtaguid_net->sock_tag_tree);
+	tag_counter_set_tree_erase(&qtaguid_net->tag_counter_set_tree);
+	uid_tag_data_tree_erase(&qtaguid_net->uid_tag_data_tree);
+	/* proc_qtu_data_tree should be empty already because the
+	 * netns won't be destroyed until all open file descriptors
+	 * for /dev/xt_qtaguid are closed.
+	 */
+}
+
+static struct xt_match qtaguid_mt_reg __read_mostly = {
+	/*
+	 * This module masquerades as the "owner" module so that iptables
+	 * tools can deal with it.
+	 */
+	.name       = "owner",
+	.revision   = 1,
+	.family     = NFPROTO_UNSPEC,
+	.checkentry = qtaguid_check,
+	.match      = qtaguid_mt,
+	.matchsize  = sizeof(struct xt_qtaguid_match_info),
+	.me         = THIS_MODULE,
+};
+
+static struct pernet_operations qtaguid_net_ops = {
+	.init   = qtaguid_net_init,
+	.exit   = qtaguid_net_exit,
+	.id     = &qtaguid_net_id,
+	.size   = sizeof(struct qtaguid_net),
+};
+
+static int __init qtaguid_mt_init(void)
+{
+	int ret;
+
+	ret = register_pernet_subsys(&qtaguid_net_ops);
+	if (ret < 0)
+		goto out1;
+
+	ret = xt_register_match(&qtaguid_mt_reg);
+	if (ret < 0)
+		goto out2;
+
+	ret = register_netdevice_notifier(&iface_netdev_notifier_blk);
+	if (ret < 0) {
+		pr_err("qtaguid: iface_stat: init failed to register dev event handler\n");
+		goto out3;
+	}
+
+	ret = register_inetaddr_notifier(&iface_inetaddr_notifier_blk);
+	if (ret < 0) {
+		pr_err("qtaguid: iface_stat: init failed to register ipv4 dev event handler\n");
+		goto out4;
+	}
+
+	ret = register_inet6addr_notifier(&iface_inet6addr_notifier_blk);
+	if (ret < 0) {
+		pr_err("qtaguid: iface_stat: init failed to register ipv6 dev event handler\n");
+		goto out5;
+	}
+
+	ret = misc_register(&qtu_device);
+	if (ret < 0)
+		goto out6;
+
+	return 0;
+
+out6:
+	unregister_inet6addr_notifier(&iface_inet6addr_notifier_blk);
+out5:
+	unregister_inetaddr_notifier(&iface_inetaddr_notifier_blk);
+out4:
+	unregister_netdevice_notifier(&iface_netdev_notifier_blk);
+out3:
+	xt_unregister_match(&qtaguid_mt_reg);
+out2:
+	unregister_pernet_subsys(&qtaguid_net_ops);
+out1:
+	return ret;
+}
+
+/*
+ * TODO: allow unloading of the module.
+ * For now stats are permanent.
+ * Kconfig forces'y/n' and never an 'm'.
+ */
+
+module_init(qtaguid_mt_init);
+MODULE_AUTHOR("jpa <jpa@google.com>");
+MODULE_DESCRIPTION("Xtables: socket owner+tag matching and associated stats");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("ipt_owner");
+MODULE_ALIAS("ip6t_owner");
+MODULE_ALIAS("ipt_qtaguid");
+MODULE_ALIAS("ip6t_qtaguid");
Index: rpi4-kernel/net/netfilter/Makefile
===================================================================
--- rpi4-kernel.orig/net/netfilter/Makefile
+++ rpi4-kernel/net/netfilter/Makefile
@@ -194,6 +194,7 @@ obj-$(CONFIG_NETFILTER_XT_MATCH_CGROUP)
 obj-$(CONFIG_NETFILTER_XT_MATCH_PHYSDEV) += xt_physdev.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_PKTTYPE) += xt_pkttype.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_POLICY) += xt_policy.o
+obj-$(CONFIG_NETFILTER_XT_MATCH_QTAGUID) += xt_qtaguid_print.o xt_qtaguid.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_QUOTA) += xt_quota.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_RATEEST) += xt_rateest.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_REALM) += xt_realm.o
Index: rpi4-kernel/net/netfilter/xt_qtaguid_internal.h
===================================================================
--- /dev/null
+++ rpi4-kernel/net/netfilter/xt_qtaguid_internal.h
@@ -0,0 +1,353 @@
+/*
+ * Kernel iptables module to track stats for packets based on user tags.
+ *
+ * (C) 2011 Google, Inc
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#ifndef __XT_QTAGUID_INTERNAL_H__
+#define __XT_QTAGUID_INTERNAL_H__
+
+#include <linux/types.h>
+#include <linux/rbtree.h>
+#include <linux/spinlock_types.h>
+#include <linux/workqueue.h>
+#include <net/net_namespace.h>
+
+/* Iface handling */
+#define IDEBUG_MASK (1<<0)
+/* Iptable Matching. Per packet. */
+#define MDEBUG_MASK (1<<1)
+/* Red-black tree handling. Per packet. */
+#define RDEBUG_MASK (1<<2)
+/* procfs ctrl/stats handling */
+#define CDEBUG_MASK (1<<3)
+/* dev and resource tracking */
+#define DDEBUG_MASK (1<<4)
+
+/* E.g (IDEBUG_MASK | CDEBUG_MASK | DDEBUG_MASK) */
+#define DEFAULT_DEBUG_MASK 0
+
+/*
+ * (Un)Define these *DEBUG to compile out/in the pr_debug calls.
+ * All undef: text size ~ 0x3030; all def: ~ 0x4404.
+ */
+#define IDEBUG
+#define MDEBUG
+#define RDEBUG
+#define CDEBUG
+#define DDEBUG
+
+#define MSK_DEBUG(mask, ...) do {                           \
+		if (unlikely(qtaguid_debug_mask & (mask)))  \
+			pr_debug(__VA_ARGS__);              \
+	} while (0)
+#ifdef IDEBUG
+#define IF_DEBUG(...) MSK_DEBUG(IDEBUG_MASK, __VA_ARGS__)
+#else
+#define IF_DEBUG(...) no_printk(__VA_ARGS__)
+#endif
+#ifdef MDEBUG
+#define MT_DEBUG(...) MSK_DEBUG(MDEBUG_MASK, __VA_ARGS__)
+#else
+#define MT_DEBUG(...) no_printk(__VA_ARGS__)
+#endif
+#ifdef RDEBUG
+#define RB_DEBUG(...) MSK_DEBUG(RDEBUG_MASK, __VA_ARGS__)
+#else
+#define RB_DEBUG(...) no_printk(__VA_ARGS__)
+#endif
+#ifdef CDEBUG
+#define CT_DEBUG(...) MSK_DEBUG(CDEBUG_MASK, __VA_ARGS__)
+#else
+#define CT_DEBUG(...) no_printk(__VA_ARGS__)
+#endif
+#ifdef DDEBUG
+#define DR_DEBUG(...) MSK_DEBUG(DDEBUG_MASK, __VA_ARGS__)
+#else
+#define DR_DEBUG(...) no_printk(__VA_ARGS__)
+#endif
+
+extern uint qtaguid_debug_mask;
+
+/*---------------------------------------------------------------------------*/
+/*
+ * Tags:
+ *
+ * They represent what the data usage counters will be tracked against.
+ * By default a tag is just based on the UID.
+ * The UID is used as the base for policing, and can not be ignored.
+ * So a tag will always at least represent a UID (uid_tag).
+ *
+ * A tag can be augmented with an "accounting tag" which is associated
+ * with a UID.
+ * User space can set the acct_tag portion of the tag which is then used
+ * with sockets: all data belonging to that socket will be counted against the
+ * tag. The policing is then based on the tag's uid_tag portion,
+ * and stats are collected for the acct_tag portion separately.
+ *
+ * There could be
+ * a:  {acct_tag=1, uid_tag=10003}
+ * b:  {acct_tag=2, uid_tag=10003}
+ * c:  {acct_tag=3, uid_tag=10003}
+ * d:  {acct_tag=0, uid_tag=10003}
+ * a, b, and c represent tags associated with specific sockets.
+ * d is for the totals for that uid, including all untagged traffic.
+ * Typically d is used with policing/quota rules.
+ *
+ * We want tag_t big enough to distinguish uid_t and acct_tag.
+ * It might become a struct if needed.
+ * Nothing should be using it as an int.
+ */
+typedef uint64_t tag_t;  /* Only used via accessors */
+
+#define TAG_UID_MASK 0xFFFFFFFFULL
+#define TAG_ACCT_MASK (~0xFFFFFFFFULL)
+
+static inline int tag_compare(tag_t t1, tag_t t2)
+{
+	return t1 < t2 ? -1 : t1 == t2 ? 0 : 1;
+}
+
+static inline tag_t combine_atag_with_uid(tag_t acct_tag, uid_t uid)
+{
+	return acct_tag | uid;
+}
+static inline tag_t make_tag_from_uid(uid_t uid)
+{
+	return uid;
+}
+static inline uid_t get_uid_from_tag(tag_t tag)
+{
+	return tag & TAG_UID_MASK;
+}
+static inline tag_t get_utag_from_tag(tag_t tag)
+{
+	return tag & TAG_UID_MASK;
+}
+static inline tag_t get_atag_from_tag(tag_t tag)
+{
+	return tag & TAG_ACCT_MASK;
+}
+
+static inline bool valid_atag(tag_t tag)
+{
+	return !(tag & TAG_UID_MASK);
+}
+static inline tag_t make_atag_from_value(uint32_t value)
+{
+	return (uint64_t)value << 32;
+}
+/*---------------------------------------------------------------------------*/
+
+/*
+ * Maximum number of socket tags that a UID is allowed to have active.
+ * Multiple processes belonging to the same UID contribute towards this limit.
+ * Special UIDs that can impersonate a UID also contribute (e.g. download
+ * manager, ...)
+ */
+#define DEFAULT_MAX_SOCK_TAGS 1024
+
+/*
+ * For now we only track 2 sets of counters.
+ * The default set is 0.
+ * Userspace can activate another set for a given uid being tracked.
+ */
+#define IFS_MAX_COUNTER_SETS 2
+
+enum ifs_tx_rx {
+	IFS_TX,
+	IFS_RX,
+	IFS_MAX_DIRECTIONS
+};
+
+/* For now, TCP, UDP, the rest */
+enum ifs_proto {
+	IFS_TCP,
+	IFS_UDP,
+	IFS_PROTO_OTHER,
+	IFS_MAX_PROTOS
+};
+
+struct byte_packet_counters {
+	uint64_t bytes;
+	uint64_t packets;
+};
+
+struct data_counters {
+	struct byte_packet_counters bpc[IFS_MAX_COUNTER_SETS][IFS_MAX_DIRECTIONS][IFS_MAX_PROTOS];
+};
+
+static inline uint64_t dc_sum_bytes(struct data_counters *counters,
+				    int set,
+				    enum ifs_tx_rx direction)
+{
+	return counters->bpc[set][direction][IFS_TCP].bytes
+		+ counters->bpc[set][direction][IFS_UDP].bytes
+		+ counters->bpc[set][direction][IFS_PROTO_OTHER].bytes;
+}
+
+static inline uint64_t dc_sum_packets(struct data_counters *counters,
+				      int set,
+				      enum ifs_tx_rx direction)
+{
+	return counters->bpc[set][direction][IFS_TCP].packets
+		+ counters->bpc[set][direction][IFS_UDP].packets
+		+ counters->bpc[set][direction][IFS_PROTO_OTHER].packets;
+}
+
+
+/* Generic X based nodes used as a base for rb_tree ops */
+struct tag_node {
+	struct rb_node node;
+	tag_t tag;
+};
+
+struct tag_stat {
+	struct tag_node tn;
+	struct data_counters counters;
+	/*
+	 * If this tag is acct_tag based, we need to count against the
+	 * matching parent uid_tag.
+	 */
+	struct data_counters *parent_counters;
+};
+
+struct iface_stat {
+	struct list_head list;  /* in iface_stat_list */
+	char *ifname;
+	bool active;
+	/* net_dev is only valid for active iface_stat */
+	struct net_device *net_dev;
+
+	struct byte_packet_counters totals_via_dev[IFS_MAX_DIRECTIONS];
+	struct data_counters totals_via_skb;
+	/*
+	 * We keep the last_known, because some devices reset their counters
+	 * just before NETDEV_UP, while some will reset just before
+	 * NETDEV_REGISTER (which is more normal).
+	 * So now, if the device didn't do a NETDEV_UNREGISTER and we see
+	 * its current dev stats smaller that what was previously known, we
+	 * assume an UNREGISTER and just use the last_known.
+	 */
+	struct byte_packet_counters last_known[IFS_MAX_DIRECTIONS];
+	/* last_known is usable when last_known_valid is true */
+	bool last_known_valid;
+
+	struct proc_dir_entry *proc_ptr;
+
+	struct rb_root tag_stat_tree;
+	spinlock_t tag_stat_list_lock;
+};
+
+/* This is needed to create proc_dir_entries from atomic context. */
+struct iface_stat_work {
+	struct work_struct iface_work;
+	struct iface_stat *iface_entry;
+	struct net_device *net_dev;
+};
+
+/*
+ * Track tag that this socket is transferring data for, and not necessarily
+ * the uid that owns the socket.
+ * This is the tag against which tag_stat.counters will be billed.
+ * These structs need to be looked up by sock and pid.
+ */
+struct sock_tag {
+	struct rb_node sock_node;
+	struct sock *sk;  /* Only used as a number, never dereferenced */
+	/* Used to associate with a given pid */
+	struct list_head list;   /* in proc_qtu_data.sock_tag_list */
+	pid_t pid;
+
+	tag_t tag;
+};
+
+struct qtaguid_event_counts {
+	/* Various successful events */
+	atomic64_t sockets_tagged;
+	atomic64_t sockets_untagged;
+	atomic64_t counter_set_changes;
+	atomic64_t delete_cmds;
+	atomic64_t iface_events;  /* Number of NETDEV_* events handled */
+
+	atomic64_t match_calls;   /* Number of times iptables called mt */
+	/* Number of times iptables called mt from pre or post routing hooks */
+	atomic64_t match_calls_prepost;
+	/*
+	 * match_found_sk_*: numbers related to the netfilter matching
+	 * function finding a sock for the sk_buff.
+	 * Total skbs processed is sum(match_found*).
+	 */
+	atomic64_t match_found_sk;   /* An sk was already in the sk_buff. */
+	/* The connection tracker had or didn't have the sk. */
+	atomic64_t match_found_sk_in_ct;
+	atomic64_t match_found_no_sk_in_ct;
+	/*
+	 * No sk could be found. No apparent owner. Could happen with
+	 * unsolicited traffic.
+	 */
+	atomic64_t match_no_sk;
+	/*
+	 * The file ptr in the sk_socket wasn't there and we couldn't get GID.
+	 * This might happen for traffic while the socket is being closed.
+	 */
+	atomic64_t match_no_sk_gid;
+};
+
+/* Track the set active_set for the given tag. */
+struct tag_counter_set {
+	struct tag_node tn;
+	int active_set;
+};
+
+/*----------------------------------------------*/
+/*
+ * The qtu uid data is used to track resources that are created directly or
+ * indirectly by processes (uid tracked).
+ * It is shared by the processes with the same uid.
+ * Some of the resource will be counted to prevent further rogue allocations,
+ * some will need freeing once the owner process (uid) exits.
+ */
+struct uid_tag_data {
+	struct rb_node node;
+	uid_t uid;
+
+	/*
+	 * For the uid, how many accounting tags have been set.
+	 */
+	int num_active_tags;
+	/* Track the number of proc_qtu_data that reference it */
+	int num_pqd;
+	struct rb_root tag_ref_tree;
+	/* No tag_node_tree_lock; use uid_tag_data_tree_lock */
+};
+
+struct tag_ref {
+	struct tag_node tn;
+
+	/*
+	 * This tracks the number of active sockets that have a tag on them
+	 * which matches this tag_ref.tn.tag.
+	 * A tag ref can live on after the sockets are untagged.
+	 * A tag ref can only be removed during a tag delete command.
+	 */
+	int num_sock_tags;
+};
+
+struct proc_qtu_data {
+	struct net *net;
+	struct rb_node node;
+	pid_t pid;
+
+	struct uid_tag_data *parent_tag_data;
+
+	/* Tracks the sock_tags that need freeing upon this proc's death */
+	struct list_head sock_tag_list;
+	/* No spinlock_t sock_tag_list_lock; use the global one. */
+};
+
+/*----------------------------------------------*/
+#endif  /* ifndef __XT_QTAGUID_INTERNAL_H__ */
Index: rpi4-kernel/net/netfilter/xt_qtaguid_print.c
===================================================================
--- /dev/null
+++ rpi4-kernel/net/netfilter/xt_qtaguid_print.c
@@ -0,0 +1,565 @@
+/*
+ * Pretty printing Support for iptables xt_qtaguid module.
+ *
+ * (C) 2011 Google, Inc
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/*
+ * Most of the functions in this file just waste time if DEBUG is not defined.
+ * The matching xt_qtaguid_print.h will static inline empty funcs if the needed
+ * debug flags ore not defined.
+ * Those funcs that fail to allocate memory will panic as there is no need to
+ * hobble allong just pretending to do the requested work.
+ */
+
+#define DEBUG
+
+#include <linux/fs.h>
+#include <linux/gfp.h>
+#include <linux/net.h>
+#include <linux/rbtree.h>
+#include <linux/slab.h>
+#include <linux/spinlock_types.h>
+#include <net/sock.h>
+
+#include "xt_qtaguid_internal.h"
+#include "xt_qtaguid_print.h"
+
+#ifdef DDEBUG
+
+static void _bug_on_err_or_null(void *ptr)
+{
+	if (IS_ERR_OR_NULL(ptr)) {
+		pr_err("qtaguid: kmalloc failed\n");
+		BUG();
+	}
+}
+
+char *pp_tag_t(tag_t *tag)
+{
+	char *res;
+
+	if (!tag)
+		res = kasprintf(GFP_ATOMIC, "tag_t@null{}");
+	else
+		res = kasprintf(GFP_ATOMIC,
+				"tag_t@%p{tag=0x%llx, uid=%u}",
+				tag, *tag, get_uid_from_tag(*tag));
+	_bug_on_err_or_null(res);
+	return res;
+}
+
+char *pp_data_counters(struct data_counters *dc, bool showValues)
+{
+	char *res;
+
+	if (!dc)
+		res = kasprintf(GFP_ATOMIC, "data_counters@null{}");
+	else if (showValues)
+		res = kasprintf(
+			GFP_ATOMIC, "data_counters@%p{"
+			"set0{"
+			"rx{"
+			"tcp{b=%llu, p=%llu}, "
+			"udp{b=%llu, p=%llu},"
+			"other{b=%llu, p=%llu}}, "
+			"tx{"
+			"tcp{b=%llu, p=%llu}, "
+			"udp{b=%llu, p=%llu},"
+			"other{b=%llu, p=%llu}}}, "
+			"set1{"
+			"rx{"
+			"tcp{b=%llu, p=%llu}, "
+			"udp{b=%llu, p=%llu},"
+			"other{b=%llu, p=%llu}}, "
+			"tx{"
+			"tcp{b=%llu, p=%llu}, "
+			"udp{b=%llu, p=%llu},"
+			"other{b=%llu, p=%llu}}}}",
+			dc,
+			dc->bpc[0][IFS_RX][IFS_TCP].bytes,
+			dc->bpc[0][IFS_RX][IFS_TCP].packets,
+			dc->bpc[0][IFS_RX][IFS_UDP].bytes,
+			dc->bpc[0][IFS_RX][IFS_UDP].packets,
+			dc->bpc[0][IFS_RX][IFS_PROTO_OTHER].bytes,
+			dc->bpc[0][IFS_RX][IFS_PROTO_OTHER].packets,
+			dc->bpc[0][IFS_TX][IFS_TCP].bytes,
+			dc->bpc[0][IFS_TX][IFS_TCP].packets,
+			dc->bpc[0][IFS_TX][IFS_UDP].bytes,
+			dc->bpc[0][IFS_TX][IFS_UDP].packets,
+			dc->bpc[0][IFS_TX][IFS_PROTO_OTHER].bytes,
+			dc->bpc[0][IFS_TX][IFS_PROTO_OTHER].packets,
+			dc->bpc[1][IFS_RX][IFS_TCP].bytes,
+			dc->bpc[1][IFS_RX][IFS_TCP].packets,
+			dc->bpc[1][IFS_RX][IFS_UDP].bytes,
+			dc->bpc[1][IFS_RX][IFS_UDP].packets,
+			dc->bpc[1][IFS_RX][IFS_PROTO_OTHER].bytes,
+			dc->bpc[1][IFS_RX][IFS_PROTO_OTHER].packets,
+			dc->bpc[1][IFS_TX][IFS_TCP].bytes,
+			dc->bpc[1][IFS_TX][IFS_TCP].packets,
+			dc->bpc[1][IFS_TX][IFS_UDP].bytes,
+			dc->bpc[1][IFS_TX][IFS_UDP].packets,
+			dc->bpc[1][IFS_TX][IFS_PROTO_OTHER].bytes,
+			dc->bpc[1][IFS_TX][IFS_PROTO_OTHER].packets);
+	else
+		res = kasprintf(GFP_ATOMIC, "data_counters@%p{...}", dc);
+	_bug_on_err_or_null(res);
+	return res;
+}
+
+char *pp_tag_node(struct tag_node *tn)
+{
+	char *tag_str;
+	char *res;
+
+	if (!tn) {
+		res = kasprintf(GFP_ATOMIC, "tag_node@null{}");
+		_bug_on_err_or_null(res);
+		return res;
+	}
+	tag_str = pp_tag_t(&tn->tag);
+	res = kasprintf(GFP_ATOMIC,
+			"tag_node@%p{tag=%s}",
+			tn, tag_str);
+	_bug_on_err_or_null(res);
+	kfree(tag_str);
+	return res;
+}
+
+char *pp_tag_ref(struct tag_ref *tr)
+{
+	char *tn_str;
+	char *res;
+
+	if (!tr) {
+		res = kasprintf(GFP_ATOMIC, "tag_ref@null{}");
+		_bug_on_err_or_null(res);
+		return res;
+	}
+	tn_str = pp_tag_node(&tr->tn);
+	res = kasprintf(GFP_ATOMIC,
+			"tag_ref@%p{%s, num_sock_tags=%d}",
+			tr, tn_str, tr->num_sock_tags);
+	_bug_on_err_or_null(res);
+	kfree(tn_str);
+	return res;
+}
+
+char *pp_tag_stat(struct tag_stat *ts)
+{
+	char *tn_str;
+	char *counters_str;
+	char *parent_counters_str;
+	char *res;
+
+	if (!ts) {
+		res = kasprintf(GFP_ATOMIC, "tag_stat@null{}");
+		_bug_on_err_or_null(res);
+		return res;
+	}
+	tn_str = pp_tag_node(&ts->tn);
+	counters_str = pp_data_counters(&ts->counters, true);
+	parent_counters_str = pp_data_counters(ts->parent_counters, false);
+	res = kasprintf(GFP_ATOMIC,
+			"tag_stat@%p{%s, counters=%s, parent_counters=%s}",
+			ts, tn_str, counters_str, parent_counters_str);
+	_bug_on_err_or_null(res);
+	kfree(tn_str);
+	kfree(counters_str);
+	kfree(parent_counters_str);
+	return res;
+}
+
+char *pp_iface_stat(struct iface_stat *is)
+{
+	char *res;
+	if (!is) {
+		res = kasprintf(GFP_ATOMIC, "iface_stat@null{}");
+	} else {
+		struct data_counters *cnts = &is->totals_via_skb;
+		res = kasprintf(GFP_ATOMIC, "iface_stat@%p{"
+				"list=list_head{...}, "
+				"ifname=%s, "
+				"total_dev={rx={bytes=%llu, "
+				"packets=%llu}, "
+				"tx={bytes=%llu, "
+				"packets=%llu}}, "
+				"total_skb={rx={bytes=%llu, "
+				"packets=%llu}, "
+				"tx={bytes=%llu, "
+				"packets=%llu}}, "
+				"last_known_valid=%d, "
+				"last_known={rx={bytes=%llu, "
+				"packets=%llu}, "
+				"tx={bytes=%llu, "
+				"packets=%llu}}, "
+				"active=%d, "
+				"net_dev=%p, "
+				"proc_ptr=%p, "
+				"tag_stat_tree=rb_root{...}}",
+				is,
+				is->ifname,
+				is->totals_via_dev[IFS_RX].bytes,
+				is->totals_via_dev[IFS_RX].packets,
+				is->totals_via_dev[IFS_TX].bytes,
+				is->totals_via_dev[IFS_TX].packets,
+				dc_sum_bytes(cnts, 0, IFS_RX),
+				dc_sum_packets(cnts, 0, IFS_RX),
+				dc_sum_bytes(cnts, 0, IFS_TX),
+				dc_sum_packets(cnts, 0, IFS_TX),
+				is->last_known_valid,
+				is->last_known[IFS_RX].bytes,
+				is->last_known[IFS_RX].packets,
+				is->last_known[IFS_TX].bytes,
+				is->last_known[IFS_TX].packets,
+				is->active,
+				is->net_dev,
+				is->proc_ptr);
+	}
+	_bug_on_err_or_null(res);
+	return res;
+}
+
+char *pp_sock_tag(struct sock_tag *st)
+{
+	char *tag_str;
+	char *res;
+
+	if (!st) {
+		res = kasprintf(GFP_ATOMIC, "sock_tag@null{}");
+		_bug_on_err_or_null(res);
+		return res;
+	}
+	tag_str = pp_tag_t(&st->tag);
+	res = kasprintf(GFP_ATOMIC, "sock_tag@%p{"
+			"sock_node=rb_node{...}, "
+			"sk=%p (f_count=%d), list=list_head{...}, "
+			"pid=%u, tag=%s}",
+			st, st->sk, refcount_read(&st->sk->sk_refcnt),
+			st->pid, tag_str);
+	_bug_on_err_or_null(res);
+	kfree(tag_str);
+	return res;
+}
+
+char *pp_uid_tag_data(struct uid_tag_data *utd)
+{
+	char *res;
+
+	if (!utd)
+		res = kasprintf(GFP_ATOMIC, "uid_tag_data@null{}");
+	else
+		res = kasprintf(GFP_ATOMIC, "uid_tag_data@%p{"
+				"uid=%u, num_active_acct_tags=%d, "
+				"num_pqd=%d, "
+				"tag_node_tree=rb_root{...}, "
+				"proc_qtu_data_tree=rb_root{...}}",
+				utd, utd->uid,
+				utd->num_active_tags, utd->num_pqd);
+	_bug_on_err_or_null(res);
+	return res;
+}
+
+char *pp_proc_qtu_data(struct proc_qtu_data *pqd)
+{
+	char *parent_tag_data_str;
+	char *res;
+
+	if (!pqd) {
+		res = kasprintf(GFP_ATOMIC, "proc_qtu_data@null{}");
+		_bug_on_err_or_null(res);
+		return res;
+	}
+	parent_tag_data_str = pp_uid_tag_data(pqd->parent_tag_data);
+	res = kasprintf(GFP_ATOMIC, "proc_qtu_data@%p{"
+			"node=rb_node{...}, pid=%u, "
+			"parent_tag_data=%s, "
+			"sock_tag_list=list_head{...}}",
+			pqd, pqd->pid, parent_tag_data_str
+		);
+	_bug_on_err_or_null(res);
+	kfree(parent_tag_data_str);
+	return res;
+}
+
+/*------------------------------------------*/
+void prdebug_sock_tag_tree(int indent_level,
+			   struct rb_root *sock_tag_tree)
+{
+	struct rb_node *node;
+	struct sock_tag *sock_tag_entry;
+	char *str;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (RB_EMPTY_ROOT(sock_tag_tree)) {
+		str = "sock_tag_tree=rb_root{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "sock_tag_tree=rb_root{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	for (node = rb_first(sock_tag_tree);
+	     node;
+	     node = rb_next(node)) {
+		sock_tag_entry = rb_entry(node, struct sock_tag, sock_node);
+		str = pp_sock_tag(sock_tag_entry);
+		pr_debug("%*d: %s,\n", indent_level*2, indent_level, str);
+		kfree(str);
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_sock_tag_list(int indent_level,
+			   struct list_head *sock_tag_list)
+{
+	struct sock_tag *sock_tag_entry;
+	char *str;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (list_empty(sock_tag_list)) {
+		str = "sock_tag_list=list_head{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "sock_tag_list=list_head{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	list_for_each_entry(sock_tag_entry, sock_tag_list, list) {
+		str = pp_sock_tag(sock_tag_entry);
+		pr_debug("%*d: %s,\n", indent_level*2, indent_level, str);
+		kfree(str);
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_proc_qtu_data_tree(int indent_level,
+				struct rb_root *proc_qtu_data_tree)
+{
+	char *str;
+	struct rb_node *node;
+	struct proc_qtu_data *proc_qtu_data_entry;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (RB_EMPTY_ROOT(proc_qtu_data_tree)) {
+		str = "proc_qtu_data_tree=rb_root{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "proc_qtu_data_tree=rb_root{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	for (node = rb_first(proc_qtu_data_tree);
+	     node;
+	     node = rb_next(node)) {
+		proc_qtu_data_entry = rb_entry(node,
+					       struct proc_qtu_data,
+					       node);
+		str = pp_proc_qtu_data(proc_qtu_data_entry);
+		pr_debug("%*d: %s,\n", indent_level*2, indent_level,
+			 str);
+		kfree(str);
+		indent_level++;
+		prdebug_sock_tag_list(indent_level,
+				      &proc_qtu_data_entry->sock_tag_list);
+		indent_level--;
+
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_tag_ref_tree(int indent_level, struct rb_root *tag_ref_tree)
+{
+	char *str;
+	struct rb_node *node;
+	struct tag_ref *tag_ref_entry;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (RB_EMPTY_ROOT(tag_ref_tree)) {
+		str = "tag_ref_tree{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "tag_ref_tree{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	for (node = rb_first(tag_ref_tree);
+	     node;
+	     node = rb_next(node)) {
+		tag_ref_entry = rb_entry(node,
+					 struct tag_ref,
+					 tn.node);
+		str = pp_tag_ref(tag_ref_entry);
+		pr_debug("%*d: %s,\n", indent_level*2, indent_level,
+			 str);
+		kfree(str);
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_uid_tag_data_tree(int indent_level,
+			       struct rb_root *uid_tag_data_tree)
+{
+	char *str;
+	struct rb_node *node;
+	struct uid_tag_data *uid_tag_data_entry;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (RB_EMPTY_ROOT(uid_tag_data_tree)) {
+		str = "uid_tag_data_tree=rb_root{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "uid_tag_data_tree=rb_root{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	for (node = rb_first(uid_tag_data_tree);
+	     node;
+	     node = rb_next(node)) {
+		uid_tag_data_entry = rb_entry(node, struct uid_tag_data,
+					      node);
+		str = pp_uid_tag_data(uid_tag_data_entry);
+		pr_debug("%*d: %s,\n", indent_level*2, indent_level, str);
+		kfree(str);
+		if (!RB_EMPTY_ROOT(&uid_tag_data_entry->tag_ref_tree)) {
+			indent_level++;
+			prdebug_tag_ref_tree(indent_level,
+					     &uid_tag_data_entry->tag_ref_tree);
+			indent_level--;
+		}
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_tag_stat_tree(int indent_level,
+				  struct rb_root *tag_stat_tree)
+{
+	char *str;
+	struct rb_node *node;
+	struct tag_stat *ts_entry;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (RB_EMPTY_ROOT(tag_stat_tree)) {
+		str = "tag_stat_tree{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "tag_stat_tree{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	for (node = rb_first(tag_stat_tree);
+	     node;
+	     node = rb_next(node)) {
+		ts_entry = rb_entry(node, struct tag_stat, tn.node);
+		str = pp_tag_stat(ts_entry);
+		pr_debug("%*d: %s\n", indent_level*2, indent_level,
+			 str);
+		kfree(str);
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_iface_stat_list(int indent_level,
+			     struct list_head *iface_stat_list)
+{
+	char *str;
+	struct iface_stat *iface_entry;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (list_empty(iface_stat_list)) {
+		str = "iface_stat_list=list_head{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "iface_stat_list=list_head{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	list_for_each_entry(iface_entry, iface_stat_list, list) {
+		str = pp_iface_stat(iface_entry);
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		kfree(str);
+
+		spin_lock_bh(&iface_entry->tag_stat_list_lock);
+		if (!RB_EMPTY_ROOT(&iface_entry->tag_stat_tree)) {
+			indent_level++;
+			prdebug_tag_stat_tree(indent_level,
+					      &iface_entry->tag_stat_tree);
+			indent_level--;
+		}
+		spin_unlock_bh(&iface_entry->tag_stat_list_lock);
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+#endif  /* ifdef DDEBUG */
+/*------------------------------------------*/
+static const char * const netdev_event_strings[] = {
+	"netdev_unknown",
+	"NETDEV_UP",
+	"NETDEV_DOWN",
+	"NETDEV_REBOOT",
+	"NETDEV_CHANGE",
+	"NETDEV_REGISTER",
+	"NETDEV_UNREGISTER",
+	"NETDEV_CHANGEMTU",
+	"NETDEV_CHANGEADDR",
+	"NETDEV_GOING_DOWN",
+	"NETDEV_CHANGENAME",
+	"NETDEV_FEAT_CHANGE",
+	"NETDEV_BONDING_FAILOVER",
+	"NETDEV_PRE_UP",
+	"NETDEV_PRE_TYPE_CHANGE",
+	"NETDEV_POST_TYPE_CHANGE",
+	"NETDEV_POST_INIT",
+	"NETDEV_UNREGISTER_BATCH",
+	"NETDEV_RELEASE",
+	"NETDEV_NOTIFY_PEERS",
+	"NETDEV_JOIN",
+};
+
+const char *netdev_evt_str(int netdev_event)
+{
+	if (netdev_event < 0
+	    || netdev_event >= ARRAY_SIZE(netdev_event_strings))
+		return "bad event num";
+	return netdev_event_strings[netdev_event];
+}
Index: rpi4-kernel/net/netfilter/xt_qtaguid_print.h
===================================================================
--- /dev/null
+++ rpi4-kernel/net/netfilter/xt_qtaguid_print.h
@@ -0,0 +1,120 @@
+/*
+ * Pretty printing Support for iptables xt_qtaguid module.
+ *
+ * (C) 2011 Google, Inc
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#ifndef __XT_QTAGUID_PRINT_H__
+#define __XT_QTAGUID_PRINT_H__
+
+#include "xt_qtaguid_internal.h"
+
+#ifdef DDEBUG
+
+char *pp_tag_t(tag_t *tag);
+char *pp_data_counters(struct data_counters *dc, bool showValues);
+char *pp_tag_node(struct tag_node *tn);
+char *pp_tag_ref(struct tag_ref *tr);
+char *pp_tag_stat(struct tag_stat *ts);
+char *pp_iface_stat(struct iface_stat *is);
+char *pp_sock_tag(struct sock_tag *st);
+char *pp_uid_tag_data(struct uid_tag_data *qtd);
+char *pp_proc_qtu_data(struct proc_qtu_data *pqd);
+
+/*------------------------------------------*/
+void prdebug_sock_tag_list(int indent_level,
+			   struct list_head *sock_tag_list);
+void prdebug_sock_tag_tree(int indent_level,
+			   struct rb_root *sock_tag_tree);
+void prdebug_proc_qtu_data_tree(int indent_level,
+				struct rb_root *proc_qtu_data_tree);
+void prdebug_tag_ref_tree(int indent_level, struct rb_root *tag_ref_tree);
+void prdebug_uid_tag_data_tree(int indent_level,
+			       struct rb_root *uid_tag_data_tree);
+void prdebug_tag_stat_tree(int indent_level,
+			   struct rb_root *tag_stat_tree);
+void prdebug_iface_stat_list(int indent_level,
+			     struct list_head *iface_stat_list);
+
+#else
+
+/*------------------------------------------*/
+static inline char *pp_tag_t(tag_t *tag)
+{
+	return NULL;
+}
+static inline char *pp_data_counters(struct data_counters *dc, bool showValues)
+{
+	return NULL;
+}
+static inline char *pp_tag_node(struct tag_node *tn)
+{
+	return NULL;
+}
+static inline char *pp_tag_ref(struct tag_ref *tr)
+{
+	return NULL;
+}
+static inline char *pp_tag_stat(struct tag_stat *ts)
+{
+	return NULL;
+}
+static inline char *pp_iface_stat(struct iface_stat *is)
+{
+	return NULL;
+}
+static inline char *pp_sock_tag(struct sock_tag *st)
+{
+	return NULL;
+}
+static inline char *pp_uid_tag_data(struct uid_tag_data *qtd)
+{
+	return NULL;
+}
+static inline char *pp_proc_qtu_data(struct proc_qtu_data *pqd)
+{
+	return NULL;
+}
+
+/*------------------------------------------*/
+static inline
+void prdebug_sock_tag_list(int indent_level,
+			   struct list_head *sock_tag_list)
+{
+}
+static inline
+void prdebug_sock_tag_tree(int indent_level,
+			   struct rb_root *sock_tag_tree)
+{
+}
+static inline
+void prdebug_proc_qtu_data_tree(int indent_level,
+				struct rb_root *proc_qtu_data_tree)
+{
+}
+static inline
+void prdebug_tag_ref_tree(int indent_level, struct rb_root *tag_ref_tree)
+{
+}
+static inline
+void prdebug_uid_tag_data_tree(int indent_level,
+			       struct rb_root *uid_tag_data_tree)
+{
+}
+static inline
+void prdebug_tag_stat_tree(int indent_level,
+			   struct rb_root *tag_stat_tree)
+{
+}
+static inline
+void prdebug_iface_stat_list(int indent_level,
+			     struct list_head *iface_stat_list)
+{
+}
+#endif
+/*------------------------------------------*/
+const char *netdev_evt_str(int netdev_event);
+#endif  /* ifndef __XT_QTAGUID_PRINT_H__ */
Index: rpi4-kernel/include/linux/mm_inline.h
===================================================================
--- rpi4-kernel.orig/include/linux/mm_inline.h
+++ rpi4-kernel/include/linux/mm_inline.h
@@ -79,6 +79,25 @@ static __always_inline enum lru_list pag
 	return lru;
 }
 
+static inline bool lru_gen_enabled(void)
+{
+  return false;
+}
+
+static inline bool lru_gen_addition(struct page *page, struct lruvec *lruvec, bool front)
+{
+  return false;
+}
+
+static inline bool lru_gen_deletion(struct page *page, struct lruvec *lruvec)
+{
+  return false;
+}
+
+static inline void page_inc_usage(struct page *page)
+{
+}
+
 static __always_inline void add_page_to_lru_list(struct page *page,
 				struct lruvec *lruvec)
 {
Index: rpi4-kernel/mm/madvise.c
===================================================================
--- rpi4-kernel.orig/mm/madvise.c
+++ rpi4-kernel/mm/madvise.c
@@ -138,7 +138,7 @@ static long madvise_behavior(struct vm_a
 	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
 	*prev = vma_merge(mm, *prev, start, end, new_flags, vma->anon_vma,
 			  vma->vm_file, pgoff, vma_policy(vma),
-			  vma->vm_userfaultfd_ctx);
+			  vma->vm_userfaultfd_ctx, vma_get_anon_name(vma));
 	if (*prev) {
 		vma = *prev;
 		goto success;
Index: rpi4-kernel/include/uapi/linux/netfilter/xt_IDLETIMER.h
===================================================================
--- rpi4-kernel.orig/include/uapi/linux/netfilter/xt_IDLETIMER.h
+++ rpi4-kernel/include/uapi/linux/netfilter/xt_IDLETIMER.h
@@ -19,11 +19,18 @@
 #define MAX_IDLETIMER_LABEL_SIZE 28
 #define XT_IDLETIMER_ALARM 0x01
 
+#define NLMSG_MAX_SIZE 64
+
+#define NL_EVENT_TYPE_INACTIVE 0
+#define NL_EVENT_TYPE_ACTIVE 1
+
 struct idletimer_tg_info {
 	__u32 timeout;
 
 	char label[MAX_IDLETIMER_LABEL_SIZE];
 
+   __u8 send_nl_msg;
+
 	/* for kernel module internal use only */
 	struct idletimer_tg *timer __attribute__((aligned(8)));
 };
Index: rpi4-kernel/fs/namespace.c
===================================================================
--- rpi4-kernel.orig/fs/namespace.c
+++ rpi4-kernel/fs/namespace.c
@@ -3275,6 +3275,15 @@ int path_mount(const char *dev_name, str
 	if (!(flags & MS_NOATIME))
 		mnt_flags |= MNT_RELATIME;
 
+  if (data_page && !(flags & MS_NOSYMFOLLOW)) {
+    if (!strncmp((char *)data_page, "nosymfollow", 11) ||
+        strstr((char *)data_page, ",nosymfollow")) {
+      WARN(1,
+           "nosymfollow passed in mount data should be changed to the MS_NOSYMFOLLOW flag.");
+      flags |= MS_NOSYMFOLLOW;
+    }
+  }
+
 	/* Separate the per-mountpoint flags */
 	if (flags & MS_NOSUID)
 		mnt_flags |= MNT_NOSUID;
Index: rpi4-kernel/net/ipv4/af_inet.c
===================================================================
--- rpi4-kernel.orig/net/ipv4/af_inet.c
+++ rpi4-kernel/net/ipv4/af_inet.c
@@ -419,6 +419,9 @@ int inet_release(struct socket *sock)
 		if (!sk->sk_kern_sock)
 			BPF_CGROUP_RUN_PROG_INET_SOCK_RELEASE(sk);
 
+#ifdef CONFIG_NETFILTER_XT_MATCH_QTAGUID
+    qtaguid_untag(sock, true);
+#endif
 		/* Applications forget to leave groups before exiting */
 		ip_mc_drop_socket(sk);
 
Index: rpi4-kernel/net/netfilter/Kconfig
===================================================================
--- rpi4-kernel.orig/net/netfilter/Kconfig
+++ rpi4-kernel/net/netfilter/Kconfig
@@ -1500,6 +1500,22 @@ config NETFILTER_XT_MATCH_PKTTYPE
 
 	  To compile it as a module, choose M here.  If unsure, say N.
 
+config NETFILTER_XT_MATCH_QTAGUID
+  bool '"quota, tag, owner" match and stats support'
+        depends on NETFILTER_XT_MATCH_SOCKET
+  depends on NETFILTER_XT_MATCH_OWNER=n
+  help
+    This option replaces the `owner' match. In addition to matching
+    on uid, it keeps stats based on a tag assigned to a socket.
+    The full tag is comprised of a UID and an accounting tag.
+    The tags are assignable to sockets from user space (e.g. a download
+    manager can assign the socket to another UID for accounting).
+    Stats and control are done via /proc/net/xt_qtaguid/.
+    It replaces owner as it takes the same arguments, but should
+    really be recognized by the iptables tool.
+
+    If unsure, say `N'.
+
 config NETFILTER_XT_MATCH_QUOTA
 	tristate '"quota" match support'
 	depends on NETFILTER_ADVANCED
Index: rpi4-kernel/net/netfilter/xt_IDLETIMER.c
===================================================================
--- rpi4-kernel.orig/net/netfilter/xt_IDLETIMER.c
+++ rpi4-kernel/net/netfilter/xt_IDLETIMER.c
@@ -6,6 +6,7 @@
  * After timer expires a kevent will be sent.
  *
  * Copyright (C) 2004, 2010 Nokia Corporation
+ *
  * Written by Timo Teras <ext-timo.teras@nokia.com>
  *
  * Converted to x_tables and reworked for upstream inclusion
@@ -26,8 +27,17 @@
 #include <linux/netfilter/xt_IDLETIMER.h>
 #include <linux/kdev_t.h>
 #include <linux/kobject.h>
+#include <linux/skbuff.h>
 #include <linux/workqueue.h>
 #include <linux/sysfs.h>
+#include <linux/rtc.h>
+#include <linux/time.h>
+#include <linux/math64.h>
+#include <linux/suspend.h>
+#include <linux/notifier.h>
+#include <net/net_namespace.h>
+#include <net/sock.h>
+#include <net/inet_sock.h>
 
 struct idletimer_tg {
 	struct list_head entry;
@@ -38,15 +48,114 @@ struct idletimer_tg {
 	struct kobject *kobj;
 	struct device_attribute attr;
 
+	struct timespec64 delayed_timer_trigger;
+	struct timespec64 last_modified_timer;
+	struct timespec64 last_suspend_time;
+	struct notifier_block pm_nb;
+
+	int timeout;
 	unsigned int refcnt;
 	u8 timer_type;
+
+	bool work_pending;
+	bool send_nl_msg;
+	bool active;
+	uid_t uid;
+	bool suspend_time_valid;
 };
 
 static LIST_HEAD(idletimer_tg_list);
 static DEFINE_MUTEX(list_mutex);
+static DEFINE_SPINLOCK(timestamp_lock);
 
 static struct kobject *idletimer_tg_kobj;
 
+static bool check_for_delayed_trigger(struct idletimer_tg *timer,
+		struct timespec64 *ts)
+{
+	bool state;
+	struct timespec64 temp;
+	spin_lock_bh(&timestamp_lock);
+	timer->work_pending = false;
+	if ((ts->tv_sec - timer->last_modified_timer.tv_sec) > timer->timeout ||
+			timer->delayed_timer_trigger.tv_sec != 0) {
+		state = false;
+		temp.tv_sec = timer->timeout;
+		temp.tv_nsec = 0;
+		if (timer->delayed_timer_trigger.tv_sec != 0) {
+			temp = timespec64_add(timer->delayed_timer_trigger,
+					      temp);
+			ts->tv_sec = temp.tv_sec;
+			ts->tv_nsec = temp.tv_nsec;
+			timer->delayed_timer_trigger.tv_sec = 0;
+			timer->work_pending = true;
+			schedule_work(&timer->work);
+		} else {
+			temp = timespec64_add(timer->last_modified_timer, temp);
+			ts->tv_sec = temp.tv_sec;
+			ts->tv_nsec = temp.tv_nsec;
+		}
+	} else {
+		state = timer->active;
+	}
+	spin_unlock_bh(&timestamp_lock);
+	return state;
+}
+
+static void notify_netlink_uevent(const char *iface, struct idletimer_tg *timer)
+{
+	char iface_msg[NLMSG_MAX_SIZE];
+	char state_msg[NLMSG_MAX_SIZE];
+	char timestamp_msg[NLMSG_MAX_SIZE];
+	char uid_msg[NLMSG_MAX_SIZE];
+	char *envp[] = { iface_msg, state_msg, timestamp_msg, uid_msg, NULL };
+	int res;
+	struct timespec64 ts;
+	uint64_t time_ns;
+	bool state;
+
+	res = snprintf(iface_msg, NLMSG_MAX_SIZE, "INTERFACE=%s",
+		       iface);
+	if (NLMSG_MAX_SIZE <= res) {
+		pr_err("message too long (%d)", res);
+		return;
+	}
+
+	ts = ktime_to_timespec64(ktime_get_boottime());
+	state = check_for_delayed_trigger(timer, &ts);
+	res = snprintf(state_msg, NLMSG_MAX_SIZE, "STATE=%s",
+			state ? "active" : "inactive");
+
+	if (NLMSG_MAX_SIZE <= res) {
+		pr_err("message too long (%d)", res);
+		return;
+	}
+
+	if (state) {
+		res = snprintf(uid_msg, NLMSG_MAX_SIZE, "UID=%u", timer->uid);
+		if (NLMSG_MAX_SIZE <= res)
+			pr_err("message too long (%d)", res);
+	} else {
+		res = snprintf(uid_msg, NLMSG_MAX_SIZE, "UID=");
+		if (NLMSG_MAX_SIZE <= res)
+			pr_err("message too long (%d)", res);
+	}
+
+	time_ns = timespec64_to_ns(&ts);
+	res = snprintf(timestamp_msg, NLMSG_MAX_SIZE, "TIME_NS=%llu", time_ns);
+	if (NLMSG_MAX_SIZE <= res) {
+		timestamp_msg[0] = '\0';
+		pr_err("message too long (%d)", res);
+	}
+
+	pr_debug("putting nlmsg: <%s> <%s> <%s> <%s>\n", iface_msg, state_msg,
+		 timestamp_msg, uid_msg);
+	kobject_uevent_env(idletimer_tg_kobj, KOBJ_CHANGE, envp);
+	return;
+
+
+}
+
 static
 struct idletimer_tg *__idletimer_tg_find_by_label(const char *label)
 {
@@ -67,6 +176,7 @@ static ssize_t idletimer_tg_show(struct
 	unsigned long expires = 0;
 	struct timespec64 ktimespec = {};
 	long time_diff = 0;
+	unsigned long now = jiffies;
 
 	mutex_lock(&list_mutex);
 
@@ -84,9 +194,13 @@ static ssize_t idletimer_tg_show(struct
 
 	mutex_unlock(&list_mutex);
 
-	if (time_after(expires, jiffies) || ktimespec.tv_sec > 0)
+	if (time_after(expires, now) || ktimespec.tv_sec > 0)
 		return snprintf(buf, PAGE_SIZE, "%ld\n", time_diff);
 
+	if (timer->send_nl_msg)
+		return sprintf(buf, "0 %d\n",
+			jiffies_to_msecs(now - expires) / 1000);
+
 	return snprintf(buf, PAGE_SIZE, "0\n");
 }
 
@@ -96,6 +210,9 @@ static void idletimer_tg_work(struct wor
 						  work);
 
 	sysfs_notify(idletimer_tg_kobj, NULL, timer->attr.attr.name);
+
+	if (timer->send_nl_msg)
+		notify_netlink_uevent(timer->attr.attr.name, timer);
 }
 
 static void idletimer_tg_expired(struct timer_list *t)
@@ -103,8 +220,61 @@ static void idletimer_tg_expired(struct
 	struct idletimer_tg *timer = from_timer(timer, t, timer);
 
 	pr_debug("timer %s expired\n", timer->attr.attr.name);
-
+	spin_lock_bh(&timestamp_lock);
+	timer->active = false;
+	timer->work_pending = true;
 	schedule_work(&timer->work);
+	spin_unlock_bh(&timestamp_lock);
+}
+
+static int idletimer_resume(struct notifier_block *notifier,
+		unsigned long pm_event, void *unused)
+{
+	struct timespec64 ts;
+	unsigned long time_diff, now = jiffies;
+	struct idletimer_tg *timer = container_of(notifier,
+			struct idletimer_tg, pm_nb);
+	if (!timer)
+		return NOTIFY_DONE;
+	switch (pm_event) {
+	case PM_SUSPEND_PREPARE:
+		timer->last_suspend_time =
+			ktime_to_timespec64(ktime_get_boottime());
+		timer->suspend_time_valid = true;
+		break;
+	case PM_POST_SUSPEND:
+		if (!timer->suspend_time_valid)
+			break;
+		timer->suspend_time_valid = false;
+
+		spin_lock_bh(&timestamp_lock);
+		if (!timer->active) {
+			spin_unlock_bh(&timestamp_lock);
+			break;
+		}
+		/* since jiffies are not updated when suspended now represents
+		 * the time it would have suspended */
+		if (time_after(timer->timer.expires, now)) {
+			ts = ktime_to_timespec64(ktime_get_boottime());
+			ts = timespec64_sub(ts, timer->last_suspend_time);
+			time_diff = timespec64_to_jiffies(&ts);
+			if (timer->timer.expires > (time_diff + now)) {
+				mod_timer_pending(&timer->timer,
+						(timer->timer.expires - time_diff));
+			} else {
+				del_timer(&timer->timer);
+				timer->timer.expires = 0;
+				timer->active = false;
+				timer->work_pending = true;
+				schedule_work(&timer->work);
+			}
+		}
+		spin_unlock_bh(&timestamp_lock);
+		break;
+	default:
+		break;
+	}
+	return NOTIFY_DONE;
 }
 
 static enum alarmtimer_restart idletimer_tg_alarmproc(struct alarm *alarm,
@@ -166,6 +336,22 @@ static int idletimer_tg_create(struct id
 
 	timer_setup(&info->timer->timer, idletimer_tg_expired, 0);
 	info->timer->refcnt = 1;
+	info->timer->send_nl_msg = (info->send_nl_msg == 0) ? false : true;
+	info->timer->active = true;
+	info->timer->timeout = info->timeout;
+
+	info->timer->delayed_timer_trigger.tv_sec = 0;
+	info->timer->delayed_timer_trigger.tv_nsec = 0;
+	info->timer->work_pending = false;
+	info->timer->uid = 0;
+	info->timer->last_modified_timer =
+		ktime_to_timespec64(ktime_get_boottime());
+
+	info->timer->pm_nb.notifier_call = idletimer_resume;
+	ret = register_pm_notifier(&info->timer->pm_nb);
+	if (ret)
+		printk(KERN_WARNING "[%s] Failed to register pm notifier %d\n",
+				__func__, ret);
 
 	INIT_WORK(&info->timer->work, idletimer_tg_work);
 
@@ -182,6 +368,42 @@ out:
 	return ret;
 }
 
+static void reset_timer(const struct idletimer_tg_info *info,
+			struct sk_buff *skb)
+{
+	unsigned long now = jiffies;
+	struct idletimer_tg *timer = info->timer;
+	bool timer_prev;
+
+	spin_lock_bh(&timestamp_lock);
+	timer_prev = timer->active;
+	timer->active = true;
+	/* timer_prev is used to guard overflow problem in time_before*/
+	if (!timer_prev || time_before(timer->timer.expires, now)) {
+		pr_debug("Starting Checkentry timer (Expired, Jiffies): %lu, %lu\n",
+				timer->timer.expires, now);
+
+		/* Stores the uid resposible for waking up the radio */
+		if (skb && (skb->sk)) {
+			timer->uid = from_kuid_munged(current_user_ns(),
+					sock_i_uid(skb_to_full_sk(skb)));
+		}
+
+		/* checks if there is a pending inactive notification*/
+		if (timer->work_pending)
+			timer->delayed_timer_trigger = timer->last_modified_timer;
+		else {
+			timer->work_pending = true;
+			schedule_work(&timer->work);
+		}
+	}
+
+	timer->last_modified_timer = ktime_to_timespec64(ktime_get_boottime());
+	mod_timer(&timer->timer,
+			msecs_to_jiffies(info->timeout * 1000) + now);
+	spin_unlock_bh(&timestamp_lock);
+}
+
 static int idletimer_tg_create_v1(struct idletimer_tg_info_v1 *info)
 {
 	int ret;
@@ -251,13 +473,23 @@ static unsigned int idletimer_tg_target(
 					 const struct xt_action_param *par)
 {
 	const struct idletimer_tg_info *info = par->targinfo;
+	unsigned long now = jiffies;
 
 	pr_debug("resetting timer %s, timeout period %u\n",
 		 info->label, info->timeout);
 
-	mod_timer(&info->timer->timer,
-		  msecs_to_jiffies(info->timeout * 1000) + jiffies);
+	BUG_ON(!info->timer);
+
+	info->timer->active = true;
 
+	if (time_before(info->timer->timer.expires, now)) {
+		schedule_work(&info->timer->work);
+		pr_debug("Starting timer %s (Expired, Jiffies): %lu, %lu\n",
+			 info->label, info->timer->timer.expires, now);
+	}
+
+	/* TODO: Avoid modifying timers on each packet */
+	reset_timer(info, skb);
 	return XT_CONTINUE;
 }
 
@@ -321,9 +553,7 @@ static int idletimer_tg_checkentry(const
 	info->timer = __idletimer_tg_find_by_label(info->label);
 	if (info->timer) {
 		info->timer->refcnt++;
-		mod_timer(&info->timer->timer,
-			  msecs_to_jiffies(info->timeout * 1000) + jiffies);
-
+		reset_timer(info, NULL);
 		pr_debug("increased refcnt of timer %s to %u\n",
 			 info->label, info->timer->refcnt);
 	} else {
@@ -336,6 +566,7 @@ static int idletimer_tg_checkentry(const
 	}
 
 	mutex_unlock(&list_mutex);
+
 	return 0;
 }
 
@@ -414,13 +645,14 @@ static void idletimer_tg_destroy(const s
 
 		list_del(&info->timer->entry);
 		del_timer_sync(&info->timer->timer);
-		cancel_work_sync(&info->timer->work);
 		sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr);
+		unregister_pm_notifier(&info->timer->pm_nb);
+		cancel_work_sync(&info->timer->work);
 		kfree(info->timer->attr.attr.name);
 		kfree(info->timer);
 	} else {
 		pr_debug("decreased refcnt of timer %s to %u\n",
-			 info->label, info->timer->refcnt);
+		info->label, info->timer->refcnt);
 	}
 
 	mutex_unlock(&list_mutex);
@@ -459,6 +691,7 @@ static void idletimer_tg_destroy_v1(cons
 static struct xt_target idletimer_tg[] __read_mostly = {
 	{
 	.name		= "IDLETIMER",
+	.revision	= 1,
 	.family		= NFPROTO_UNSPEC,
 	.target		= idletimer_tg_target,
 	.targetsize     = sizeof(struct idletimer_tg_info),
@@ -540,3 +773,4 @@ MODULE_DESCRIPTION("Xtables: idle time m
 MODULE_LICENSE("GPL v2");
 MODULE_ALIAS("ipt_IDLETIMER");
 MODULE_ALIAS("ip6t_IDLETIMER");
+MODULE_ALIAS("arpt_IDLETIMER");
Index: rpi4-kernel/include/linux/pkglist.h
===================================================================
--- /dev/null
+++ rpi4-kernel/include/linux/pkglist.h
@@ -0,0 +1,38 @@
+#ifndef _PKGLIST_H_
+#define _PKGLIST_H_
+
+#include <linux/dcache.h>
+#include <linux/uidgid.h>
+
+#define QSTR_LITERAL(string) QSTR_INIT(string, sizeof(string)-1)
+
+static inline bool str_case_eq(const char *s1, const char *s2)
+{
+	return !strcasecmp(s1, s2);
+}
+
+static inline bool str_n_case_eq(const char *s1, const char *s2, size_t len)
+{
+	return !strncasecmp(s1, s2, len);
+}
+
+static inline bool qstr_case_eq(const struct qstr *q1, const struct qstr *q2)
+{
+	return q1->len == q2->len && str_case_eq(q1->name, q2->name);
+}
+
+#define BY_NAME		BIT(0)
+#define BY_USERID	BIT(1)
+
+struct pkg_list {
+	struct list_head list;
+	void (*update)(int flags, const struct qstr *name, uint32_t userid);
+};
+
+kuid_t pkglist_get_appid(const char *key);
+kgid_t pkglist_get_ext_gid(const char *key);
+bool pkglist_user_is_excluded(const char *key, uint32_t user);
+kuid_t pkglist_get_allowed_appid(const char *key, uint32_t user);
+void pkglist_register_update_listener(struct pkg_list *pkg);
+void pkglist_unregister_update_listener(struct pkg_list *pkg);
+#endif
Index: rpi4-kernel/include/uapi/linux/magic.h
===================================================================
--- rpi4-kernel.orig/include/uapi/linux/magic.h
+++ rpi4-kernel/include/uapi/linux/magic.h
@@ -58,6 +58,8 @@
 #define REISER2FS_SUPER_MAGIC_STRING	"ReIsEr2Fs"
 #define REISER2FS_JR_SUPER_MAGIC_STRING	"ReIsEr3Fs"
 
+#define ESDFS_SUPER_MAGIC 0x00035df5
+
 #define SMB_SUPER_MAGIC		0x517B
 #define CGROUP_SUPER_MAGIC	0x27e0eb
 #define CGROUP2_SUPER_MAGIC	0x63677270
Index: rpi4-kernel/kernel/sysctl.c
===================================================================
--- rpi4-kernel.orig/kernel/sysctl.c
+++ rpi4-kernel/kernel/sysctl.c
@@ -3069,6 +3069,15 @@ static struct ctl_table vm_table[] = {
 		.mode		= 0644,
 		.proc_handler	= mmap_min_addr_handler,
 	},
+  {
+    .procname = "mmap_noexec_taint",
+    .data   = &sysctl_mmap_noexec_taint,
+    .maxlen   = sizeof(sysctl_mmap_noexec_taint),
+    .mode   = 0644,
+    .proc_handler = proc_dointvec_minmax,
+    .extra1   = SYSCTL_ZERO,
+    .extra2   = SYSCTL_ONE,
+  },
 #endif
 #ifdef CONFIG_NUMA
 	{
@@ -3140,6 +3149,13 @@ static struct ctl_table vm_table[] = {
 		.mode		= 0644,
 		.proc_handler	= proc_doulongvec_minmax,
 	},
+  {
+    .procname = "min_filelist_kbytes",
+    .data   = &min_filelist_kbytes,
+    .maxlen   = sizeof(min_filelist_kbytes),
+    .mode   = 0644,
+    .proc_handler = min_filelist_kbytes_handler,
+  },
 #ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS
 	{
 		.procname	= "mmap_rnd_bits",
Index: rpi4-kernel/mm/Kconfig
===================================================================
--- rpi4-kernel.orig/mm/Kconfig
+++ rpi4-kernel/mm/Kconfig
@@ -321,6 +321,23 @@ config DEFAULT_MMAP_MIN_ADDR
 	  This value can be changed after boot using the
 	  /proc/sys/vm/mmap_min_addr tunable.
 
+config MMAP_NOEXEC_TAINT
+  int "Turns on tainting of mmap()d files from noexec mountpoints"
+  default 1 if MMU
+  default 0 if !MMU
+  help
+    By default, the ability to change the protections of a virtual
+    memory area to allow execution depend on if the vma has the
+    VM_MAYEXEC flag.  When mapping regions from files, VM_MAYEXEC
+    will be unset if the containing mountpoint is mounted MNT_NOEXEC.
+    By setting the value to 0, any mmap()d region may be later
+    mprotect()d with PROT_EXEC.
+
+    If unsure, keep the value set to 1.
+
+    This value can be changed after boot using the
+    /proc/sys/vm/mmap_noexec_taint tunable.
+
 config ARCH_SUPPORTS_MEMORY_FAILURE
 	bool
 
@@ -899,4 +916,15 @@ config SECRETMEM
 
 source "mm/damon/Kconfig"
 
+config LOW_MEM_NOTIFY
+  bool "Create device that lets processes detect low-memory conditions"
+  default n
+  help
+    A process can poll the /dev/low_mem device to be notified of
+    low-memory conditions.  The process can then attempt to free memory
+    before a OOM condition develops and the OOM killer takes over.  This
+    is meant to be used in systems with no or very little swap space.  In
+    the presence of large swap space, the system is likely to become
+    unusable before the OOM killer is triggered.
+
 endmenu
Index: rpi4-kernel/mm/util.c
===================================================================
--- rpi4-kernel.orig/mm/util.c
+++ rpi4-kernel/mm/util.c
@@ -848,6 +848,7 @@ int sysctl_overcommit_memory __read_most
 int sysctl_overcommit_ratio __read_mostly = 50;
 unsigned long sysctl_overcommit_kbytes __read_mostly;
 int sysctl_max_map_count __read_mostly = DEFAULT_MAX_MAP_COUNT;
+int sysctl_mmap_noexec_taint __read_mostly = CONFIG_MMAP_NOEXEC_TAINT;
 unsigned long sysctl_user_reserve_kbytes __read_mostly = 1UL << 17; /* 128MB */
 unsigned long sysctl_admin_reserve_kbytes __read_mostly = 1UL << 13; /* 8MB */
 
Index: rpi4-kernel/mm/low-mem-notify.c
===================================================================
--- /dev/null
+++ rpi4-kernel/mm/low-mem-notify.c
@@ -0,0 +1,398 @@
+/*
+ * mm/low-mem-notify.c
+ *
+ * Sends low-memory notifications to processes via /dev/low-mem.
+ *
+ * Copyright (C) 2012 The Chromium OS Authors
+ * This program is free software, released under the GPL.
+ * Based on a proposal by Minchan Kim
+ *
+ * A process that polls /dev/low-mem is notified of a low-memory situation.
+ * The intent is to allow the process to free some memory before the OOM killer
+ * is invoked.
+ *
+ * A low-memory condition is estimated by subtracting anonymous memory
+ * (i.e. process data segments), kernel memory, and a fixed amount of
+ * file-backed memory from total memory.  This is just a heuristic, as in
+ * general we don't know how much memory can be reclaimed before we try to
+ * reclaim it, and that's too expensive or too late.
+ *
+ * This is tailored to Chromium OS, where a single program (the browser)
+ * controls most of the memory, and (currently) no swap space is used.
+ */
+
+
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/poll.h>
+#include <linux/slab.h>
+#include <linux/mm.h>
+#include <linux/ctype.h>
+#include <linux/ratelimit.h>
+#include <linux/stddef.h>
+#include <linux/swap.h>
+#include <linux/mm_inline.h>
+
+#define MB (1 << 20)
+
+static DECLARE_WAIT_QUEUE_HEAD(low_mem_wait);
+static atomic_t low_mem_state = ATOMIC_INIT(0);
+
+/* We support up to this many different thresholds. */
+#define LOW_MEM_THRESHOLD_MAX 5
+
+/* This is a list of thresholds in pages and should be in ascending order. */
+static unsigned long low_mem_thresholds[LOW_MEM_THRESHOLD_MAX] = {
+	50 * MB / PAGE_SIZE
+};
+static unsigned int low_mem_threshold_count = 1;
+
+static bool low_mem_margin_enabled = true;
+static unsigned int low_mem_ram_vs_swap_weight = 4;
+
+void low_mem_notify(void)
+{
+	atomic_set(&low_mem_state, true);
+	wake_up(&low_mem_wait);
+}
+
+/*
+ * Compute available memory used by files that can be reclaimed quickly.
+ */
+static unsigned long get_available_file_mem(void)
+{
+	unsigned long file_mem =
+			global_node_page_state(NR_ACTIVE_FILE) +
+			global_node_page_state(NR_INACTIVE_FILE);
+	unsigned long dirty_mem = global_node_page_state(NR_FILE_DIRTY);
+	unsigned long min_file_mem = lru_gen_enabled() ?
+				     0 : min_filelist_kbytes >> (PAGE_SHIFT - 10);
+	unsigned long clean_file_mem = file_mem > dirty_mem ?
+			file_mem - dirty_mem : 0;
+	/* Conservatively estimate the amount of available_file_mem */
+	unsigned long available_file_mem = clean_file_mem > min_file_mem ?
+			clean_file_mem - min_file_mem : 0;
+	return available_file_mem;
+}
+
+/*
+ * Available anonymous memory.
+ */
+static unsigned long get_available_anon_mem(void)
+{
+	return global_node_page_state(NR_ACTIVE_ANON) +
+		global_node_page_state(NR_INACTIVE_ANON);
+}
+
+/*
+ * Compute "available" memory, that is either free memory or memory that can be
+ * reclaimed quickly, adjusted for the presence of swap.
+ */
+static unsigned long get_available_mem_adj(void)
+{
+	/* free_mem is completely unallocated; clean file-backed memory
+	 * (file_mem - dirty_mem) is easy to reclaim, except for the last
+	 * min_filelist_kbytes. totalreserve_pages is the reserve of pages that
+	 * are not available to user space.
+	 */
+	unsigned long raw_free_mem = global_zone_page_state(NR_FREE_PAGES);
+	unsigned long free_mem = raw_free_mem > totalreserve_pages ?
+			raw_free_mem - totalreserve_pages : 0;
+	unsigned long available_mem = free_mem + get_available_file_mem();
+	unsigned long swappable_pages = min_t(unsigned long,
+			get_nr_swap_pages(), get_available_anon_mem());
+	/*
+	 * The contribution of swap is reduced by a factor of
+	 * low_mem_ram_vs_swap_weight.
+	 */
+	return available_mem + swappable_pages / low_mem_ram_vs_swap_weight;
+}
+
+#ifdef CONFIG_SYSFS
+static void low_mem_threshold_notify(void);
+#else
+static void low_mem_threshold_notify(void)
+{
+}
+#endif
+
+/*
+ * Returns TRUE if we are in a low memory state.
+ */
+bool low_mem_check(void)
+{
+	static bool was_low_mem;	/* = false, as per style guide */
+	static atomic_t in_low_mem_check = ATOMIC_INIT(0);
+	/* last observed threshold */
+	static unsigned int low_mem_threshold_last = UINT_MAX;
+	/* Limit logging low memory to once per second. */
+	static DEFINE_RATELIMIT_STATE(low_mem_logging_ratelimit, 1 * HZ, 1);
+
+	/* We declare a low-memory condition when a combination of RAM and swap
+	 * space is low.
+	 */
+	unsigned long available_mem = get_available_mem_adj();
+	/*
+	 * For backwards compatibility with the older margin interface, we will
+	 * trigger the /dev/chromeos-low_mem device when we are below the
+	 * lowest threshold
+	 */
+	bool is_low_mem = available_mem < low_mem_thresholds[0];
+	unsigned int threshold_lowest = UINT_MAX;
+	int i;
+
+	if (!low_mem_margin_enabled)
+		return false;
+
+	if (atomic_read(&in_low_mem_check) || atomic_xchg(&in_low_mem_check, 1))
+		return was_low_mem;
+
+	if (unlikely(is_low_mem && !was_low_mem) &&
+	    __ratelimit(&low_mem_logging_ratelimit)) {
+		pr_info("entering low_mem (avail RAM = %lu kB, avail swap %lu kB, avail file %lu kB, anon mem: %lu kB)\n",
+			available_mem * PAGE_SIZE / 1024,
+			get_nr_swap_pages() * PAGE_SIZE / 1024,
+			get_available_file_mem() * PAGE_SIZE / 1024,
+			get_available_anon_mem() * PAGE_SIZE / 1024);
+	}
+	was_low_mem = is_low_mem;
+
+	if (is_low_mem)
+		low_mem_notify();
+
+	for (i = 0; i < low_mem_threshold_count; i++) {
+		if (available_mem < low_mem_thresholds[i]) {
+			threshold_lowest = i;
+			break;
+		}
+	}
+
+	/* we crossed one or more thresholds */
+	if (unlikely(threshold_lowest < low_mem_threshold_last))
+		low_mem_threshold_notify();
+
+	low_mem_threshold_last = threshold_lowest;
+
+	atomic_set(&in_low_mem_check, 0);
+
+	return is_low_mem;
+}
+
+static int low_mem_notify_open(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static int low_mem_notify_release(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static __poll_t low_mem_notify_poll(struct file *file, poll_table *wait)
+{
+	/* Update state to reflect any recent freeing. */
+	atomic_set(&low_mem_state, low_mem_check());
+
+	poll_wait(file, &low_mem_wait, wait);
+
+	if (low_mem_margin_enabled && atomic_read(&low_mem_state))
+		return POLLIN;
+
+	return 0;
+}
+
+const struct file_operations low_mem_notify_fops = {
+	.open = low_mem_notify_open,
+	.release = low_mem_notify_release,
+	.poll = low_mem_notify_poll,
+};
+EXPORT_SYMBOL(low_mem_notify_fops);
+
+#ifdef CONFIG_SYSFS
+
+#define LOW_MEM_ATTR(_name)				      \
+	static struct kobj_attribute low_mem_##_name##_attr = \
+		__ATTR(_name, 0644, low_mem_##_name##_show,   \
+		       low_mem_##_name##_store)
+
+static ssize_t low_mem_margin_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf)
+{
+	int i;
+	ssize_t written = 0;
+
+	if (!low_mem_margin_enabled || !low_mem_threshold_count)
+		return sprintf(buf, "off\n");
+
+	for (i = 0; i < low_mem_threshold_count; i++)
+		written += sprintf(buf + written, "%lu ",
+			    low_mem_thresholds[i] * PAGE_SIZE / MB);
+	written += sprintf(buf + written, "\n");
+	return written;
+}
+
+static ssize_t low_mem_margin_store(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buf, size_t count)
+{
+	int i = 0, consumed = 0;
+	const char *start = buf;
+	char *endp;
+	unsigned long thresholds[LOW_MEM_THRESHOLD_MAX];
+
+	memset(thresholds, 0, sizeof(thresholds));
+	/*
+	 * Even though the API does not say anything about this, the string in
+	 * buf is zero-terminated (as long as count < PAGE_SIZE) because buf is
+	 * a newly allocated zero-filled page.  Most other sysfs handlers rely
+	 * on this too.
+	 */
+	if (strncmp("off", buf, 3) == 0) {
+		pr_info("low_mem: disabling notifier\n");
+		low_mem_margin_enabled = false;
+		return count;
+	}
+	if (strncmp("on", buf, 2) == 0) {
+		pr_info("low_mem: enabling notifier\n");
+		low_mem_margin_enabled = true;
+		return count;
+	}
+	/*
+	 * This takes a space separated list of thresholds in ascending order,
+	 * and a trailing newline is optional.
+	 */
+	while (consumed < count) {
+		if (i >= LOW_MEM_THRESHOLD_MAX) {
+			pr_warn("low-mem: too many thresholds");
+			return -EINVAL;
+		}
+		/* special case for trailing newline */
+		if (*start == '\n')
+			break;
+
+		thresholds[i] = simple_strtoul(start, &endp, 0);
+		if ((endp == start) && *endp != '\n')
+			return -EINVAL;
+
+		/* make sure each is larger than the last one */
+		if (i && thresholds[i] <= thresholds[i - 1]) {
+			pr_warn("low-mem: thresholds not in increasing order: %lu then %lu\n",
+				thresholds[i - 1], thresholds[i]);
+			return -EINVAL;
+		}
+
+		if (thresholds[i] * (MB / PAGE_SIZE) > totalram_pages()) {
+			pr_warn("low-mem: threshold too high\n");
+			return -EINVAL;
+		}
+
+		consumed += endp - start + 1;
+		start = endp + 1;
+		i++;
+	}
+
+	low_mem_threshold_count = i;
+	low_mem_margin_enabled = !!low_mem_threshold_count;
+
+	/* Convert to pages outside the allocator fast path. */
+	for (i = 0; i < low_mem_threshold_count; i++) {
+		low_mem_thresholds[i] = thresholds[i] * (MB / PAGE_SIZE);
+		pr_info("low_mem: threshold[%d] %lu MB\n", i,
+			low_mem_thresholds[i] * PAGE_SIZE / MB);
+	}
+
+	return count;
+}
+LOW_MEM_ATTR(margin);
+
+static ssize_t low_mem_ram_vs_swap_weight_show(struct kobject *kobj,
+					       struct kobj_attribute *attr,
+					       char *buf)
+{
+	return sprintf(buf, "%u\n", low_mem_ram_vs_swap_weight);
+}
+
+static ssize_t low_mem_ram_vs_swap_weight_store(struct kobject *kobj,
+						struct kobj_attribute *attr,
+						const char *buf, size_t count)
+{
+	int err;
+	unsigned weight;
+
+	err = kstrtouint(buf, 10, &weight);
+	if (err)
+		return -EINVAL;
+
+	/* The special value 0 represents infinity. */
+	low_mem_ram_vs_swap_weight = !weight ? -1 : weight;
+	pr_info("low_mem: setting ram weight to %u\n",
+		low_mem_ram_vs_swap_weight);
+	return count;
+}
+LOW_MEM_ATTR(ram_vs_swap_weight);
+
+static ssize_t low_mem_available_show(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      char *buf)
+{
+	unsigned long available_mem = get_available_mem_adj();
+
+	return sprintf(buf, "%lu\n",
+		       available_mem / (MB / PAGE_SIZE));
+}
+
+static ssize_t low_mem_available_store(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       const char *buf, size_t count)
+{
+	return -EINVAL;
+}
+LOW_MEM_ATTR(available);
+
+static struct attribute *low_mem_attrs[] = {
+	&low_mem_margin_attr.attr,
+	&low_mem_ram_vs_swap_weight_attr.attr,
+	&low_mem_available_attr.attr,
+	NULL,
+};
+
+static struct attribute_group low_mem_attr_group = {
+	.attrs = low_mem_attrs,
+	.name = "chromeos-low_mem",
+};
+
+static struct kernfs_node *low_mem_available_dirent;
+
+static void low_mem_threshold_notify(void)
+{
+	if (low_mem_available_dirent)
+		sysfs_notify_dirent(low_mem_available_dirent);
+}
+
+static int __init low_mem_init(void)
+{
+	int err;
+	struct kernfs_node *low_mem_node;
+
+	err = sysfs_create_group(mm_kobj, &low_mem_attr_group);
+	if (err) {
+		pr_err("low_mem: register sysfs failed\n");
+		return err;
+	}
+
+	low_mem_node = sysfs_get_dirent(mm_kobj->sd, "chromeos-low_mem");
+	if (low_mem_node) {
+		low_mem_available_dirent =
+		    sysfs_get_dirent(low_mem_node, "available");
+		sysfs_put(low_mem_node);
+	}
+
+	if (!low_mem_available_dirent)
+		pr_warn("unable to find dirent for \"available\" attribute\n");
+
+	return 0;
+}
+module_init(low_mem_init)
+
+#endif
Index: rpi4-kernel/mm/Makefile
===================================================================
--- rpi4-kernel.orig/mm/Makefile
+++ rpi4-kernel/mm/Makefile
@@ -117,6 +117,7 @@ obj-$(CONFIG_SECRETMEM) += secretmem.o
 obj-$(CONFIG_CMA_SYSFS) += cma_sysfs.o
 obj-$(CONFIG_USERFAULTFD) += userfaultfd.o
 obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o
+obj-$(CONFIG_LOW_MEM_NOTIFY) += low-mem-notify.o
 obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o
 obj-$(CONFIG_DAMON) += damon/
 obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
Index: rpi4-kernel/fs/cifs/xattr.c
===================================================================
--- rpi4-kernel.orig/fs/cifs/xattr.c
+++ rpi4-kernel/fs/cifs/xattr.c
@@ -281,7 +281,7 @@ static int cifs_creation_time_get(struct
 
 static int cifs_xattr_get(const struct xattr_handler *handler,
 			  struct dentry *dentry, struct inode *inode,
-			  const char *name, void *value, size_t size)
+			  const char *name, void *value, size_t size, int flags)
 {
 	ssize_t rc = -EOPNOTSUPP;
 	unsigned int xid;
Index: rpi4-kernel/fs/overlayfs/copy_up.c
===================================================================
--- rpi4-kernel.orig/fs/overlayfs/copy_up.c
+++ rpi4-kernel/fs/overlayfs/copy_up.c
@@ -1046,7 +1046,7 @@ static int ovl_copy_up_flags(struct dent
 		dput(parent);
 		dput(next);
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 
 	return err;
 }
Index: rpi4-kernel/fs/overlayfs/dir.c
===================================================================
--- rpi4-kernel.orig/fs/overlayfs/dir.c
+++ rpi4-kernel/fs/overlayfs/dir.c
@@ -137,7 +137,8 @@ kill_whiteout:
 	goto out;
 }
 
-int ovl_mkdir_real(struct inode *dir, struct dentry **newdentry, umode_t mode)
+static int ovl_mkdir_real(struct inode *dir, struct dentry **newdentry,
+			  umode_t mode)
 {
 	int err;
 	struct dentry *d, *dentry = *newdentry;
@@ -569,7 +570,7 @@ static int ovl_create_or_link(struct den
 			      struct ovl_cattr *attr, bool origin)
 {
 	int err;
-	const struct cred *old_cred;
+	const struct cred *old_cred, *hold_cred = NULL;
 	struct cred *override_cred;
 	struct dentry *parent = dentry->d_parent;
 
@@ -589,9 +590,9 @@ static int ovl_create_or_link(struct den
 			goto out_revert_creds;
 	}
 
+	err = -ENOMEM;
+	override_cred = prepare_creds();
 	if (!attr->hardlink) {
-		err = -ENOMEM;
-		override_cred = prepare_creds();
 		if (!override_cred)
 			goto out_revert_creds;
 		/*
@@ -610,13 +611,13 @@ static int ovl_create_or_link(struct den
 		override_cred->fsuid = inode->i_uid;
 		override_cred->fsgid = inode->i_gid;
 		err = security_dentry_create_files_as(dentry,
-				attr->mode, &dentry->d_name, old_cred,
+				attr->mode, &dentry->d_name, old_cred ? old_cred : current_cred(),
 				override_cred);
 		if (err) {
 			put_cred(override_cred);
 			goto out_revert_creds;
 		}
-		put_cred(override_creds(override_cred));
+		hold_cred = override_creds(override_cred);
 		put_cred(override_cred);
 	}
 
@@ -624,9 +625,10 @@ static int ovl_create_or_link(struct den
 		err = ovl_create_upper(dentry, inode, attr);
 	else
 		err = ovl_create_over_whiteout(dentry, inode, attr);
-
 out_revert_creds:
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred ?: hold_cred);
+	if (old_cred && hold_cred)
+		put_cred(hold_cred);
 	return err;
 }
 
@@ -703,7 +705,7 @@ static int ovl_set_link_redirect(struct
 
 	old_cred = ovl_override_creds(dentry->d_sb);
 	err = ovl_set_redirect(dentry, false);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 
 	return err;
 }
@@ -921,7 +923,7 @@ static int ovl_do_remove(struct dentry *
 		err = ovl_remove_upper(dentry, is_dir, &list);
 	else
 		err = ovl_remove_and_whiteout(dentry, &list);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	if (!err) {
 		if (is_dir)
 			clear_nlink(dentry->d_inode);
@@ -1230,13 +1232,9 @@ static int ovl_rename(struct user_namesp
 				goto out_dput;
 		}
 	} else {
-		if (!d_is_negative(newdentry)) {
-			if (!new_opaque || !ovl_is_whiteout(newdentry))
-				goto out_dput;
-		} else {
-			if (flags & RENAME_EXCHANGE)
-				goto out_dput;
-		}
+		if (!d_is_negative(newdentry) &&
+		    (!new_opaque || !ovl_is_whiteout(newdentry)))
+			goto out_dput;
 	}
 
 	if (olddentry == trap)
@@ -1295,7 +1293,7 @@ out_dput_old:
 out_unlock:
 	unlock_rename(new_upperdir, old_upperdir);
 out_revert_creds:
-	revert_creds(old_cred);
+	ovl_revert_creds(old->d_sb, old_cred);
 	if (update_nlink)
 		ovl_nlink_end(new);
 out_drop_write:
Index: rpi4-kernel/fs/overlayfs/file.c
===================================================================
--- rpi4-kernel.orig/fs/overlayfs/file.c
+++ rpi4-kernel/fs/overlayfs/file.c
@@ -17,7 +17,6 @@
 
 struct ovl_aio_req {
 	struct kiocb iocb;
-	refcount_t ref;
 	struct kiocb *orig_iocb;
 	struct fd fd;
 };
@@ -61,7 +60,7 @@ static struct file *ovl_open_realfile(co
 		realfile = open_with_fake_path(&file->f_path, flags, realinode,
 					       current_cred());
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 
 	pr_debug("open(%p[%pD2/%c], 0%o) -> (%p, 0%o)\n",
 		 file, file, ovl_whatisit(inode, realinode), file->f_flags,
@@ -205,7 +204,7 @@ static loff_t ovl_llseek(struct file *fi
 
 	old_cred = ovl_override_creds(inode->i_sb);
 	ret = vfs_llseek(real.file, offset, whence);
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 
 	file->f_pos = real.file->f_pos;
 	ovl_inode_unlock(inode);
@@ -253,14 +252,6 @@ static rwf_t ovl_iocb_to_rwf(int ifl)
 	return flags;
 }
 
-static inline void ovl_aio_put(struct ovl_aio_req *aio_req)
-{
-	if (refcount_dec_and_test(&aio_req->ref)) {
-		fdput(aio_req->fd);
-		kmem_cache_free(ovl_aio_request_cachep, aio_req);
-	}
-}
-
 static void ovl_aio_cleanup_handler(struct ovl_aio_req *aio_req)
 {
 	struct kiocb *iocb = &aio_req->iocb;
@@ -277,7 +268,8 @@ static void ovl_aio_cleanup_handler(stru
 	}
 
 	orig_iocb->ki_pos = iocb->ki_pos;
-	ovl_aio_put(aio_req);
+	fdput(aio_req->fd);
+	kmem_cache_free(ovl_aio_request_cachep, aio_req);
 }
 
 static void ovl_aio_rw_complete(struct kiocb *iocb, long res, long res2)
@@ -304,12 +296,6 @@ static ssize_t ovl_read_iter(struct kioc
 	if (ret)
 		return ret;
 
-	ret = -EINVAL;
-	if (iocb->ki_flags & IOCB_DIRECT &&
-	    (!real.file->f_mapping->a_ops ||
-	     !real.file->f_mapping->a_ops->direct_IO))
-		goto out_fdput;
-
 	old_cred = ovl_override_creds(file_inode(file)->i_sb);
 	if (is_sync_kiocb(iocb)) {
 		ret = vfs_iter_read(real.file, iter, &iocb->ki_pos,
@@ -327,16 +313,15 @@ static ssize_t ovl_read_iter(struct kioc
 		aio_req->orig_iocb = iocb;
 		kiocb_clone(&aio_req->iocb, iocb, real.file);
 		aio_req->iocb.ki_complete = ovl_aio_rw_complete;
-		refcount_set(&aio_req->ref, 2);
 		ret = vfs_iocb_iter_read(real.file, &aio_req->iocb, iter);
-		ovl_aio_put(aio_req);
 		if (ret != -EIOCBQUEUED)
 			ovl_aio_cleanup_handler(aio_req);
 	}
 out:
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file)->i_sb, old_cred);
+
 	ovl_file_accessed(file);
-out_fdput:
+
 	fdput(real);
 
 	return ret;
@@ -365,12 +350,6 @@ static ssize_t ovl_write_iter(struct kio
 	if (ret)
 		goto out_unlock;
 
-	ret = -EINVAL;
-	if (iocb->ki_flags & IOCB_DIRECT &&
-	    (!real.file->f_mapping->a_ops ||
-	     !real.file->f_mapping->a_ops->direct_IO))
-		goto out_fdput;
-
 	if (!ovl_should_sync(OVL_FS(inode->i_sb)))
 		ifl &= ~(IOCB_DSYNC | IOCB_SYNC);
 
@@ -400,15 +379,12 @@ static ssize_t ovl_write_iter(struct kio
 		kiocb_clone(&aio_req->iocb, iocb, real.file);
 		aio_req->iocb.ki_flags = ifl;
 		aio_req->iocb.ki_complete = ovl_aio_rw_complete;
-		refcount_set(&aio_req->ref, 2);
 		ret = vfs_iocb_iter_write(real.file, &aio_req->iocb, iter);
-		ovl_aio_put(aio_req);
 		if (ret != -EIOCBQUEUED)
 			ovl_aio_cleanup_handler(aio_req);
 	}
 out:
-	revert_creds(old_cred);
-out_fdput:
+	ovl_revert_creds(file_inode(file)->i_sb, old_cred);
 	fdput(real);
 
 out_unlock:
@@ -452,7 +428,7 @@ static ssize_t ovl_splice_write(struct p
 	file_end_write(real.file);
 	/* Update size */
 	ovl_copyattr(inode);
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 	fdput(real);
 
 out_unlock:
@@ -479,7 +455,7 @@ static int ovl_fsync(struct file *file,
 	if (file_inode(real.file) == ovl_inode_upper(file_inode(file))) {
 		old_cred = ovl_override_creds(file_inode(file)->i_sb);
 		ret = vfs_fsync_range(real.file, start, end, datasync);
-		revert_creds(old_cred);
+		ovl_revert_creds(file_inode(file)->i_sb, old_cred);
 	}
 
 	fdput(real);
@@ -503,7 +479,7 @@ static int ovl_mmap(struct file *file, s
 
 	old_cred = ovl_override_creds(file_inode(file)->i_sb);
 	ret = call_mmap(vma->vm_file, vma);
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file)->i_sb, old_cred);
 	ovl_file_accessed(file);
 
 	return ret;
@@ -529,7 +505,7 @@ static long ovl_fallocate(struct file *f
 
 	old_cred = ovl_override_creds(file_inode(file)->i_sb);
 	ret = vfs_fallocate(real.file, mode, offset, len);
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file)->i_sb, old_cred);
 
 	/* Update size */
 	ovl_copyattr(inode);
@@ -554,7 +530,7 @@ static int ovl_fadvise(struct file *file
 
 	old_cred = ovl_override_creds(file_inode(file)->i_sb);
 	ret = vfs_fadvise(real.file, offset, len, advice);
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file)->i_sb, old_cred);
 
 	fdput(real);
 
@@ -613,7 +589,7 @@ static loff_t ovl_copyfile(struct file *
 						flags);
 		break;
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file_out)->i_sb, old_cred);
 
 	/* Update size */
 	ovl_copyattr(inode_out);
Index: rpi4-kernel/fs/overlayfs/inode.c
===================================================================
--- rpi4-kernel.orig/fs/overlayfs/inode.c
+++ rpi4-kernel/fs/overlayfs/inode.c
@@ -78,7 +78,7 @@ int ovl_setattr(struct user_namespace *m
 		inode_lock(upperdentry->d_inode);
 		old_cred = ovl_override_creds(dentry->d_sb);
 		err = notify_change(&init_user_ns, upperdentry, attr, NULL);
-		revert_creds(old_cred);
+    ovl_revert_creds(dentry->d_sb, old_cred);
 		if (!err)
 			ovl_copyattr(dentry->d_inode);
 		inode_unlock(upperdentry->d_inode);
@@ -270,7 +270,7 @@ int ovl_getattr(struct user_namespace *m
 		stat->nlink = dentry->d_inode->i_nlink;
 
 out:
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 
 	return err;
 }
@@ -305,7 +305,7 @@ int ovl_permission(struct user_namespace
 		mask |= MAY_READ;
 	}
 	err = inode_permission(&init_user_ns, realinode, mask);
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 
 	return err;
 }
@@ -322,7 +322,7 @@ static const char *ovl_get_link(struct d
 
 	old_cred = ovl_override_creds(dentry->d_sb);
 	p = vfs_get_link(ovl_dentry_real(dentry), done);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	return p;
 }
 
@@ -374,7 +374,7 @@ int ovl_xattr_set(struct dentry *dentry,
 		WARN_ON(flags != XATTR_REPLACE);
 		err = vfs_removexattr(&init_user_ns, realdentry, name);
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 
 	/* copy c/mtime */
 	ovl_copyattr(inode);
@@ -396,7 +396,7 @@ int ovl_xattr_get(struct dentry *dentry,
 	old_cred = ovl_override_creds(dentry->d_sb);
 	res = __vfs_getxattr(&init_user_ns, realdentry, d_inode(realdentry), name,
           value, size, flags);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	return res;
 }
 
@@ -424,7 +424,7 @@ ssize_t ovl_listxattr(struct dentry *den
 
 	old_cred = ovl_override_creds(dentry->d_sb);
 	res = vfs_listxattr(realdentry, list, size);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	if (res <= 0 || size == 0)
 		return res;
 
@@ -462,7 +462,7 @@ struct posix_acl *ovl_get_acl(struct ino
 
 	old_cred = ovl_override_creds(inode->i_sb);
 	acl = get_acl(realinode, type);
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 
 	return acl;
 }
@@ -496,7 +496,7 @@ static int ovl_fiemap(struct inode *inod
 
 	old_cred = ovl_override_creds(inode->i_sb);
 	err = realinode->i_op->fiemap(realinode, fieinfo, start, len);
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 
 	return err;
 }
Index: rpi4-kernel/fs/overlayfs/namei.c
===================================================================
--- rpi4-kernel.orig/fs/overlayfs/namei.c
+++ rpi4-kernel/fs/overlayfs/namei.c
@@ -108,7 +108,8 @@ int ovl_check_fb_len(struct ovl_fb *fb,
 static struct ovl_fh *ovl_get_fh(struct ovl_fs *ofs, struct dentry *dentry,
 				 enum ovl_xattr ox)
 {
-	int res, err;
+	ssize_t res;
+	int err;
 	struct ovl_fh *fh = NULL;
 
 	res = ovl_do_getxattr(ofs, dentry, ox, NULL, 0);
@@ -143,10 +144,10 @@ out:
 	return NULL;
 
 fail:
-	pr_warn_ratelimited("failed to get origin (%i)\n", res);
+	pr_warn_ratelimited("failed to get origin (%zi)\n", res);
 	goto out;
 invalid:
-	pr_warn_ratelimited("invalid origin (%*phN)\n", res, fh);
+	pr_warn_ratelimited("invalid origin (%*phN)\n", (int)res, fh);
 	goto out;
 }
 
@@ -1106,7 +1107,7 @@ struct dentry *ovl_lookup(struct inode *
 	ovl_dentry_update_reval(dentry, upperdentry,
 			DCACHE_OP_REVALIDATE | DCACHE_OP_WEAK_REVALIDATE);
 
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	if (origin_path) {
 		dput(origin_path->dentry);
 		kfree(origin_path);
@@ -1133,7 +1134,7 @@ out_put_upper:
 	kfree(upperredirect);
 out:
 	kfree(d.redirect);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	return ERR_PTR(err);
 }
 
@@ -1185,7 +1186,7 @@ bool ovl_lower_positive(struct dentry *d
 			dput(this);
 		}
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 
 	return positive;
 }
Index: rpi4-kernel/fs/overlayfs/readdir.c
===================================================================
--- rpi4-kernel.orig/fs/overlayfs/readdir.c
+++ rpi4-kernel/fs/overlayfs/readdir.c
@@ -286,7 +286,7 @@ static int ovl_check_whiteouts(struct de
 		}
 		inode_unlock(dir->d_inode);
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(rdd->dentry->d_sb, old_cred);
 
 	return err;
 }
@@ -789,7 +789,7 @@ static int ovl_iterate(struct file *file
 	}
 	err = 0;
 out:
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	return err;
 }
 
@@ -841,7 +841,7 @@ static struct file *ovl_dir_open_realfil
 
 	old_cred = ovl_override_creds(file_inode(file)->i_sb);
 	res = ovl_path_open(realpath, O_RDONLY | (file->f_flags & O_LARGEFILE));
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file)->i_sb, old_cred);
 
 	return res;
 }
@@ -967,7 +967,7 @@ int ovl_check_empty_dir(struct dentry *d
 
 	old_cred = ovl_override_creds(dentry->d_sb);
 	err = ovl_dir_read_merged(dentry, list, &root);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	if (err)
 		return err;
 
Index: rpi4-kernel/net/core/dev_ioctl.c
===================================================================
--- rpi4-kernel.orig/net/core/dev_ioctl.c
+++ rpi4-kernel/net/core/dev_ioctl.c
@@ -577,7 +577,7 @@ int dev_ioctl(struct net *net, unsigned
 	case SIOCBRADDIF:
 	case SIOCBRDELIF:
 	case SIOCSHWTSTAMP:
-		if (!ns_capable(net->user_ns, CAP_NET_ADMIN))
+		if (!android_ns_capable(net, CAP_NET_ADMIN))
 			return -EPERM;
 		fallthrough;
 	case SIOCBONDSLAVEINFOQUERY:
Index: rpi4-kernel/net/core/sysctl_net_core.c
===================================================================
--- rpi4-kernel.orig/net/core/sysctl_net_core.c
+++ rpi4-kernel/net/core/sysctl_net_core.c
@@ -624,6 +624,8 @@ static __net_init int sysctl_core_net_in
 {
 	struct ctl_table *tbl;
 
+  net->core.sysctl_android_paranoid = 0;
+
 	tbl = netns_core_table;
 	if (!net_eq(net, &init_net)) {
 		tbl = kmemdup(tbl, sizeof(netns_core_table), GFP_KERNEL);
Index: rpi4-kernel/net/xfrm/xfrm_algo.c
===================================================================
--- rpi4-kernel.orig/net/xfrm/xfrm_algo.c
+++ rpi4-kernel/net/xfrm/xfrm_algo.c
@@ -237,7 +237,7 @@ static struct xfrm_algo_desc aalg_list[]
 
 	.uinfo = {
 		.auth = {
-			.icv_truncbits = 96,
+			.icv_truncbits = IS_ENABLED(CONFIG_ANDROID) ? 128 : 96,
 			.icv_fullbits = 256,
 		}
 	},
Index: rpi4-kernel/net/xfrm/xfrm_state.c
===================================================================
--- rpi4-kernel.orig/net/xfrm/xfrm_state.c
+++ rpi4-kernel/net/xfrm/xfrm_state.c
@@ -2441,21 +2441,23 @@ int xfrm_user_policy(struct sock *sk, in
 	if (IS_ERR(data))
 		return PTR_ERR(data);
 
-	if (in_compat_syscall()) {
-		struct xfrm_translator *xtr = xfrm_get_translator();
+  if (IS_ENABLED(CONFIG_XFRM_USER_COMPAT)) {
+    if (in_compat_syscall()) {
+    	struct xfrm_translator *xtr = xfrm_get_translator();
 
-		if (!xtr) {
-			kfree(data);
-			return -EOPNOTSUPP;
-		}
+    	if (!xtr) {
+    		kfree(data);
+    		return -EOPNOTSUPP;
+    	}
 
-		err = xtr->xlate_user_policy_sockptr(&data, optlen);
-		xfrm_put_translator(xtr);
-		if (err) {
-			kfree(data);
-			return err;
-		}
-	}
+    	err = xtr->xlate_user_policy_sockptr(&data, optlen);
+    	xfrm_put_translator(xtr);
+    	if (err) {
+    		kfree(data);
+    		return err;
+    	}
+    }
+  }
 
 	err = -EINVAL;
 	rcu_read_lock();
Index: rpi4-kernel/net/xfrm/xfrm_sysctl.c
===================================================================
--- rpi4-kernel.orig/net/xfrm/xfrm_sysctl.c
+++ rpi4-kernel/net/xfrm/xfrm_sysctl.c
@@ -56,8 +56,10 @@ int __net_init xfrm_sysctl_init(struct n
 	table[3].data = &net->xfrm.sysctl_acq_expires;
 
 	/* Don't export sysctls to unprivileged users */
-	if (net->user_ns != &init_user_ns)
-		table[0].procname = NULL;
+	if (net->user_ns != &init_user_ns) {
+		table[0] = table[3];
+    table[1].procname = NULL;
+  }
 
 	net->xfrm.sysctl_hdr = register_net_sysctl(net, "net/core", table);
 	if (!net->xfrm.sysctl_hdr)
Index: rpi4-kernel/net/xfrm/xfrm_user.c
===================================================================
--- rpi4-kernel.orig/net/xfrm/xfrm_user.c
+++ rpi4-kernel/net/xfrm/xfrm_user.c
@@ -2875,20 +2875,22 @@ static int xfrm_user_rcv_msg(struct sk_b
 	if (!netlink_net_capable(skb, CAP_NET_ADMIN))
 		return -EPERM;
 
-	if (in_compat_syscall()) {
-		struct xfrm_translator *xtr = xfrm_get_translator();
+  if (IS_ENABLED(CONFIG_XFRM_USER_COMPAT)) {
+  	if (in_compat_syscall()) {
+  		struct xfrm_translator *xtr = xfrm_get_translator();
 
-		if (!xtr)
-			return -EOPNOTSUPP;
+  		if (!xtr)
+  			return -EOPNOTSUPP;
 
-		nlh64 = xtr->rcv_msg_compat(nlh, link->nla_max,
-					    link->nla_pol, extack);
-		xfrm_put_translator(xtr);
-		if (IS_ERR(nlh64))
-			return PTR_ERR(nlh64);
-		if (nlh64)
-			nlh = nlh64;
-	}
+  		nlh64 = xtr->rcv_msg_compat(nlh, link->nla_max,
+  					    link->nla_pol, extack);
+  		xfrm_put_translator(xtr);
+  		if (IS_ERR(nlh64))
+  			return PTR_ERR(nlh64);
+  		if (nlh64)
+  			nlh = nlh64;
+  	}
+  }
 
 	if ((type == (XFRM_MSG_GETSA - XFRM_MSG_BASE) ||
 	     type == (XFRM_MSG_GETPOLICY - XFRM_MSG_BASE)) &&
Index: rpi4-kernel/kernel/cgroup/cgroup-v1.c
===================================================================
--- rpi4-kernel.orig/kernel/cgroup/cgroup-v1.c
+++ rpi4-kernel/kernel/cgroup/cgroup-v1.c
@@ -519,7 +519,8 @@ static ssize_t __cgroup1_procs_write(str
 	tcred = get_task_cred(task);
 	if (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&
 	    !uid_eq(cred->euid, tcred->uid) &&
-	    !uid_eq(cred->euid, tcred->suid))
+	    !uid_eq(cred->euid, tcred->suid) &&
+	    !ns_capable(tcred->user_ns, CAP_SYS_NICE))
 		ret = -EACCES;
 	put_cred(tcred);
 	if (ret)
Index: rpi4-kernel/kernel/cgroup/cgroup.c
===================================================================
--- rpi4-kernel.orig/kernel/cgroup/cgroup.c
+++ rpi4-kernel/kernel/cgroup/cgroup.c
@@ -1742,7 +1742,6 @@ int rebind_subsystems(struct cgroup_root
 	struct cgroup *dcgrp = &dst_root->cgrp;
 	struct cgroup_subsys *ss;
 	int ssid, i, ret;
-	u16 dfl_disable_ss_mask = 0;
 
 	lockdep_assert_held(&cgroup_mutex);
 
@@ -1759,28 +1758,8 @@ int rebind_subsystems(struct cgroup_root
 		/* can't move between two non-dummy roots either */
 		if (ss->root != &cgrp_dfl_root && dst_root != &cgrp_dfl_root)
 			return -EBUSY;
-
-		/*
-		 * Collect ssid's that need to be disabled from default
-		 * hierarchy.
-		 */
-		if (ss->root == &cgrp_dfl_root)
-			dfl_disable_ss_mask |= 1 << ssid;
-
 	} while_each_subsys_mask();
 
-	if (dfl_disable_ss_mask) {
-		struct cgroup *scgrp = &cgrp_dfl_root.cgrp;
-
-		/*
-		 * Controllers from default hierarchy that need to be rebound
-		 * are all disabled together in one go.
-		 */
-		cgrp_dfl_root.subsys_mask &= ~dfl_disable_ss_mask;
-		WARN_ON(cgroup_apply_control(scgrp));
-		cgroup_finalize_control(scgrp, 0);
-	}
-
 	do_each_subsys_mask(ss, ssid, ss_mask) {
 		struct cgroup_root *src_root = ss->root;
 		struct cgroup *scgrp = &src_root->cgrp;
@@ -1789,12 +1768,10 @@ int rebind_subsystems(struct cgroup_root
 
 		WARN_ON(!css || cgroup_css(dcgrp, ss));
 
-		if (src_root != &cgrp_dfl_root) {
-			/* disable from the source */
-			src_root->subsys_mask &= ~(1 << ssid);
-			WARN_ON(cgroup_apply_control(scgrp));
-			cgroup_finalize_control(scgrp, 0);
-		}
+		/* disable from the source */
+		src_root->subsys_mask &= ~(1 << ssid);
+		WARN_ON(cgroup_apply_control(scgrp));
+		cgroup_finalize_control(scgrp, 0);
 
 		/* rebind */
 		RCU_INIT_POINTER(scgrp->subsys[ssid], NULL);
@@ -2213,10 +2190,8 @@ static void cgroup_kill_sb(struct super_
 	 * And don't kill the default root.
 	 */
 	if (list_empty(&root->cgrp.self.children) && root != &cgrp_dfl_root &&
-	    !percpu_ref_is_dying(&root->cgrp.self.refcnt)) {
-		cgroup_bpf_offline(&root->cgrp);
+	    !percpu_ref_is_dying(&root->cgrp.self.refcnt))
 		percpu_ref_kill(&root->cgrp.self.refcnt);
-	}
 	cgroup_put(&root->cgrp);
 	kernfs_kill_sb(sb);
 }
@@ -5875,9 +5850,6 @@ int __init cgroup_init_early(void)
 	return 0;
 }
 
-static u16 cgroup_enable_mask __initdata;
-static int __init cgroup_disable(char *str);
-
 /**
  * cgroup_init - cgroup initialization
  *
@@ -5916,12 +5888,6 @@ int __init cgroup_init(void)
 
 	mutex_unlock(&cgroup_mutex);
 
-	/*
-	 * Apply an implicit disable, knowing that an explicit enable will
-	 * prevent if from doing anything.
-	 */
-	cgroup_disable("memory");
-
 	for_each_subsys(ss, ssid) {
 		if (ss->early_init) {
 			struct cgroup_subsys_state *css =
@@ -6512,10 +6478,6 @@ static int __init cgroup_disable(char *s
 			    strcmp(token, ss->legacy_name))
 				continue;
 
-			/* An explicit cgroup_enable overrides a disable */
-			if (cgroup_enable_mask & (1 << i))
-				continue;
-
 			static_branch_disable(cgroup_subsys_enabled_key[i]);
 			pr_info("Disabling %s control group subsystem\n",
 				ss->name);
@@ -6534,31 +6496,6 @@ static int __init cgroup_disable(char *s
 }
 __setup("cgroup_disable=", cgroup_disable);
 
-static int __init cgroup_enable(char *str)
-{
-	struct cgroup_subsys *ss;
-	char *token;
-	int i;
-
-	while ((token = strsep(&str, ",")) != NULL) {
-		if (!*token)
-			continue;
-
-		for_each_subsys(ss, i) {
-			if (strcmp(token, ss->name) &&
-			    strcmp(token, ss->legacy_name))
-				continue;
-
-			cgroup_enable_mask |= 1 << i;
-			static_branch_enable(cgroup_subsys_enabled_key[i]);
-			pr_info("Enabling %s control group subsystem\n",
-				ss->name);
-		}
-	}
-	return 1;
-}
-__setup("cgroup_enable=", cgroup_enable);
-
 void __init __weak enable_debug_cgroup(void) { }
 
 static int __init enable_cgroup_debug(char *str)
Index: rpi4-kernel/kernel/cgroup/rstat.c
===================================================================
--- rpi4-kernel.orig/kernel/cgroup/rstat.c
+++ rpi4-kernel/kernel/cgroup/rstat.c
@@ -433,6 +433,8 @@ static void root_cgroup_cputime(struct t
 		cputime->sum_exec_runtime += user;
 		cputime->sum_exec_runtime += sys;
 		cputime->sum_exec_runtime += cpustat[CPUTIME_STEAL];
+		cputime->sum_exec_runtime += cpustat[CPUTIME_GUEST];
+		cputime->sum_exec_runtime += cpustat[CPUTIME_GUEST_NICE];
 	}
 }
 
Index: rpi4-kernel/include/linux/cgroup.h
===================================================================
--- rpi4-kernel.orig/include/linux/cgroup.h
+++ rpi4-kernel/include/linux/cgroup.h
@@ -433,6 +433,18 @@ static inline void cgroup_put(struct cgr
 	css_put(&cgrp->self);
 }
 
+extern struct mutex cgroup_mutex;
+
+static inline void cgroup_lock(void)
+{
+	mutex_lock(&cgroup_mutex);
+}
+
+static inline void cgroup_unlock(void)
+{
+	mutex_unlock(&cgroup_mutex);
+}
+
 /**
  * task_css_set_check - obtain a task's css_set with extra access conditions
  * @task: the task to obtain css_set for
@@ -447,7 +459,6 @@ static inline void cgroup_put(struct cgr
  * as locks used during the cgroup_subsys::attach() methods.
  */
 #ifdef CONFIG_PROVE_RCU
-extern struct mutex cgroup_mutex;
 extern spinlock_t css_set_lock;
 #define task_css_set_check(task, __c)					\
 	rcu_dereference_check((task)->cgroups,				\
@@ -708,6 +719,8 @@ struct cgroup;
 static inline u64 cgroup_id(const struct cgroup *cgrp) { return 1; }
 static inline void css_get(struct cgroup_subsys_state *css) {}
 static inline void css_put(struct cgroup_subsys_state *css) {}
+static inline void cgroup_lock(void) {}
+static inline void cgroup_unlock(void) {}
 static inline int cgroup_attach_task_all(struct task_struct *from,
 					 struct task_struct *t) { return 0; }
 static inline int cgroupstats_build(struct cgroupstats *stats,
Index: rpi4-kernel/include/linux/xattr.h
===================================================================
--- rpi4-kernel.orig/include/linux/xattr.h
+++ rpi4-kernel/include/linux/xattr.h
@@ -34,7 +34,7 @@ struct xattr_handler {
 	bool (*list)(struct dentry *dentry);
 	int (*get)(const struct xattr_handler *, struct dentry *dentry,
 		   struct inode *inode, const char *name, void *buffer,
-		   size_t size);
+		   size_t size, int flags);
 	int (*set)(const struct xattr_handler *,
 		   struct user_namespace *mnt_userns, struct dentry *dentry,
 		   struct inode *inode, const char *name, const void *buffer,
Index: rpi4-kernel/security/security.c
===================================================================
--- rpi4-kernel.orig/security/security.c
+++ rpi4-kernel/security/security.c
@@ -749,27 +749,27 @@ static int lsm_superblock_alloc(struct s
 
 /* Security operations */
 
-int security_binder_set_context_mgr(struct task_struct *mgr)
+int security_binder_set_context_mgr(const struct cred *mgr)
 {
-	return call_int_hook(binder_set_context_mgr, 0, mgr);
+  return call_int_hook(binder_set_context_mgr, 0, mgr);
 }
 
-int security_binder_transaction(struct task_struct *from,
-				struct task_struct *to)
+int security_binder_transaction(const struct cred *from,
+        const struct cred *to)
 {
-	return call_int_hook(binder_transaction, 0, from, to);
+  return call_int_hook(binder_transaction, 0, from, to);
 }
 
-int security_binder_transfer_binder(struct task_struct *from,
-				    struct task_struct *to)
+int security_binder_transfer_binder(const struct cred *from,
+            const struct cred *to)
 {
-	return call_int_hook(binder_transfer_binder, 0, from, to);
+  return call_int_hook(binder_transfer_binder, 0, from, to);
 }
 
-int security_binder_transfer_file(struct task_struct *from,
-				  struct task_struct *to, struct file *file)
+int security_binder_transfer_file(const struct cred *from,
+          const struct cred *to, struct file *file)
 {
-	return call_int_hook(binder_transfer_file, 0, from, to, file);
+  return call_int_hook(binder_transfer_file, 0, from, to, file);
 }
 
 int security_ptrace_access_check(struct task_struct *child, unsigned int mode)
Index: rpi4-kernel/include/linux/lsm_hook_defs.h
===================================================================
--- rpi4-kernel.orig/include/linux/lsm_hook_defs.h
+++ rpi4-kernel/include/linux/lsm_hook_defs.h
@@ -26,13 +26,13 @@
  *   #undef LSM_HOOK
  * };
  */
-LSM_HOOK(int, 0, binder_set_context_mgr, struct task_struct *mgr)
-LSM_HOOK(int, 0, binder_transaction, struct task_struct *from,
-	 struct task_struct *to)
-LSM_HOOK(int, 0, binder_transfer_binder, struct task_struct *from,
-	 struct task_struct *to)
-LSM_HOOK(int, 0, binder_transfer_file, struct task_struct *from,
-	 struct task_struct *to, struct file *file)
+LSM_HOOK(int, 0, binder_set_context_mgr, const struct cred *mgr)
+LSM_HOOK(int, 0, binder_transaction, const struct cred *from,
+	 const struct cred *to)
+LSM_HOOK(int, 0, binder_transfer_binder, const struct cred *from,
+	 const struct cred *to)
+LSM_HOOK(int, 0, binder_transfer_file, const struct cred *from,
+	 const struct cred *to, struct file *file)
 LSM_HOOK(int, 0, ptrace_access_check, struct task_struct *child,
 	 unsigned int mode)
 LSM_HOOK(int, 0, ptrace_traceme, struct task_struct *parent)
Index: rpi4-kernel/security/commoncap.c
===================================================================
--- rpi4-kernel.orig/security/commoncap.c
+++ rpi4-kernel/security/commoncap.c
@@ -298,8 +298,8 @@ int cap_inode_need_killpriv(struct dentr
 	struct inode *inode = d_backing_inode(dentry);
 	int error;
 
-	error = __vfs_getxattr(dentry, inode, XATTR_NAME_CAPS, NULL, 0,
-            XATTR_NOSECURITY);
+	error = __vfs_getxattr(&init_user_ns, dentry, inode, XATTR_NAME_CAPS,
+			       NULL, 0, XATTR_NOSECURITY);
 	return error > 0;
 }
 
@@ -402,10 +402,8 @@ int cap_inode_getsecurity(struct user_na
 				      &tmpbuf, size, GFP_NOFS);
 	dput(dentry);
 
-	if (ret < 0 || !tmpbuf) {
-		size = ret;
-		goto out_free;
-	}
+	if (ret < 0 || !tmpbuf)
+		return ret;
 
 	fs_ns = inode->i_sb->s_user_ns;
 	cap = (struct vfs_cap_data *) tmpbuf;
@@ -666,9 +664,9 @@ int get_vfs_caps_from_disk(struct user_n
 		return -ENODATA;
 
 	fs_ns = inode->i_sb->s_user_ns;
-	size = __vfs_getxattr((struct dentry *)dentry, inode,
+	size = __vfs_getxattr(&init_user_ns, (struct dentry *)dentry, inode,
 			      XATTR_NAME_CAPS, &data, XATTR_CAPS_SZ,
-            XATTR_NOSECURITY);
+			      XATTR_NOSECURITY);
 	if (size == -ENODATA || size == -EOPNOTSUPP)
 		/* no data, that's ok */
 		return -ENODATA;
Index: rpi4-kernel/fs/ecryptfs/ecryptfs_kernel.h
===================================================================
--- rpi4-kernel.orig/fs/ecryptfs/ecryptfs_kernel.h
+++ rpi4-kernel/fs/ecryptfs/ecryptfs_kernel.h
@@ -662,6 +662,7 @@ int ecryptfs_read_lower_page_segment(str
 				     pgoff_t page_index,
 				     size_t offset_in_page, size_t size,
 				     struct inode *ecryptfs_inode);
+int ecryptfs_fsync_lower(struct inode *ecryptfs_inode, int datasync);
 struct page *ecryptfs_get_locked_page(struct inode *inode, loff_t index);
 int ecryptfs_parse_packet_length(unsigned char *data, size_t *size,
 				 size_t *length_size);
Index: rpi4-kernel/fs/ecryptfs/mmap.c
===================================================================
--- rpi4-kernel.orig/fs/ecryptfs/mmap.c
+++ rpi4-kernel/fs/ecryptfs/mmap.c
@@ -422,8 +422,9 @@ static int ecryptfs_write_inode_size_to_
 		goto out;
 	}
 	inode_lock(lower_inode);
-	size = __vfs_getxattr(lower_dentry, lower_inode, ECRYPTFS_XATTR_NAME,
-			      xattr_virt, PAGE_SIZE, XATTR_NOSECURITY);
+	size = __vfs_getxattr(&init_user_ns, lower_dentry, lower_inode,
+			      ECRYPTFS_XATTR_NAME, xattr_virt, PAGE_SIZE,
+			      XATTR_NOSECURITY);
 	if (size < 0)
 		size = 8;
 	put_unaligned_be64(i_size_read(ecryptfs_inode), xattr_virt);
Index: rpi4-kernel/mm/page_alloc.c
===================================================================
--- rpi4-kernel.orig/mm/page_alloc.c
+++ rpi4-kernel/mm/page_alloc.c
@@ -72,6 +72,7 @@
 #include <linux/padata.h>
 #include <linux/khugepaged.h>
 #include <linux/buffer_head.h>
+#include <linux/low-mem-notify.h>
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
 #include <asm/div64.h>
@@ -4820,6 +4821,7 @@ should_reclaim_retry(gfp_t gfp_mask, uns
 	 * several times in the row.
 	 */
 	if (*no_progress_loops > MAX_RECLAIM_RETRIES) {
+    low_mem_notify();
 		/* Before OOM, exhaust highatomic_reserve */
 		return unreserve_highatomic_pageblock(ac, true);
 	}
@@ -5433,6 +5435,7 @@ struct page *__alloc_pages(gfp_t gfp, un
 			&alloc_gfp, &alloc_flags))
 		return NULL;
 
+  low_mem_check();
 	/*
 	 * Forbid the first pass from falling back to types that fragment
 	 * memory until all local zones are considered.
Index: rpi4-kernel/fs/ecryptfs/read_write.c
===================================================================
--- rpi4-kernel.orig/fs/ecryptfs/read_write.c
+++ rpi4-kernel/fs/ecryptfs/read_write.c
@@ -261,3 +261,25 @@ int ecryptfs_read_lower_page_segment(str
 	flush_dcache_page(page_for_ecryptfs);
 	return rc;
 }
+
+/**
+ * ecryptfs_fsync_lower
+ * @ecryptfs_inode: The eCryptfs inode
+ * @datasync: Only perform a fdatasync operation
+ *
+ * Write back data and metadata for the lower file to disk.  If @datasync is
+ * set only metadata needed to access modified file data is written.
+ *
+ * Returns 0 on success; less than zero on error
+ */
+int ecryptfs_fsync_lower(struct inode *ecryptfs_inode, int datasync)
+{
+	struct file *lower_file;
+
+	lower_file = ecryptfs_inode_to_private(ecryptfs_inode)->lower_file;
+	if (!lower_file)
+		return -EIO;
+	if (!lower_file->f_op->fsync)
+		return 0;
+	return vfs_fsync(lower_file, datasync);
+}
Index: rpi4-kernel/kernel/events/core.c
===================================================================
--- rpi4-kernel.orig/kernel/events/core.c
+++ rpi4-kernel/kernel/events/core.c
@@ -6673,7 +6673,7 @@ static void perf_pending_task(struct cal
 	put_event(event);
 }
 
-/*              
+/*
  * We assume there is only KVM supporting the callbacks.
  * Later on, we might change it to a list if there is
  * another virtualization implementation supporting the callbacks.
@@ -12193,9 +12193,9 @@ perf_check_permission(struct perf_event_
  * @group_fd:		group leader event fd
  * @flags:		perf event open flags
  */
-SYSCALL_DEFINE5(perf_event_open,
-		struct perf_event_attr __user *, attr_uptr,
-		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
+int ksys_perf_event_open(
+		struct perf_event_attr __user * attr_uptr,
+		pid_t pid, int cpu, int group_fd, unsigned long flags)
 {
 	struct perf_event *group_leader = NULL, *output_event = NULL;
 	struct perf_event *event, *sibling;
@@ -12654,6 +12654,13 @@ err_fd:
 	return err;
 }
 
+SYSCALL_DEFINE5(perf_event_open,
+               struct perf_event_attr __user *, attr_uptr,
+               pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
+{
+      return ksys_perf_event_open(attr_uptr, pid, cpu, group_fd, flags);
+}
+
 /**
  * perf_event_create_kernel_counter
  *
